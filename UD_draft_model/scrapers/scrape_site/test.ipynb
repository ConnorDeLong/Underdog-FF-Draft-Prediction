{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from UD_draft_model.scrapers.scrape_site.pull_bearer_token import pull_bearer_token\n",
    "\n",
    "\n",
    "class BaseData:\n",
    "    def __init__(self, clear_json_attrs: bool=True, slate_id: str=None):\n",
    "        self._clear_json_attrs = clear_json_attrs\n",
    "\n",
    "        if slate_id is None:\n",
    "            # self.slate_id = '87a5caba-d5d7-46d9-a798-018d7c116213'\n",
    "            self.slate_id = \"f659a9be-fd34-4a1e-9c43-0816267e603d\"\n",
    "        else:\n",
    "            self.slate_id = slate_id\n",
    "\n",
    "        # user-agent and/or accept headers sometimes required\n",
    "        self.auth_header = {\n",
    "            \"accept\": \"application/json\",\n",
    "            \"user-agent\": \"Mozilla/5.0 (X11; Linux x86_64) \\\n",
    "                                AppleWebKit/537.36 (KHTML, like Gecko) \\\n",
    "                                Chrome/99.0.4844.51 Safari/537.36\",\n",
    "        }\n",
    "\n",
    "        self._player_scores_wk_1_id = 78\n",
    "        self._player_scores_wk_last_id = 78 + 17\n",
    "\n",
    "    def build_all_dfs(self, sleep_time: int=0):\n",
    "        \"\"\"\n",
    "        Overwrites every 'df_' attribute with a df that is created by running the\n",
    "        'create_' method that matches it. This serves as the primary method\n",
    "        for building the dfs associated with the class\n",
    "        \"\"\"\n",
    "\n",
    "        attrs = [attr for attr in dir(self) if attr.startswith(\"df_\")]\n",
    "        for attr in attrs:\n",
    "            method_name = \"create_\" + attr\n",
    "            try:\n",
    "                self.__dict__[attr] = getattr(self, method_name)()\n",
    "            except:\n",
    "                print(getattr(self, method_name), \"failed to run\")\n",
    "\n",
    "            if sleep_time > 0:\n",
    "                time.sleep(sleep_time)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        if self._clear_json_attrs == True:\n",
    "            self.clear_json_attrs()\n",
    "\n",
    "    def clear_json_attrs(self):\n",
    "        \"\"\"\n",
    "        Clears all the atttributes that hold the json data pulled from the API.\n",
    "        By default, this is executed when the build_all_dfs method is run\n",
    "        \"\"\"\n",
    "\n",
    "        attrs = [attr for attr in dir(self) if attr.startswith(\"json\")]\n",
    "        for attr in attrs:\n",
    "            self.__dict__[attr] = {}\n",
    "\n",
    "    def read_in_site_data(self, url, headers: dict=None) -> dict:\n",
    "        \"\"\"Pulls in the raw data from the API and returns it as a dict\"\"\"\n",
    "\n",
    "        if headers is None:\n",
    "            headers = {}\n",
    "\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        site_data = response.json()\n",
    "\n",
    "        return site_data\n",
    "\n",
    "    def create_scraped_data_df(self, scraped_data: list) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Converts a list of dictionaries into a df where the keys of the dicts are\n",
    "        used for the columns and the values are placed in the rows.\n",
    "        NOTE: this assumes the keys in all dicts are the same.\n",
    "        \"\"\"\n",
    "\n",
    "        # Get the dicionary keys to identify the columns of the list for each output dict key\n",
    "        output_data_cols = []\n",
    "        for output_data_col in scraped_data[0].keys():\n",
    "            output_data_cols.append(output_data_col)\n",
    "\n",
    "        final_data_dict = {\"columns\": output_data_cols}\n",
    "        for data_dict_id, data in enumerate(scraped_data):\n",
    "            all_data_elements = []\n",
    "            for output_data_col in output_data_cols:\n",
    "                try:\n",
    "                    data_element = data[output_data_col]\n",
    "                except:\n",
    "                    # Note: This should probably be conditional on the data type, \n",
    "                    # but just using N/A for now.\n",
    "                    data_element = \"N/A\"\n",
    "\n",
    "                all_data_elements.append(data_element)\n",
    "\n",
    "            final_data_dict[data_dict_id] = all_data_elements\n",
    "\n",
    "        final_data_df = self._convert_data_dict_to_df(final_data_dict)\n",
    "\n",
    "        return final_data_df\n",
    "\n",
    "    def _convert_data_dict_to_df(self, scraped_data_dict: dict) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Converts the dict from the create_scraped_data_dict function to a df\n",
    "        NOTE: The input dict takes the following form:\n",
    "        {'columns': [<column names>], 1: [<column values>], 2: [<column_values>], ...}\n",
    "        \"\"\"\n",
    "\n",
    "        columns = scraped_data_dict[\"columns\"]\n",
    "\n",
    "        data_keys = list(scraped_data_dict.keys())[1:]\n",
    "\n",
    "        data_for_df = []\n",
    "        for data_key in data_keys:\n",
    "            data = scraped_data_dict[data_key]\n",
    "\n",
    "            data_for_df.append(data)\n",
    "\n",
    "        final_df = pd.DataFrame(data=data_for_df, columns=columns)\n",
    "\n",
    "        return final_df\n",
    "\n",
    "    def _create_week_id_mapping(self) -> pd.DataFrame:\n",
    "        \"\"\"Creates a map between the APIs Week ID and the actual Week number\"\"\"\n",
    "\n",
    "        wk_numbers = []\n",
    "        wk_ids = []\n",
    "        for wk_number, wk_id in enumerate(\n",
    "            range(self._player_scores_wk_1_id, self._player_scores_wk_last_id + 1)\n",
    "        ):\n",
    "            wk_numbers.append(wk_number + 1)\n",
    "            wk_ids.append(wk_id)\n",
    "\n",
    "        mapping = {\"week_number\": wk_numbers, \"week_id\": wk_ids}\n",
    "        df_mapping = pd.DataFrame(data=mapping)\n",
    "\n",
    "        return df_mapping\n",
    "\n",
    "\n",
    "class ReferenceData(BaseData):\n",
    "    \"\"\"Compiles all major reference data into dataframes\"\"\"\n",
    "\n",
    "    def __init__(self, clear_json_attrs: bool = True, slate_id: str = None):\n",
    "        super().__init__(clear_json_attrs=clear_json_attrs, slate_id=slate_id)\n",
    "\n",
    "        self.auth_header = {\"accept\": \"application/json\"}\n",
    "\n",
    "        # week ID seems right but can't find the correct url for it\n",
    "        # self._player_scores_wk_1_id = 78\n",
    "        self._player_scores_wk_1_id = 1186\n",
    "\n",
    "        self.url_players = (\n",
    "            \"https://stats.underdogfantasy.com/v1/slates/\" + self.slate_id + \"/players\"\n",
    "        )\n",
    "        self.url_appearances = (\n",
    "            \"https://stats.underdogfantasy.com/v1/slates/\"\n",
    "            + self.slate_id\n",
    "            + \"/scoring_types/ccf300b0-9197-5951-bd96-cba84ad71e86/appearances\"\n",
    "        )\n",
    "        self.url_teams = \"https://stats.underdogfantasy.com/v1/teams\"\n",
    "\n",
    "        base_url_player_scores = \"https://stats.underdogfantasy.com/v1/weeks/\"\n",
    "        end_url_player_scores = (\n",
    "            \"/scoring_types/ccf300b0-9197-5951-bd96-cba84ad71e86/appearances\"\n",
    "        )\n",
    "        self.urls_player_scores = {\n",
    "            \"player_scores_wk_\"\n",
    "            + str(i + 1): base_url_player_scores\n",
    "            + str(wk_id)\n",
    "            + end_url_player_scores\n",
    "            for i, wk_id in enumerate(\n",
    "                range(self._player_scores_wk_1_id, self._player_scores_wk_1_id + 17)\n",
    "            )\n",
    "        }\n",
    "\n",
    "        self.df_players = pd.DataFrame()\n",
    "        self.df_appearances = pd.DataFrame()\n",
    "        self.df_teams = pd.DataFrame()\n",
    "        self.df_players_master = pd.DataFrame()\n",
    "        self.df_player_scores = pd.DataFrame()\n",
    "\n",
    "    def build_all_dfs(self):\n",
    "        attrs = [attr for attr in dir(self) if attr.startswith(\"df_\")]\n",
    "        for attr in attrs:\n",
    "            if attr == \"df_players_master\":\n",
    "                pass\n",
    "            else:\n",
    "                try:\n",
    "                    method_name = \"create_\" + attr\n",
    "                    self.__dict__[attr] = getattr(self, method_name)()\n",
    "                except:\n",
    "                    print(getattr(self, method_name), \"failed to run\")\n",
    "\n",
    "        # This ensures the dfs it depends on are created\n",
    "        self.df_players_master = self.create_df_players_master()\n",
    "\n",
    "        if self._clear_json_attrs == True:\n",
    "            self.clear_json_attrs()\n",
    "\n",
    "    def create_df_players(self) -> pd.DataFrame:\n",
    "        self.json_players = self.read_in_site_data(\n",
    "            self.url_players, headers=self.auth_header\n",
    "        )\n",
    "\n",
    "        initial_scraped_df = self.create_scraped_data_df(self.json_players[\"players\"])\n",
    "        initial_scraped_df.drop([\"image_url\"], axis=1, inplace=True)\n",
    "        initial_scraped_df.rename(columns={\"id\": \"player_id\"}, inplace=True)\n",
    "\n",
    "        return initial_scraped_df\n",
    "\n",
    "    def create_df_appearances(self) -> pd.DataFrame:\n",
    "        self.json_appearances = self.read_in_site_data(\n",
    "            self.url_appearances, headers=self.auth_header\n",
    "        )\n",
    "\n",
    "        initial_scraped_df = self.create_scraped_data_df(\n",
    "            self.json_appearances[\"appearances\"]\n",
    "        )\n",
    "        initial_scraped_df.drop(\n",
    "            [\"latest_news_item_updated_at\", \"score\"], axis=1, inplace=True\n",
    "        )\n",
    "\n",
    "        # 'projection' column values are dicitionaries which can be converted to a df and merged\n",
    "        projection_col = initial_scraped_df[\"projection\"].to_list()\n",
    "        projection_df = self.create_scraped_data_df(projection_col)\n",
    "\n",
    "        projection_df.drop([\"id\", \"scoring_type_id\"], axis=1, inplace=True)\n",
    "        projection_df.rename(\n",
    "            columns={\"points\": \"season_projected_points\"}, inplace=True\n",
    "        )\n",
    "\n",
    "        final_df = pd.merge(\n",
    "            initial_scraped_df,\n",
    "            projection_df,\n",
    "            left_index=True,\n",
    "            right_index=True,\n",
    "            how=\"left\",\n",
    "        )\n",
    "        final_df.drop([\"projection\"], axis=1, inplace=True)\n",
    "\n",
    "        df_pos_map = self._create_position_mapping(final_df)\n",
    "        final_df = pd.merge(final_df, df_pos_map, on=\"position_id\", how=\"left\")\n",
    "\n",
    "        final_df.rename(columns={\"id\": \"appearance_id\"}, inplace=True)\n",
    "\n",
    "        return final_df\n",
    "\n",
    "    def create_df_teams(self) -> pd.DataFrame:\n",
    "        self.json_teams = self.read_in_site_data(\n",
    "            self.url_teams, headers=self.auth_header\n",
    "        )\n",
    "\n",
    "        initial_scraped_df = self.create_scraped_data_df(self.json_teams[\"teams\"])\n",
    "\n",
    "        keep_vars = [\"id\", \"abbr\", \"name\"]\n",
    "        final_df = initial_scraped_df[keep_vars].copy()\n",
    "\n",
    "        final_df.rename(columns={\"name\": \"team_name\", \"id\": \"team_id\"}, inplace=True)\n",
    "\n",
    "        return final_df\n",
    "\n",
    "    def create_df_players_master(self) -> pd.DataFrame:\n",
    "        \"\"\"Creates a master lookup for player attributes\"\"\"\n",
    "\n",
    "        if len(self.df_appearances) == 0:\n",
    "            self.df_appearances = self.create_df_appearances()\n",
    "\n",
    "        if len(self.df_players) == 0:\n",
    "            self.df_players = self.create_df_players()\n",
    "\n",
    "        if len(self.df_teams) == 0:\n",
    "            self.df_teams = self.create_df_teams()\n",
    "\n",
    "        # Team is more accurate in the df_players data and position from df_appearances\n",
    "        # reflects the posisiton at the time of the draft\n",
    "        df_appearances = self.df_appearances.drop([\"team_id\"], axis=1, inplace=False)\n",
    "        df_players = self.df_players.drop([\"position_id\"], axis=1, inplace=False)\n",
    "\n",
    "        final_df = pd.merge(df_appearances, df_players, on=\"player_id\", how=\"left\")\n",
    "\n",
    "        final_df = pd.merge(final_df, self.df_teams, on=\"team_id\", how=\"left\")\n",
    "\n",
    "        return final_df\n",
    "\n",
    "    def create_df_player_scores(self):\n",
    "        \"\"\"\n",
    "        This no longer appears to work due to either a change in the endpoint\n",
    "        or the starting point week id is wrong\n",
    "        \"\"\"\n",
    "\n",
    "        self.json_player_scores = {\n",
    "            \"player_scores_wk_\"\n",
    "            + str(i + 1): self.read_in_site_data(\n",
    "                self.urls_player_scores[\"player_scores_wk_\" + str(i + 1)],\n",
    "                headers=self.auth_header,\n",
    "            )\n",
    "            for i, wk_id in enumerate(\n",
    "                range(self._player_scores_wk_1_id, self._player_scores_wk_1_id + 17)\n",
    "            )\n",
    "        }\n",
    "\n",
    "        player_scores_df_list = []\n",
    "        for wk_id in range(1, 18):\n",
    "            if (\n",
    "                len(\n",
    "                    self.json_player_scores[\"player_scores_wk_\" + str(wk_id)][\n",
    "                        \"appearances\"\n",
    "                    ]\n",
    "                )\n",
    "                > 0\n",
    "            ):\n",
    "                player_scores_json = self.json_player_scores[\n",
    "                    \"player_scores_wk_\" + str(wk_id)\n",
    "                ]\n",
    "                player_scores_df = self._create_df_player_scores_one_wk(\n",
    "                    player_scores_json[\"appearances\"]\n",
    "                )\n",
    "                player_scores_df[\"week_number\"] = wk_id\n",
    "\n",
    "                player_scores_df_list.append(player_scores_df)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        player_scores_df = pd.concat(player_scores_df_list)\n",
    "        player_scores_df.reset_index(inplace=True)\n",
    "\n",
    "        return player_scores_df\n",
    "\n",
    "    def _create_df_player_scores_one_wk(self, scraped_data: list) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Each weeks player scores are contained in its own URL - this creates a df\n",
    "        of those scores for one week\n",
    "        \"\"\"\n",
    "\n",
    "        initial_scraped_df = self.create_scraped_data_df(scraped_data)\n",
    "        initial_scraped_df.drop([\"latest_news_item_updated_at\"], axis=1, inplace=True)\n",
    "\n",
    "        # 'projection' column values are dicitionaries which can be converted to a df and merged\n",
    "        projection_col = initial_scraped_df[\"projection\"].to_list()\n",
    "        projection_df = self.create_scraped_data_df(projection_col)\n",
    "\n",
    "        projection_df = projection_df[[\"points\"]]\n",
    "        projection_df.rename(columns={\"points\": \"projected_points\"}, inplace=True)\n",
    "\n",
    "        score_col = initial_scraped_df[\"score\"].to_list()\n",
    "        score_df = self.create_scraped_data_df(score_col)\n",
    "\n",
    "        score_df = score_df[[\"points\"]]\n",
    "        score_df.rename(columns={\"points\": \"actual_points\"}, inplace=True)\n",
    "\n",
    "        final_df = pd.merge(\n",
    "            initial_scraped_df,\n",
    "            projection_df,\n",
    "            left_index=True,\n",
    "            right_index=True,\n",
    "            how=\"left\",\n",
    "        )\n",
    "        final_df = pd.merge(\n",
    "            final_df, score_df, left_index=True, right_index=True, how=\"left\"\n",
    "        )\n",
    "\n",
    "        final_df.drop([\"projection\", \"score\"], axis=1, inplace=True)\n",
    "\n",
    "        return final_df\n",
    "\n",
    "    def _create_position_mapping(self, df_appearances: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Creates df that maps position to position_id since this cant be found in the API\n",
    "        \"\"\"\n",
    "\n",
    "        df_pos_map = df_appearances.copy()\n",
    "\n",
    "        df_pos_map[\"position\"] = df_pos_map[\"position_rank\"].str[0:2]\n",
    "        df_pos_map = df_pos_map[[\"position_id\", \"position\"]].loc[\n",
    "            df_pos_map[\"position\"].notnull()\n",
    "        ]\n",
    "        df_pos_map = df_pos_map.drop_duplicates(\n",
    "            subset=[\"position\", \"position_id\"], keep=\"first\"\n",
    "        )\n",
    "\n",
    "        return df_pos_map\n",
    "\n",
    "\n",
    "class LeagueData(BaseData):\n",
    "    \"\"\"Compiles all major league specific data into dataframes\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, league_ids: list, bearer_token: str, clear_json_attrs: bool = True\n",
    "    ):\n",
    "        super().__init__(clear_json_attrs=clear_json_attrs)\n",
    "\n",
    "        self.auth_header['authorization'] = bearer_token\n",
    "\n",
    "        self.url_drafts = {}\n",
    "        self.url_weekly_scores = {}\n",
    "        for league_id in league_ids:\n",
    "            url_draft = \"https://api.underdogfantasy.com/v2/drafts/\" + league_id\n",
    "            url_weekly_scores = (\n",
    "                \"https://api.underdogfantasy.com/v1/drafts/\"\n",
    "                + league_id\n",
    "                + \"/weekly_scores\"\n",
    "            )\n",
    "\n",
    "            self.url_drafts[league_id] = url_draft\n",
    "            self.url_weekly_scores[league_id] = url_weekly_scores\n",
    "\n",
    "        self.json_drafts = {}\n",
    "        self.json_weekly_scores = {}\n",
    "\n",
    "        self.df_drafts = pd.DataFrame()\n",
    "        self.df_weekly_scores = pd.DataFrame()\n",
    "\n",
    "    def create_df_drafts(self) -> pd.DataFrame:\n",
    "        dfs = []\n",
    "        for league_id in self.league_ids:\n",
    "            df = self._create_df_draft_ind_league(league_id)\n",
    "            dfs.append(df)\n",
    "\n",
    "        final_df = pd.concat(dfs)\n",
    "\n",
    "        return final_df\n",
    "\n",
    "    def create_df_weekly_scores(self) -> pd.DataFrame:\n",
    "        dfs = []\n",
    "        for league_id in self.league_ids:\n",
    "            df = self._create_df_weekly_scores_ind_league(league_id)\n",
    "            dfs.append(df)\n",
    "\n",
    "        final_df = pd.concat(dfs)\n",
    "        final_df.reset_index(inplace=True)\n",
    "\n",
    "        week_mapping = self._create_week_id_mapping()\n",
    "        final_df = pd.merge(final_df, week_mapping, on=\"week_id\")\n",
    "\n",
    "        return final_df\n",
    "\n",
    "    def _create_df_draft_ind_league(self, league_id: str) -> pd.DataFrame:\n",
    "        self.json_drafts[league_id] = self.read_in_site_data(\n",
    "            self.url_drafts[league_id], headers=self.auth_header\n",
    "        )\n",
    "        scraped_data = self.json_drafts[league_id][\"draft\"][\"picks\"]\n",
    "\n",
    "        initial_scraped_df = self.create_scraped_data_df(scraped_data)\n",
    "        initial_scraped_df.drop([\"projection_average\"], axis=1, inplace=True)\n",
    "\n",
    "        initial_scraped_df[\"draft_id\"] = league_id\n",
    "\n",
    "        return initial_scraped_df\n",
    "\n",
    "    def _create_df_weekly_scores_ind_league(self, league_id: str) -> pd.DataFrame:\n",
    "        self.json_weekly_scores[league_id] = self.read_in_site_data(\n",
    "            self.url_weekly_scores[league_id], headers=self.auth_header\n",
    "        )\n",
    "        scraped_data = self.json_weekly_scores[league_id][\"draft_weekly_scores\"]\n",
    "\n",
    "        initial_scraped_df = self.create_scraped_data_df(scraped_data)\n",
    "\n",
    "        weekly_scores = self._pull_out_weekly_scores(initial_scraped_df)\n",
    "\n",
    "        initial_scraped_df.drop([\"week\", \"draft_entries_points\"], axis=1, inplace=True)\n",
    "\n",
    "        final_scraped_df = pd.merge(\n",
    "            left=weekly_scores, right=initial_scraped_df, on=\"id\", how=\"left\"\n",
    "        )\n",
    "        final_scraped_df.drop([\"id\"], axis=1, inplace=True)\n",
    "\n",
    "        return final_scraped_df\n",
    "\n",
    "    def _pull_out_weekly_scores(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Each row represents one week where each teams score is contained\n",
    "        within a dicitonary for that week. This pulls those scores out and\n",
    "        puts them in a Team/Week level df\n",
    "        \"\"\"\n",
    "\n",
    "        df = df.copy()\n",
    "\n",
    "        all_weekly_scores = []\n",
    "        for index, row in df.iterrows():\n",
    "            row_id = row[\"id\"]\n",
    "            week_id = row[\"week\"][\"id\"]\n",
    "            status = row[\"week\"][\"status\"]\n",
    "            points_dict = row[\"draft_entries_points\"]\n",
    "\n",
    "            for user_id, points in points_dict.items():\n",
    "                weekly_scores = [row_id, week_id, status, user_id, points]\n",
    "\n",
    "                all_weekly_scores.append(weekly_scores)\n",
    "\n",
    "        columns = [\"id\", \"week_id\", \"status\", \"user_id\", \"total_points\"]\n",
    "        df = pd.DataFrame(data=all_weekly_scores, columns=columns)\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "class UserData(BaseData):\n",
    "    \n",
    "    def __init__(self, bearer_token: str, clear_json_attrs: bool = True):\n",
    "        \"\"\"\n",
    "        Note: This requires the user-agent header - Should be able to grab this\n",
    "        with the bearer token, but hard coding for now\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(clear_json_attrs=clear_json_attrs)\n",
    "\n",
    "        self.auth_header['authorization'] = bearer_token\n",
    "\n",
    "        self.url_base_leagues = (\n",
    "            \"https://api.underdogfantasy.com/v2/user/slates/\"\n",
    "            + self.slate_id\n",
    "            + \"/live_drafts\"\n",
    "        )\n",
    "        # self.url_base_leagues = 'https://api.underdogfantasy.com/v2/user/slates/' \n",
    "        # + self.slate_id + '/completed_drafts'\n",
    "        # self.url_base_leagues = 'https://api.underdogfantasy.com/v2/user/slates/' \n",
    "        # + self.slate_id + '/settled_drafts'\n",
    "        self.url_tourney_league_ids = (\n",
    "            \"https://api.underdogfantasy.com/v1/user/slates/\"\n",
    "            + self.slate_id\n",
    "            + \"/tournament_rounds\"\n",
    "        )\n",
    "\n",
    "        self.json_leagues = {}\n",
    "\n",
    "        self.df_all_leagues = pd.DataFrame()\n",
    "\n",
    "    def create_df_all_leagues(self, league_urls: list = None) -> pd.DataFrame:\n",
    "        if league_urls is None:\n",
    "            league_urls = self._create_league_urls()\n",
    "\n",
    "        leagues = []\n",
    "        for i, league_url in enumerate(league_urls):\n",
    "            df = self._create_df_leagues(league_url, \"league_\" + str(i + 1))\n",
    "            leagues.append(df)\n",
    "\n",
    "        df_all_leagues = pd.concat(leagues)\n",
    "        df_all_leagues.reset_index(inplace=True)\n",
    "        df_all_leagues.drop(columns=[\"index\"], inplace=True)\n",
    "\n",
    "        return df_all_leagues\n",
    "\n",
    "    def _create_df_leagues(self, url_base: str, json_leagues_key: str) -> pd.DataFrame:\n",
    "        self.json_leagues[json_leagues_key] = self._create_json_leagues(url_base)\n",
    "        scraped_data = self.json_leagues[json_leagues_key]\n",
    "\n",
    "        leagues_df_list = []\n",
    "        for leagues_page in scraped_data.values():\n",
    "            leagues_page_df = self.create_scraped_data_df(leagues_page[\"drafts\"])\n",
    "            leagues_df_list.append(leagues_page_df)\n",
    "\n",
    "        leagues_df = pd.concat(leagues_df_list)\n",
    "\n",
    "        return leagues_df\n",
    "\n",
    "    def _create_json_leagues(self, url_base: str) -> dict:\n",
    "        \"\"\"\n",
    "        Loops through all the different pages that contain the league level data\n",
    "        and stores each as an entry in a dict\n",
    "        \"\"\"\n",
    "\n",
    "        url_exists = True\n",
    "        i = 1\n",
    "        leagues_json_dict = {}\n",
    "        while url_exists:\n",
    "            if i == 1:\n",
    "                url = url_base\n",
    "            else:\n",
    "                url = url_base + \"?page=\" + str(i)\n",
    "\n",
    "            leagues = self.read_in_site_data(url, headers=self.auth_header)\n",
    "\n",
    "            if len(leagues[\"drafts\"]) > 0:\n",
    "                leagues_json_dict[\"page_\" + str(i)] = leagues\n",
    "            else:\n",
    "                url_exists = False\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        return leagues_json_dict\n",
    "\n",
    "    def _create_df_tourney_league_ids(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Tournament leagues (i.e. Puppy 1, Puppy 2, etc.) require the ID of the\n",
    "        tourney in order to find all entries in it - This creates of all tourney\n",
    "        IDs that has at least one entry\n",
    "        \"\"\"\n",
    "\n",
    "        json_tourney_league_ids = self.read_in_site_data(\n",
    "            self.url_tourney_league_ids, headers=self.auth_header\n",
    "        )\n",
    "        scraped_data = json_tourney_league_ids[\"tournament_rounds\"]\n",
    "\n",
    "        initial_scraped_df = self.create_scraped_data_df(scraped_data)\n",
    "\n",
    "        # Pulling out the 'id' from the 'tournament' dict in case this is whats needed\n",
    "        tournament_col = initial_scraped_df[\"tournament\"].to_list()\n",
    "        tournament_df = self.create_scraped_data_df(tournament_col)\n",
    "        tournament_df.rename(columns={\"id\": \"tournament_id\"}, inplace=True)\n",
    "        tournament_df = tournament_df[\"tournament_id\"]\n",
    "\n",
    "        initial_scraped_df.drop([\"tournament\"], axis=1, inplace=True)\n",
    "        final_df = initial_scraped_df.join(tournament_df)\n",
    "\n",
    "        return final_df\n",
    "\n",
    "    def _create_league_urls(self, tourney_league_ids: list = None) -> list:\n",
    "        \"\"\"\n",
    "        Creates a list of all the URLs that contain entries\n",
    "        \"\"\"\n",
    "        \n",
    "        if tourney_league_ids is None:\n",
    "            tourney_league_ids = list(self._create_df_tourney_league_ids()[\"id\"])\n",
    "\n",
    "        base_url = \"https://api.underdogfantasy.com/v1/user/tournament_rounds/\"\n",
    "        tourney_league_urls = []\n",
    "        for tourney_league_id in tourney_league_ids:\n",
    "            tourney_league_url = base_url + tourney_league_id + \"/drafts\"\n",
    "            tourney_league_urls.append(tourney_league_url)\n",
    "\n",
    "        tourney_league_urls.append(self.url_base_leagues)\n",
    "\n",
    "        return tourney_league_urls\n",
    "\n",
    "\n",
    "class ContestRefs(BaseData):\n",
    "    \"\"\" \n",
    "    Compiles all major contest related data into dataframes.\n",
    "    Note that this includes contests specific to a user (e.g. completed\n",
    "    slates, settled slates, etc.)    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bearer_token: str, clear_json_attrs: bool = True):\n",
    "        super().__init__(clear_json_attrs=clear_json_attrs, slate_id=None)\n",
    "\n",
    "        self.auth_header['authorization'] = bearer_token\n",
    "\n",
    "        self.url_slates_available = (\n",
    "            'https://stats.underdogfantasy.com/v1/sports/nfl/slates'\n",
    "        )\n",
    "        self.url_slates_completed = (\n",
    "            'https://api.underdogfantasy.com/v2/user/completed_slates'\n",
    "        )\n",
    "        self.url_slates_settled = (\n",
    "            'https://api.underdogfantasy.com/v1/user/sports/nfl/settled_slates'\n",
    "        )\n",
    "        self.url_scoring_types = (\n",
    "            'https://stats.underdogfantasy.com/v1/scoring_types'\n",
    "        )\n",
    "        self.url_contest_styles = (\n",
    "            'https://stats.underdogfantasy.com/v1/contest_styles'\n",
    "        )\n",
    "\n",
    "        self.df_slates_available = None\n",
    "        self.df_slates_completed = None\n",
    "        self.df_slates_settled = None\n",
    "        self.df_scoring_types = None\n",
    "        self.df_contest_styles = None\n",
    "\n",
    "        self.json = {}\n",
    "\n",
    "    def create_df_slates_available(self, headers: dict=None, clear_json: bool=False,\n",
    "    update_attr: bool=False) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Creates a slate level df of all slates that are available to \n",
    "        draft in.\n",
    "        \"\"\"\n",
    "\n",
    "        df = self._create_df_slates(\n",
    "            self.url_slates_available, 'slates_available',\n",
    "            headers=headers, clear_json=clear_json\n",
    "        )\n",
    "\n",
    "        if update_attr:\n",
    "            self.df_slates_available = df\n",
    "\n",
    "        return df\n",
    "\n",
    "    def create_df_slates_completed(self, headers: dict=None, clear_json: bool=False,\n",
    "    update_attr: bool=False) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Creates a slate level df of all slates that are completed.\n",
    "        \"\"\"\n",
    "\n",
    "        df = self._create_df_slates(\n",
    "            self.url_slates_completed, 'slates_completed',\n",
    "            headers=headers, clear_json=clear_json\n",
    "        )\n",
    "\n",
    "        if update_attr:\n",
    "            self.df_slates_completed = df\n",
    "\n",
    "        return df\n",
    "\n",
    "    def create_df_slates_settled(self, headers: dict=None, clear_json: bool=False,\n",
    "    update_attr: bool=False) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Creates a slate level df of all slates that are settled.\n",
    "        \"\"\"\n",
    "\n",
    "        df = self._create_df_slates(\n",
    "            self.url_slates_settled, 'slates_settled',\n",
    "            headers=headers, clear_json=clear_json\n",
    "        )\n",
    "\n",
    "        if update_attr:\n",
    "            self.df_slates_settled = df\n",
    "\n",
    "        return df\n",
    "\n",
    "    def create_df_scoring_types(self, headers: dict=None, clear_json: bool=False,\n",
    "    update_attr: bool=False) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Creates a scoring type level df with the scoring types of all existing\n",
    "        NFL contests.\n",
    "\n",
    "        Notes:\n",
    "            - This is needed to automate the \"appearances\" (i.e. draft rank)\n",
    "            pull which uses the id as part of the url string\n",
    "            - 'display_stats' contains more descriptive information about each\n",
    "            scoring_type, but that data isn't needed now and would take some\n",
    "            time to pull out and structure.\n",
    "        \"\"\"\n",
    "\n",
    "        if headers is None:\n",
    "            headers=self.auth_header\n",
    "        \n",
    "        self.json['scoring_types'] = self.read_in_site_data(\n",
    "            self.url_scoring_types, headers=self.auth_header\n",
    "        )\n",
    "\n",
    "        df = self.create_scraped_data_df(self.json['scoring_types'][\"scoring_types\"])\n",
    "\n",
    "        df = df.loc[df['sport_id'] == 'NFL']\n",
    "\n",
    "        if update_attr:\n",
    "            self.df_scoring_types = df\n",
    "\n",
    "        if clear_json:\n",
    "            del self.json['scoring_types']\n",
    "\n",
    "        return df\n",
    "\n",
    "    def create_df_contest_styles(self, headers: dict=None, clear_json: bool=False,\n",
    "    update_attr: bool=False) -> pd.DataFrame:\n",
    "\n",
    "        if headers is None:\n",
    "            headers=self.auth_header\n",
    "        \n",
    "        self.json['contest_styles'] = self.read_in_site_data(\n",
    "            self.url_contest_styles, headers=self.auth_header\n",
    "        )\n",
    "\n",
    "        df = self.create_scraped_data_df(self.json['contest_styles'][\"contest_styles\"])\n",
    "\n",
    "        df = df.loc[df['sport_id'] == 'NFL']\n",
    "\n",
    "        if update_attr:\n",
    "            self.df_contest_styles = df\n",
    "\n",
    "        if clear_json:\n",
    "            del self.json['contest_styles'] \n",
    "\n",
    "        return df\n",
    "\n",
    "    def get_slate_ids(self, slate_type: str):\n",
    "        \"\"\"\n",
    "        Returns a list that contains each slate id for the slate_type.\n",
    "        slate_type must be 'available', 'completed', or 'settled'.\n",
    "        \"\"\"\n",
    "\n",
    "        attr = 'df_slates_' + slate_type.strip()\n",
    "        method_name = 'create_df_slates_' + slate_type.strip()\n",
    "\n",
    "        # NEED TO DOUBLE CHECK THIS - \n",
    "        if self.__dict__[attr] is None:\n",
    "            df = getattr(self, method_name)()\n",
    "        else:\n",
    "            df = self.__dict__[attr]\n",
    "\n",
    "        slates = list(df['id'])\n",
    "\n",
    "        return slates        \n",
    "\n",
    "    def _create_df_slates(self, url_slate: str, slate_type: str,\n",
    "        headers: dict=None, clear_json: bool=False\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\" \n",
    "        Helper method that can be used to create a df for any slate type.\n",
    "        \"\"\"\n",
    "\n",
    "        if headers is None:\n",
    "            headers=self.auth_header\n",
    "        \n",
    "        self.json[slate_type] = self.read_in_site_data(\n",
    "            url_slate, headers=self.auth_header\n",
    "        )\n",
    "\n",
    "        df = self.create_scraped_data_df(self.json[slate_type][\"slates\"])\n",
    "\n",
    "        if clear_json:\n",
    "            del self.json[slate_type]\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "def create_underdog_df_dict(bearer_token: str, sleep_time: int = 0) -> dict:\n",
    "    \"\"\"Creates a dictionary of dfs containing the most relevant UD data\"\"\"\n",
    "\n",
    "    ref_data = ReferenceData()\n",
    "    ref_data.build_all_dfs()\n",
    "\n",
    "    user_data = UserData(bearer_token)\n",
    "    user_data.build_all_dfs()\n",
    "    league_ids = list(user_data.df_all_leagues[\"id\"])\n",
    "\n",
    "    league_data = LeagueData(league_ids, bearer_token)\n",
    "    league_data.build_all_dfs(sleep_time=sleep_time)\n",
    "\n",
    "    df_players_master = ref_data.df_players_master\n",
    "    df_player_scores = ref_data.df_player_scores\n",
    "\n",
    "    player_vars = [\n",
    "        \"appearance_id\",\n",
    "        \"player_id\",\n",
    "        \"position\",\n",
    "        \"team_name\",\n",
    "        \"first_name\",\n",
    "        \"last_name\",\n",
    "    ]\n",
    "    df_drafts = pd.merge(\n",
    "        league_data.df_drafts,\n",
    "        df_players_master[player_vars],\n",
    "        on=\"appearance_id\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "    df_weekly_scores = league_data.df_weekly_scores\n",
    "\n",
    "    final_dict = {\n",
    "        \"df_players_master\": df_players_master,\n",
    "        \"df_player_scores\": df_player_scores,\n",
    "        \"df_drafts\": df_drafts,\n",
    "        \"df_weekly_scores\": df_weekly_scores,\n",
    "        \"df_league_info\": user_data.df_all_leagues,\n",
    "    }\n",
    "\n",
    "    return final_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bearer eyJhbGciOiJIUzI1NiJ9.eyJqdGkiOiI0ZWNkMDVmZi1kYTExLTQwY2UtYTYwNS05ZDVmZjZlMGMwODciLCJzdWIiOiIwMzcxNzEzMS1lMDUzLTQ1MWQtOWJlNi0wOTc1NWY1ODc1YWUiLCJzY3AiOiJ1c2VyIiwiYXVkIjpudWxsLCJpYXQiOjE2NzU3Mzk4NjUsImV4cCI6MTY3ODM2OTYxMX0.j-dXl5tgcdh0NNFLa_gHhABWjVDccVyRpBd3mSa91kQ\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 50)\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "\n",
    "### Variables to change ###\n",
    "chromedriver_path = \"/usr/bin/chromedriver\"\n",
    "username = input(\"Enter Underdog username: \")\n",
    "password = getpass.getpass()\n",
    "\n",
    "### Keep as is ###\n",
    "url = \"https://underdogfantasy.com/lobby\"\n",
    "bearer_token = pull_bearer_token(url, chromedriver_path, username, password)\n",
    "\n",
    "print(bearer_token)\n",
    "\n",
    "### Pull all major UD data elements ###\n",
    "# underdog_data = create_underdog_df_dict(bearer_token, sleep_time=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>contest_style_ids</th>\n",
       "      <th>cutoff_at</th>\n",
       "      <th>description</th>\n",
       "      <th>game_count</th>\n",
       "      <th>lobby_hidden</th>\n",
       "      <th>sport_id</th>\n",
       "      <th>start_at</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>53ec8ef2-e1f2-401c-9a83-c0838d6a2b77</td>\n",
       "      <td>[502f3754-7bf4-4cc4-baf3-ea7d93f09318]</td>\n",
       "      <td>2023-02-12T23:22:00Z</td>\n",
       "      <td>1 - games</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NFL</td>\n",
       "      <td>2023-02-12T23:30:00Z</td>\n",
       "      <td>KC @ PHI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a1f9cf2c-1b60-4e3b-b837-22db7e0b232a</td>\n",
       "      <td>[9e62863e-1b29-53e8-8aca-2aae06aaac5f]</td>\n",
       "      <td>2023-09-07T23:52:00Z</td>\n",
       "      <td>2023 Way Too Early</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NFL</td>\n",
       "      <td>2023-09-08T00:00:00Z</td>\n",
       "      <td>2023 Way Too Early</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  \\\n",
       "0  53ec8ef2-e1f2-401c-9a83-c0838d6a2b77   \n",
       "1  a1f9cf2c-1b60-4e3b-b837-22db7e0b232a   \n",
       "\n",
       "                        contest_style_ids             cutoff_at  \\\n",
       "0  [502f3754-7bf4-4cc4-baf3-ea7d93f09318]  2023-02-12T23:22:00Z   \n",
       "1  [9e62863e-1b29-53e8-8aca-2aae06aaac5f]  2023-09-07T23:52:00Z   \n",
       "\n",
       "          description  game_count  lobby_hidden sport_id  \\\n",
       "0           1 - games         1.0         False      NFL   \n",
       "1  2023 Way Too Early         NaN         False      NFL   \n",
       "\n",
       "               start_at               title  \n",
       "0  2023-02-12T23:30:00Z            KC @ PHI  \n",
       "1  2023-09-08T00:00:00Z  2023 Way Too Early  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_available = 'https://stats.underdogfantasy.com/v1/sports/nfl/slates'\n",
    "url_completed = 'https://api.underdogfantasy.com/v2/user/completed_slates'\n",
    "url_settled = 'https://api.underdogfantasy.com/v1/user/sports/nfl/settled_slates'\n",
    "\n",
    "url_st = 'https://stats.underdogfantasy.com/v1/scoring_types'\n",
    "\n",
    "auth_header = {\n",
    "    \"accept\": \"application/json\",    \n",
    "    \"authorization\": bearer_token,\n",
    "    \"user-agent\": \"Mozilla/5.0 (X11; Linux x86_64) \\\n",
    "                        AppleWebKit/537.36 (KHTML, like Gecko) \\\n",
    "                        Chrome/99.0.4844.51 Safari/537.36\"\n",
    "}\n",
    "\n",
    "base = BaseData()\n",
    "\n",
    "data = base.read_in_site_data(url_available, headers=auth_header)\n",
    "# data = base.read_in_site_data(url_st)\n",
    "\n",
    "base.create_scraped_data_df(data['slates'])\n",
    "\n",
    "# data['slates'][0].keys()\n",
    "# data['scoring_types'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContestRefs(BaseData):\n",
    "    \"\"\" \n",
    "    Compiles all major contest related data into dataframes.\n",
    "    Note that this includes contests specific to a user (e.g. completed\n",
    "    slates, settled slates, etc.)    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bearer_token: str, clear_json_attrs: bool = True):\n",
    "        super().__init__(clear_json_attrs=clear_json_attrs, slate_id=None)\n",
    "\n",
    "        self.auth_header['authorization'] = bearer_token\n",
    "\n",
    "        self.url_slates_available = (\n",
    "            'https://stats.underdogfantasy.com/v1/sports/nfl/slates'\n",
    "        )\n",
    "        self.url_slates_completed = (\n",
    "            'https://api.underdogfantasy.com/v2/user/completed_slates'\n",
    "        )\n",
    "        self.url_slates_settled = (\n",
    "            'https://api.underdogfantasy.com/v1/user/sports/nfl/settled_slates'\n",
    "        )\n",
    "        self.url_scoring_types = (\n",
    "            'https://stats.underdogfantasy.com/v1/scoring_types'\n",
    "        )\n",
    "        self.url_contest_styles = (\n",
    "            'https://stats.underdogfantasy.com/v1/contest_styles'\n",
    "        )\n",
    "\n",
    "        self.df_slates_available = None\n",
    "        self.df_slates_completed = None\n",
    "        self.df_slates_settled = None\n",
    "        self.df_scoring_types = None\n",
    "        self.df_contest_styles = None\n",
    "\n",
    "        self.json = {}\n",
    "\n",
    "    def create_df_slates_available(self, headers: dict=None, clear_json: bool=False,\n",
    "    update_attr: bool=False) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Creates a slate level df of all slates that are available to \n",
    "        draft in.\n",
    "        \"\"\"\n",
    "\n",
    "        df = self._create_df_slates(\n",
    "            self.url_slates_available, 'slates_available',\n",
    "            headers=headers, clear_json=clear_json\n",
    "        )\n",
    "\n",
    "        if update_attr:\n",
    "            self.df_slates_available = df\n",
    "\n",
    "        return df\n",
    "\n",
    "    def create_df_slates_completed(self, headers: dict=None, clear_json: bool=False,\n",
    "    update_attr: bool=False) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Creates a slate level df of all slates that are completed.\n",
    "        \"\"\"\n",
    "\n",
    "        df = self._create_df_slates(\n",
    "            self.url_slates_completed, 'slates_completed',\n",
    "            headers=headers, clear_json=clear_json\n",
    "        )\n",
    "\n",
    "        if update_attr:\n",
    "            self.df_slates_completed = df\n",
    "\n",
    "        return df\n",
    "\n",
    "    def create_df_slates_settled(self, headers: dict=None, clear_json: bool=False,\n",
    "    update_attr: bool=False) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Creates a slate level df of all slates that are settled.\n",
    "        \"\"\"\n",
    "\n",
    "        df = self._create_df_slates(\n",
    "            self.url_slates_settled, 'slates_settled',\n",
    "            headers=headers, clear_json=clear_json\n",
    "        )\n",
    "\n",
    "        if update_attr:\n",
    "            self.df_slates_settled = df\n",
    "\n",
    "        return df\n",
    "\n",
    "    def create_df_scoring_types(self, headers: dict=None, clear_json: bool=False,\n",
    "    update_attr: bool=False) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Creates a scoring type level df with the scoring types of all existing\n",
    "        NFL contests.\n",
    "\n",
    "        Notes:\n",
    "            - This is needed to automate the \"appearances\" (i.e. draft rank)\n",
    "            pull which uses the id as part of the url string\n",
    "            - 'display_stats' contains more descriptive information about each\n",
    "            scoring_type, but that data isn't needed now and would take some\n",
    "            time to pull out and structure.\n",
    "        \"\"\"\n",
    "\n",
    "        if headers is None:\n",
    "            headers=self.auth_header\n",
    "        \n",
    "        self.json['scoring_types'] = self.read_in_site_data(\n",
    "            self.url_scoring_types, headers=self.auth_header\n",
    "        )\n",
    "\n",
    "        df = self.create_scraped_data_df(self.json['scoring_types'][\"scoring_types\"])\n",
    "\n",
    "        df = df.loc[df['sport_id'] == 'NFL']\n",
    "\n",
    "        if update_attr:\n",
    "            self.df_scoring_types = df\n",
    "\n",
    "        if clear_json:\n",
    "            del self.json['scoring_types']\n",
    "\n",
    "        return df\n",
    "\n",
    "    def create_df_contest_styles(self, headers: dict=None, clear_json: bool=False,\n",
    "    update_attr: bool=False) -> pd.DataFrame:\n",
    "\n",
    "        if headers is None:\n",
    "            headers=self.auth_header\n",
    "        \n",
    "        self.json['contest_styles'] = self.read_in_site_data(\n",
    "            self.url_contest_styles, headers=self.auth_header\n",
    "        )\n",
    "\n",
    "        df = self.create_scraped_data_df(self.json['contest_styles'][\"contest_styles\"])\n",
    "\n",
    "        df = df.loc[df['sport_id'] == 'NFL']\n",
    "\n",
    "        if update_attr:\n",
    "            self.df_contest_styles = df\n",
    "\n",
    "        if clear_json:\n",
    "            del self.json['contest_styles'] \n",
    "\n",
    "        return df\n",
    "\n",
    "    def get_slate_ids(self, slate_type: str):\n",
    "        \"\"\"\n",
    "        Returns a list that contains each slate id for the slate_type.\n",
    "        slate_type must be 'available', 'completed', or 'settled'.\n",
    "        \"\"\"\n",
    "\n",
    "        attr = 'df_slates_' + slate_type.strip()\n",
    "        method_name = 'create_df_slates_' + slate_type.strip()\n",
    "\n",
    "        # NEED TO DOUBLE CHECK THIS - \n",
    "        if self.__dict__[attr] is None:\n",
    "            df = getattr(self, method_name)()\n",
    "        else:\n",
    "            df = self.__dict__[attr]\n",
    "\n",
    "        slates = list(df['id'])\n",
    "\n",
    "        return slates        \n",
    "\n",
    "    def _create_df_slates(self, url_slate: str, slate_type: str,\n",
    "        headers: dict=None, clear_json: bool=False\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\" \n",
    "        Helper method that can be used to create a df for any slate type.\n",
    "        \"\"\"\n",
    "\n",
    "        if headers is None:\n",
    "            headers=self.auth_header\n",
    "        \n",
    "        self.json[slate_type] = self.read_in_site_data(\n",
    "            url_slate, headers=self.auth_header\n",
    "        )\n",
    "\n",
    "        df = self.create_scraped_data_df(self.json[slate_type][\"slates\"])\n",
    "\n",
    "        if clear_json:\n",
    "            del self.json[slate_type]\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "site = ContestRefs(bearer_token)\n",
    "leagues = LeagueData(['1eafd8e3-c601-4363-896d-41050d37127c'], bearer_token)\n",
    "user = UserData(bearer_token)\n",
    "\n",
    "# site.create_df_slates_completed(headers=auth_header)\n",
    "# site.create_df_scoring_types()\n",
    "# site.create_df_contest_styles()\n",
    "\n",
    "# df = site.create_df_contest_styles()\n",
    "\n",
    "# df.loc[df['status'] == 'active']\n",
    "\n",
    "\n",
    "# leagues.create_df_drafts()\n",
    "\n",
    "# user.create_df_all_leagues()\n",
    "\n",
    "site.build_all_dfs()\n",
    "\n",
    "# site.read_in_site_data(site.url_slates_available, headers=None)\n",
    "\n",
    "# site.json.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"event\":\"pusher:connection_established\",\"data\":\"{\\\"socket_id\\\":\\\"16587.5347409\\\"}\"}\n",
      "{\"event\":\"connection_established\",\"data\":\"{\\\"socket_id\\\":\\\"16587.5347409\\\"}\"}\n",
      "{\"event\":\"pusher:error\",\"data\":{\"code\":null,\"message\":\"Pusher protocol versions <= 3 have been deprecated. Please update your client library.\"}}\n",
      "{\"event\":\"entry_count_change\",\"data\":\"{\\\"id\\\":\\\"e39cd77c-9105-47c1-aa05-c22b80156c05\\\",\\\"draft_type\\\":\\\"fast\\\",\\\"entry_count\\\":2,\\\"status\\\":\\\"filled\\\"}\",\"channel\":\"draft-e39cd77c-9105-47c1-aa05-c22b80156c05\"}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10665/1182404202.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mws\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Python-Projects/UD-Draft-Model/Repo-Work/UD-Draft-Model/venv/lib/python3.8/site-packages/websocket/_core.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    360\u001b[0m         \"\"\"\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m             \u001b[0mopcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mopcode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mABNF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOPCODE_TEXT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Python-Projects/UD-Draft-Model/Repo-Work/UD-Draft-Model/venv/lib/python3.8/site-packages/websocket/_core.py\u001b[0m in \u001b[0;36mrecv_data\u001b[0;34m(self, control_frame)\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0mtuple\u001b[0m \u001b[0mof\u001b[0m \u001b[0moperation\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \"\"\"\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mopcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_data_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontrol_frame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mopcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Python-Projects/UD-Draft-Model/Repo-Work/UD-Draft-Model/venv/lib/python3.8/site-packages/websocket/_core.py\u001b[0m in \u001b[0;36mrecv_data_frame\u001b[0;34m(self, control_frame)\u001b[0m\n\u001b[1;32m    404\u001b[0m         \"\"\"\n\u001b[1;32m    405\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m             \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0misEnabledForTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m                 \u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"++Rcv raw: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Python-Projects/UD-Draft-Model/Repo-Work/UD-Draft-Model/venv/lib/python3.8/site-packages/websocket/_core.py\u001b[0m in \u001b[0;36mrecv_frame\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mABNF\u001b[0m \u001b[0mframe\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m         \"\"\"\n\u001b[0;32m--> 445\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msend_close\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSTATUS_NORMAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mb\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Python-Projects/UD-Draft-Model/Repo-Work/UD-Draft-Model/venv/lib/python3.8/site-packages/websocket/_abnf.py\u001b[0m in \u001b[0;36mrecv_frame\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0;31m# Header\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_received_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrsv1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrsv2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrsv3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Python-Projects/UD-Draft-Model/Repo-Work/UD-Draft-Model/venv/lib/python3.8/site-packages/websocket/_abnf.py\u001b[0m in \u001b[0;36mrecv_header\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrecv_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m         \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m         \u001b[0mb1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0mfin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb1\u001b[0m \u001b[0;34m>>\u001b[0m \u001b[0;36m7\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Python-Projects/UD-Draft-Model/Repo-Work/UD-Draft-Model/venv/lib/python3.8/site-packages/websocket/_abnf.py\u001b[0m in \u001b[0;36mrecv_strict\u001b[0;34m(self, bufsize)\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0;31m# buffers allocated and then shrunk, which results in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m             \u001b[0;31m# fragmentation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m             \u001b[0mbytes_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16384\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshortage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0mshortage\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Python-Projects/UD-Draft-Model/Repo-Work/UD-Draft-Model/venv/lib/python3.8/site-packages/websocket/_core.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, bufsize)\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mWebSocketConnectionClosedException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Python-Projects/UD-Draft-Model/Repo-Work/UD-Draft-Model/venv/lib/python3.8/site-packages/websocket/_socket.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(sock, bufsize)\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mbytes_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0mbytes_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mWebSocketTimeoutException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Connection timed out\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Python-Projects/UD-Draft-Model/Repo-Work/UD-Draft-Model/venv/lib/python3.8/site-packages/websocket/_socket.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLWantReadError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1224\u001b[0m                     \u001b[0;34m\"non-zero flags not allowed in calls to recv() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m                     self.__class__)\n\u001b[0;32m-> 1226\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1227\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1099\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1101\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1102\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import websocket\n",
    "import json\n",
    "\n",
    "headers = {\n",
    "    'Pragma': 'no-cache',\n",
    "    'Origin': 'https://underdogfantasy.com',\n",
    "    'Accept-Language': 'en-US,en;q=0.9',\n",
    "    'Sec-WebSocket-Key': 'GbG2TtxcWxxrM1aDWJ+rAA==',\n",
    "    'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.51 Safari/537.36',\n",
    "    'Upgrade': 'websocket',\n",
    "    'Sec-WebSocket-Extensions': 'permessage-deflate; client_max_window_bits',\n",
    "    'Cache-Control': 'no-cache',\n",
    "    'Connection': 'Upgrade',\n",
    "    'Sec-WebSocket-Version': '13',\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'protocol': '7',\n",
    "    'client': 'js',\n",
    "    'version': '7.0.6',\n",
    "    'flash': 'false',\n",
    "}\n",
    "\n",
    "# response = requests.get('wss://ws-us2.pusher.com/app/038c354984811c43f658', \n",
    "#     params=params,\n",
    "#     headers=headers\n",
    "# )\n",
    "\n",
    "ws = websocket.create_connection('wss://ws-us2.pusher.com/app/038c354984811c43f658',\n",
    "    headers=headers\n",
    "    , params=params\n",
    ")\n",
    "\n",
    "message = {\n",
    "    \"event\":\"pusher:subscribe\",\n",
    "    \"data\":{\"auth\":\"\",\"channel\":\"draft-e39cd77c-9105-47c1-aa05-c22b80156c05\"}\n",
    "}\n",
    "ws.send(json.dumps(message))\n",
    "\n",
    "# Printing all the result\n",
    "while True:\n",
    "    try:\n",
    "        result = ws.recv()\n",
    "        print(result)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dc4492b5efc155a9cdcda09d43a20d3fd18b0b2d151b680a1414de88ecead1b9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
