{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\n",
    "class BaseData:\n",
    "    def __init__(self, clear_json_attrs: bool=True):\n",
    "        self._clear_json_attrs = clear_json_attrs\n",
    "\n",
    "        # user-agent and/or accept headers sometimes required\n",
    "        self.auth_header = {\n",
    "            \"accept\": \"application/json\",\n",
    "            \"user-agent\": \"Mozilla/5.0 (X11; Linux x86_64) \\\n",
    "                                AppleWebKit/537.36 (KHTML, like Gecko) \\\n",
    "                                Chrome/99.0.4844.51 Safari/537.36\",\n",
    "        }\n",
    "\n",
    "        self._player_scores_wk_1_id = 78\n",
    "        self._player_scores_wk_last_id = 78 + 17\n",
    "\n",
    "    def build_all_dfs(self, sleep_time: int=0):\n",
    "        \"\"\"\n",
    "        Overwrites every 'df_' attribute with a df that is created by running the\n",
    "        'create_' method that matches it. This serves as the primary method\n",
    "        for building the dfs associated with the class\n",
    "        \"\"\"\n",
    "\n",
    "        attrs = [attr for attr in dir(self) if attr.startswith(\"df_\")]\n",
    "        for attr in attrs:\n",
    "            method_name = \"create_\" + attr\n",
    "            self.__dict__[attr] = getattr(self, method_name)()\n",
    "            try:\n",
    "                self.__dict__[attr] = getattr(self, method_name)()\n",
    "            except:\n",
    "                print(getattr(self, method_name), \"failed to run\")\n",
    "\n",
    "            if sleep_time > 0:\n",
    "                time.sleep(sleep_time)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        if self._clear_json_attrs == True:\n",
    "            self.clear_json_attrs()\n",
    "\n",
    "    def clear_json_attrs(self):\n",
    "        \"\"\"\n",
    "        Clears all the atttributes that hold the json data pulled from the API.\n",
    "        By default, this is executed when the build_all_dfs method is run\n",
    "        \"\"\"\n",
    "\n",
    "        attrs = [attr for attr in dir(self) if attr.startswith(\"json\")]\n",
    "        for attr in attrs:\n",
    "            self.__dict__[attr] = {}\n",
    "\n",
    "    def read_in_site_data(self, url, headers: dict=None) -> dict:\n",
    "        \"\"\"Pulls in the raw data from the API and returns it as a dict\"\"\"\n",
    "\n",
    "        if headers is None:\n",
    "            headers = {}\n",
    "\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        site_data = response.json()\n",
    "\n",
    "        return site_data\n",
    "\n",
    "    def create_scraped_data_df(self, scraped_data: list) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Converts a list of dictionaries into a df where the keys of the dicts are\n",
    "        used for the columns and the values are placed in the rows.\n",
    "        NOTE: this assumes the keys in all dicts are the same.\n",
    "        \"\"\"\n",
    "\n",
    "        # Get the dicionary keys to identify the columns of the list for each output dict key\n",
    "        output_data_cols = []\n",
    "        for output_data_col in scraped_data[0].keys():\n",
    "            output_data_cols.append(output_data_col)\n",
    "\n",
    "        final_data_dict = {\"columns\": output_data_cols}\n",
    "        for data_dict_id, data in enumerate(scraped_data):\n",
    "            all_data_elements = []\n",
    "            for output_data_col in output_data_cols:\n",
    "                try:\n",
    "                    data_element = data[output_data_col]\n",
    "                except:\n",
    "                    # Note: This should probably be conditional on the data type, \n",
    "                    # but just using N/A for now.\n",
    "                    data_element = \"N/A\"\n",
    "\n",
    "                all_data_elements.append(data_element)\n",
    "\n",
    "            final_data_dict[data_dict_id] = all_data_elements\n",
    "\n",
    "        final_data_df = self._convert_data_dict_to_df(final_data_dict)\n",
    "\n",
    "        return final_data_df\n",
    "\n",
    "    def _convert_data_dict_to_df(self, scraped_data_dict: dict) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Converts the dict from the create_scraped_data_dict function to a df\n",
    "        NOTE: The input dict takes the following form:\n",
    "        {'columns': [<column names>], 1: [<column values>], 2: [<column_values>], ...}\n",
    "        \"\"\"\n",
    "\n",
    "        columns = scraped_data_dict[\"columns\"]\n",
    "\n",
    "        data_keys = list(scraped_data_dict.keys())[1:]\n",
    "\n",
    "        data_for_df = []\n",
    "        for data_key in data_keys:\n",
    "            data = scraped_data_dict[data_key]\n",
    "\n",
    "            data_for_df.append(data)\n",
    "\n",
    "        final_df = pd.DataFrame(data=data_for_df, columns=columns)\n",
    "\n",
    "        return final_df\n",
    "\n",
    "    def _create_week_id_mapping(self) -> pd.DataFrame:\n",
    "        \"\"\"Creates a map between the APIs Week ID and the actual Week number\"\"\"\n",
    "\n",
    "        wk_numbers = []\n",
    "        wk_ids = []\n",
    "        for wk_number, wk_id in enumerate(\n",
    "            range(self._player_scores_wk_1_id, self._player_scores_wk_last_id + 1)\n",
    "        ):\n",
    "            wk_numbers.append(wk_number + 1)\n",
    "            wk_ids.append(wk_id)\n",
    "\n",
    "        mapping = {\"week_number\": wk_numbers, \"week_id\": wk_ids}\n",
    "        df_mapping = pd.DataFrame(data=mapping)\n",
    "\n",
    "        return df_mapping\n",
    "\n",
    "\n",
    "class DraftsDetail(BaseData):\n",
    "    \"\"\"Compiles all major league specific data into dataframes\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, league_ids: list, bearer_token: str, clear_json_attrs: bool = True\n",
    "    ):\n",
    "        super().__init__(clear_json_attrs=clear_json_attrs)\n",
    "\n",
    "        self.auth_header['authorization'] = bearer_token\n",
    "\n",
    "        self.league_ids = league_ids\n",
    "\n",
    "        self.url_drafts = {}\n",
    "        self.url_weekly_scores = {}\n",
    "        for league_id in league_ids:\n",
    "            url_draft = \"https://api.underdogfantasy.com/v2/drafts/\" + league_id\n",
    "            url_weekly_scores = (\n",
    "                \"https://api.underdogfantasy.com/v1/drafts/\"\n",
    "                + league_id\n",
    "                + \"/weekly_scores\"\n",
    "            )\n",
    "\n",
    "            self.url_drafts[league_id] = url_draft\n",
    "            self.url_weekly_scores[league_id] = url_weekly_scores\n",
    "\n",
    "        self.json_drafts = {}\n",
    "        self.json_weekly_scores = {}\n",
    "\n",
    "        self.df_drafts = pd.DataFrame()\n",
    "        self.df_draft_entries = pd.DataFrame()\n",
    "        self.df_weekly_scores = pd.DataFrame()\n",
    "\n",
    "    def create_df_drafts(self) -> pd.DataFrame:\n",
    "        dfs = []\n",
    "        for league_id in self.league_ids:\n",
    "            df = self._create_df_draft_ind_league(league_id)\n",
    "            dfs.append(df)\n",
    "\n",
    "        final_df = pd.concat(dfs)\n",
    "\n",
    "        return final_df\n",
    "\n",
    "    def create_df_draft_entries(self) -> pd.DataFrame:\n",
    "        dfs = []\n",
    "        for league_id in self.league_ids:\n",
    "            df = self._create_df_draft_entries_ind_league(league_id)\n",
    "            dfs.append(df)\n",
    "\n",
    "        final_df = pd.concat(dfs)\n",
    "\n",
    "        return final_df\n",
    "\n",
    "    def create_df_weekly_scores(self) -> pd.DataFrame:\n",
    "        dfs = []\n",
    "        for league_id in self.league_ids:\n",
    "            df = self._create_df_weekly_scores_ind_league(league_id)\n",
    "            dfs.append(df)\n",
    "\n",
    "        final_df = pd.concat(dfs)\n",
    "        final_df.reset_index(inplace=True)\n",
    "\n",
    "        week_mapping = self._create_week_id_mapping()\n",
    "        final_df = pd.merge(final_df, week_mapping, on=\"week_id\")\n",
    "\n",
    "        return final_df\n",
    "\n",
    "    def _create_df_draft_ind_league(self, league_id: str) -> pd.DataFrame:\n",
    "        self.json_drafts[league_id] = self.read_in_site_data(\n",
    "            self.url_drafts[league_id], headers=self.auth_header\n",
    "        )\n",
    "        scraped_data = self.json_drafts[league_id][\"draft\"][\"picks\"]\n",
    "\n",
    "        initial_scraped_df = self.create_scraped_data_df(scraped_data)\n",
    "        initial_scraped_df.drop([\"projection_average\"], axis=1, inplace=True)\n",
    "\n",
    "        initial_scraped_df[\"draft_id\"] = league_id\n",
    "\n",
    "        return initial_scraped_df\n",
    "\n",
    "    def _create_df_draft_entries_ind_league(self, league_id: str) -> pd.DataFrame:\n",
    "        \"\"\" \n",
    "        Creates a df of all users in the draft, sorted by pick order.\n",
    "        \"\"\"\n",
    "\n",
    "        json = self.read_in_site_data(url, self.auth_header)\n",
    "\n",
    "        df_entries = self.create_scraped_data_df(json['draft']['draft_entries'])\n",
    "        df_users = self.create_scraped_data_df(json['draft']['users'])\n",
    "\n",
    "        df_users.rename(columns={'id': 'user_id'}, inplace=True)\n",
    "        df_users = df_users[['user_id', 'username']]\n",
    "\n",
    "        df = pd.merge(df_entries, df_users, how='left', on='user_id')\n",
    "        df = df.sort_values(by='pick_order').reset_index(drop=True)\n",
    "\n",
    "        df['draft_id'] = league_id\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _create_df_weekly_scores_ind_league(self, league_id: str) -> pd.DataFrame:\n",
    "        self.json_weekly_scores[league_id] = self.read_in_site_data(\n",
    "            self.url_weekly_scores[league_id], headers=self.auth_header\n",
    "        )\n",
    "        scraped_data = self.json_weekly_scores[league_id][\"draft_weekly_scores\"]\n",
    "\n",
    "        initial_scraped_df = self.create_scraped_data_df(scraped_data)\n",
    "\n",
    "        weekly_scores = self._pull_out_weekly_scores(initial_scraped_df)\n",
    "\n",
    "        initial_scraped_df.drop([\"week\", \"draft_entries_points\"], axis=1, inplace=True)\n",
    "\n",
    "        final_scraped_df = pd.merge(\n",
    "            left=weekly_scores, right=initial_scraped_df, on=\"id\", how=\"left\"\n",
    "        )\n",
    "        final_scraped_df.drop([\"id\"], axis=1, inplace=True)\n",
    "\n",
    "        return final_scraped_df\n",
    "\n",
    "    def _pull_out_weekly_scores(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Each row represents one week where each teams score is contained\n",
    "        within a dicitonary for that week. This pulls those scores out and\n",
    "        puts them in a Team/Week level df\n",
    "        \"\"\"\n",
    "\n",
    "        df = df.copy()\n",
    "\n",
    "        all_weekly_scores = []\n",
    "        for index, row in df.iterrows():\n",
    "            row_id = row[\"id\"]\n",
    "            week_id = row[\"week\"][\"id\"]\n",
    "            status = row[\"week\"][\"status\"]\n",
    "            points_dict = row[\"draft_entries_points\"]\n",
    "\n",
    "            for user_id, points in points_dict.items():\n",
    "                weekly_scores = [row_id, week_id, status, user_id, points]\n",
    "\n",
    "                all_weekly_scores.append(weekly_scores)\n",
    "\n",
    "        columns = [\"id\", \"week_id\", \"status\", \"user_id\", \"total_points\"]\n",
    "        df = pd.DataFrame(data=all_weekly_scores, columns=columns)\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "class DraftsActive(BaseData):\n",
    "\n",
    "    url = 'https://api.underdogfantasy.com/v3/user/active_drafts'\n",
    "\n",
    "    def __init__(self, bearer_token: str, clear_json_attrs: bool=True):\n",
    "        super().__init__(clear_json_attrs=clear_json_attrs)\n",
    "\n",
    "        self.auth_header['authorization'] = bearer_token\n",
    "\n",
    "        self.json = {}\n",
    "        self.df_active_drafts = None\n",
    "\n",
    "    def create_df_active_drafts(self) -> pd.DataFrame:\n",
    "        \"\"\" \n",
    "        Creates a draft level df of all active drafts. \n",
    "        \"\"\"\n",
    "\n",
    "        self.json = self.read_in_site_data(\n",
    "            DraftsActive.url, headers=self.auth_header\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            df = self.create_scraped_data_df(self.json[\"drafts\"])\n",
    "            df = self._add_contest_refs(df)\n",
    "        except IndexError:\n",
    "            print(f'No data found in {DraftsActive.url} - no df will be returned')\n",
    "            df = None\n",
    "\n",
    "        if self.clear_json_attrs:\n",
    "            self.json = {}\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _add_contest_refs(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\" \n",
    "        Scoring type required to get the rankings (appearances) used\n",
    "        for the draft and rounds needed to build a draft shell.\n",
    "        \"\"\"\n",
    "\n",
    "        contest_refs = ContestRefs()\n",
    "        df_styles = contest_refs.create_df_contest_styles()\n",
    "        df_styles = df_styles[['id', 'scoring_type_id', 'rounds']]\n",
    "        df_styles.rename(columns={'id': 'contest_style_id'}, inplace=True)\n",
    "\n",
    "        df = pd.merge(df, df_styles, how='left', on='contest_style_id')\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "class Drafts(BaseData):\n",
    "    \"\"\"\n",
    "    Compiles all completed or settled draft level data for a slate.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        bearer_token: str,\n",
    "        slate,\n",
    "        clear_json_attrs: bool=True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Note: This requires the user-agent header - Should be able to grab this\n",
    "        with the bearer token, but hard coding for now\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(clear_json_attrs=clear_json_attrs)\n",
    "\n",
    "        self.auth_header['authorization'] = bearer_token\n",
    "        self.slate = slate\n",
    "\n",
    "        url_suffix = f'/{self.slate.slate_type}_drafts'\n",
    "        self.url_base_leagues = (\n",
    "            \"https://api.underdogfantasy.com/v2/user/slates/\"\n",
    "            + self.slate.id\n",
    "            + url_suffix\n",
    "        )\n",
    "        self.url_tourney_league_ids = (\n",
    "            \"https://api.underdogfantasy.com/v1/user/slates/\"\n",
    "            + self.slate.id\n",
    "            + \"/tournament_rounds\"\n",
    "        )\n",
    "\n",
    "        self.json_leagues = {}\n",
    "        self.df_all_leagues = pd.DataFrame()\n",
    "\n",
    "    def create_df_all_leagues(self, league_urls: list=None) -> pd.DataFrame:\n",
    "        if league_urls is None:\n",
    "            league_urls = self.get_league_urls()\n",
    "\n",
    "        leagues = []\n",
    "        for i, league_url in enumerate(league_urls):\n",
    "            df = self._create_df_leagues(league_url, \"league_\" + str(i + 1))\n",
    "            leagues.append(df)\n",
    "\n",
    "        df_all_leagues = pd.concat(leagues)\n",
    "        df_all_leagues.reset_index(inplace=True)\n",
    "        df_all_leagues.drop(columns=[\"index\"], inplace=True)\n",
    "\n",
    "        return df_all_leagues\n",
    "\n",
    "    def get_league_urls(self) -> list:\n",
    "        \"\"\" \n",
    "        Creates a list of all urls which store draft level data for the slate.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            tourney_league_urls = self._create_tourney_league_urls()\n",
    "        except IndexError:\n",
    "            tourney_league_urls = []\n",
    "\n",
    "        if self.slate.draft_count > 0:\n",
    "            base_league_url = [self.url_base_leagues]\n",
    "        else:\n",
    "            base_league_url = []\n",
    "\n",
    "        urls = base_league_url + tourney_league_urls\n",
    "\n",
    "        return urls        \n",
    "\n",
    "    def _create_df_leagues(self, url_base: str, json_leagues_key: str) -> pd.DataFrame:\n",
    "        self.json_leagues[json_leagues_key] = self._create_json_leagues(url_base)\n",
    "        scraped_data = self.json_leagues[json_leagues_key]\n",
    "\n",
    "        leagues_df_list = []\n",
    "        for leagues_page in scraped_data.values():\n",
    "            leagues_page_df = self.create_scraped_data_df(leagues_page[\"drafts\"])\n",
    "            leagues_df_list.append(leagues_page_df)\n",
    "\n",
    "        leagues_df = pd.concat(leagues_df_list)\n",
    "\n",
    "        return leagues_df\n",
    "\n",
    "    def _create_json_leagues(self, url_base: str) -> dict:\n",
    "        \"\"\"\n",
    "        Loops through all the different pages that contain the league level data\n",
    "        and stores each as an entry in a dict\n",
    "        \"\"\"\n",
    "\n",
    "        url_exists = True\n",
    "        i = 1\n",
    "        leagues_json_dict = {}\n",
    "        while url_exists:\n",
    "            if i == 1:\n",
    "                url = url_base\n",
    "            else:\n",
    "                url = url_base + \"?page=\" + str(i)\n",
    "\n",
    "            leagues = self.read_in_site_data(url, headers=self.auth_header)\n",
    "\n",
    "            if len(leagues[\"drafts\"]) > 0:\n",
    "                leagues_json_dict[\"page_\" + str(i)] = leagues\n",
    "            else:\n",
    "                url_exists = False\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        return leagues_json_dict\n",
    "\n",
    "    def _create_df_tourney_league_ids(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Tournament leagues (i.e. Puppy 1, Puppy 2, etc.) require the ID of the\n",
    "        tourney in order to find all entries in it - This creates of all tourney\n",
    "        IDs that has at least one entry\n",
    "        \"\"\"\n",
    "\n",
    "        json_tourney_league_ids = self.read_in_site_data(\n",
    "            self.url_tourney_league_ids, headers=self.auth_header\n",
    "        )\n",
    "        scraped_data = json_tourney_league_ids[\"tournament_rounds\"]\n",
    "\n",
    "        initial_scraped_df = self.create_scraped_data_df(scraped_data)\n",
    "\n",
    "        # Pulling out the 'id' from the 'tournament' dict in case this is whats needed\n",
    "        tournament_col = initial_scraped_df[\"tournament\"].to_list()\n",
    "        tournament_df = self.create_scraped_data_df(tournament_col)\n",
    "        tournament_df.rename(columns={\"id\": \"tournament_id\"}, inplace=True)\n",
    "        tournament_df = tournament_df[\"tournament_id\"]\n",
    "\n",
    "        initial_scraped_df.drop([\"tournament\"], axis=1, inplace=True)\n",
    "        final_df = initial_scraped_df.join(tournament_df)\n",
    "\n",
    "        return final_df\n",
    "\n",
    "    def _create_tourney_league_urls(self) -> list:\n",
    "        \"\"\"\n",
    "        Creates a list of all the URLs that contain entries\n",
    "        \"\"\"\n",
    "        \n",
    "        tourney_league_ids = list(self._create_df_tourney_league_ids()[\"id\"])\n",
    "\n",
    "        base_url = \"https://api.underdogfantasy.com/v1/user/tournament_rounds/\"\n",
    "        tourney_league_urls = []\n",
    "        for tourney_league_id in tourney_league_ids:\n",
    "            tourney_league_url = base_url + tourney_league_id + \"/drafts\"\n",
    "\n",
    "            tourney_league_urls.append(tourney_league_url)\n",
    "\n",
    "        return tourney_league_urls\n",
    "\n",
    "\n",
    "class Slates(BaseData):\n",
    "    \"\"\" \n",
    "    Compiles all available and completed slates for a specific slate type. \n",
    "    \"\"\"\n",
    "\n",
    "    url_slates_available = (\n",
    "        'https://stats.underdogfantasy.com/v1/sports/nfl/slates'\n",
    "    )\n",
    "    url_slates_completed = (\n",
    "        'https://api.underdogfantasy.com/v2/user/completed_slates'\n",
    "    )\n",
    "    url_slates_settled = (\n",
    "        'https://api.underdogfantasy.com/v1/user/sports/nfl/settled_slates'\n",
    "    )\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        bearer_token: str,\n",
    "        slate_type: str,\n",
    "        clear_json_attrs: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        slate_type must be 'available', 'completed', or 'settled'\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(clear_json_attrs=clear_json_attrs, slate_id=None)\n",
    "\n",
    "        self.auth_header['authorization'] = bearer_token\n",
    "        self.slate_type = slate_type\n",
    "\n",
    "        self.df_slates = None\n",
    "        self.slates = []\n",
    "\n",
    "        self.json = {}\n",
    "\n",
    "    def create_df_slates(self, headers: dict=None, clear_json: bool=False\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\" \n",
    "        Creates df of all the slates found.\n",
    "        \"\"\"\n",
    "\n",
    "        if headers is None:\n",
    "            headers=self.auth_header\n",
    "\n",
    "        url = self._get_url()\n",
    "        \n",
    "        self.json = self.read_in_site_data(url, headers=self.auth_header)\n",
    "\n",
    "        try:\n",
    "            df = self.create_scraped_data_df(self.json[\"slates\"])\n",
    "\n",
    "            # This is stored as a list, but seems to always only contain\n",
    "            # one id.\n",
    "            df['contest_style_ids'] = (\n",
    "                df['contest_style_ids'].apply(lambda x: x[0])\n",
    "            )\n",
    "        except IndexError:\n",
    "            print(f'No data found in {url} - no df will be returned')\n",
    "            df = None\n",
    "\n",
    "        self.slates = self._create_slates(df)\n",
    "\n",
    "        if clear_json:\n",
    "            self.json = {}\n",
    "\n",
    "        return df   \n",
    "\n",
    "    def _get_url(self) -> str:\n",
    "        \"\"\"\n",
    "        Selects the url to be used based on the slate_type.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.slate_type == 'available':\n",
    "            url = Slates.url_slates_available\n",
    "        elif self.slate_type == 'completed':\n",
    "            url = Slates.url_slates_completed\n",
    "        elif self.slate_type == 'settled':\n",
    "            url = Slates.url_slates_settled\n",
    "\n",
    "        return url\n",
    "\n",
    "    def _create_slates(self, df_slates: pd.DataFrame) -> list:\n",
    "        \"\"\" \n",
    "        Creates a list of Slate objects.\n",
    "        \"\"\"\n",
    "\n",
    "        slates = []\n",
    "        for i in range(len(df_slates)):\n",
    "            slate = Slate(df_slates.iloc[i], self.slate_type)\n",
    "\n",
    "            slates.append(slate)\n",
    "\n",
    "        return slates\n",
    "\n",
    "\n",
    "class Slate:\n",
    "\n",
    "    def __init__(self, df_slate: pd.Series, slate_type):\n",
    "        self.id = df_slate['id']\n",
    "        self.contest_style_ids = df_slate['contest_style_ids']\n",
    "        self.description = df_slate['description']\n",
    "        self.title = df_slate['title']\n",
    "        self.slate_type = slate_type\n",
    "\n",
    "        try:\n",
    "            self.draft_count = df_slate['draft_count']\n",
    "            self.tournament_draft_count = df_slate['tournament_draft_count']\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "class ReferenceData(BaseData):\n",
    "    \"\"\"Compiles all major reference data into dataframes\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        slate_id: str,\n",
    "        scoring_type_id: str,\n",
    "        clear_json_attrs: bool=True, \n",
    "    ):\n",
    "        super().__init__(clear_json_attrs=clear_json_attrs)\n",
    "\n",
    "        self.slate_id = slate_id\n",
    "        self.scoring_type_id = scoring_type_id\n",
    "\n",
    "        # week ID seems right but can't find the correct url for it\n",
    "        # self._player_scores_wk_1_id = 78\n",
    "        self._player_scores_wk_1_id = 1186\n",
    "\n",
    "        self.url_players = (\n",
    "            'https://stats.underdogfantasy.com/v1/slates/'\n",
    "            + self.slate_id \n",
    "            + '/players'\n",
    "        )\n",
    "        self.url_appearances = (\n",
    "            'https://stats.underdogfantasy.com/v1/slates/'\n",
    "            + self.slate_id\n",
    "            + '/scoring_types/'\n",
    "            + self.scoring_type_id\n",
    "            + '/appearances'\n",
    "        )\n",
    "        self.url_teams = 'https://stats.underdogfantasy.com/v1/teams'\n",
    "\n",
    "        base_url_player_scores = 'https://stats.underdogfantasy.com/v1/weeks/'\n",
    "        end_url_player_scores = (\n",
    "            '/scoring_types/'\n",
    "            + self.scoring_type_id\n",
    "            + '/appearances'\n",
    "        )\n",
    "        self.urls_player_scores = {\n",
    "            \"player_scores_wk_\"\n",
    "            + str(i + 1): base_url_player_scores\n",
    "            + str(wk_id)\n",
    "            + end_url_player_scores\n",
    "            for i, wk_id in enumerate(\n",
    "                range(self._player_scores_wk_1_id, self._player_scores_wk_1_id + 17)\n",
    "            )\n",
    "        }\n",
    "\n",
    "        self.df_players = pd.DataFrame()\n",
    "        self.df_appearances = pd.DataFrame()\n",
    "        self.df_teams = pd.DataFrame()\n",
    "        self.df_players_master = pd.DataFrame()\n",
    "        self.df_player_scores = pd.DataFrame()\n",
    "\n",
    "    def build_all_dfs(self):\n",
    "        attrs = [attr for attr in dir(self) if attr.startswith(\"df_\")]\n",
    "        for attr in attrs:\n",
    "            if attr == \"df_players_master\":\n",
    "                pass\n",
    "            else:\n",
    "                try:\n",
    "                    method_name = \"create_\" + attr\n",
    "                    self.__dict__[attr] = getattr(self, method_name)()\n",
    "                except:\n",
    "                    print(getattr(self, method_name), \"failed to run\")\n",
    "\n",
    "        # This ensures the dfs it depends on are created\n",
    "        self.df_players_master = self.create_df_players_master()\n",
    "\n",
    "        if self._clear_json_attrs == True:\n",
    "            self.clear_json_attrs()\n",
    "\n",
    "    def create_df_players(self) -> pd.DataFrame:\n",
    "        self.json_players = self.read_in_site_data(\n",
    "            self.url_players, headers=self.auth_header\n",
    "        )\n",
    "\n",
    "        initial_scraped_df = self.create_scraped_data_df(self.json_players[\"players\"])\n",
    "        initial_scraped_df.drop([\"image_url\"], axis=1, inplace=True)\n",
    "        initial_scraped_df.rename(columns={\"id\": \"player_id\"}, inplace=True)\n",
    "\n",
    "        return initial_scraped_df\n",
    "\n",
    "    def create_df_appearances(self) -> pd.DataFrame:\n",
    "        self.json_appearances = self.read_in_site_data(\n",
    "            self.url_appearances, headers=self.auth_header\n",
    "        )\n",
    "\n",
    "        initial_scraped_df = self.create_scraped_data_df(\n",
    "            self.json_appearances[\"appearances\"]\n",
    "        )\n",
    "        initial_scraped_df.drop(\n",
    "            [\"latest_news_item_updated_at\", \"score\"], axis=1, inplace=True\n",
    "        )\n",
    "\n",
    "        # 'projection' column values are dicitionaries which can be converted to a df and merged\n",
    "        projection_col = initial_scraped_df[\"projection\"].to_list()\n",
    "        projection_df = self.create_scraped_data_df(projection_col)\n",
    "\n",
    "        projection_df.drop([\"id\", \"scoring_type_id\"], axis=1, inplace=True)\n",
    "        projection_df.rename(\n",
    "            columns={\"points\": \"season_projected_points\"}, inplace=True\n",
    "        )\n",
    "\n",
    "        final_df = pd.merge(\n",
    "            initial_scraped_df,\n",
    "            projection_df,\n",
    "            left_index=True,\n",
    "            right_index=True,\n",
    "            how=\"left\",\n",
    "        )\n",
    "        final_df.drop([\"projection\"], axis=1, inplace=True)\n",
    "\n",
    "        df_pos_map = self._create_position_mapping(final_df)\n",
    "        final_df = pd.merge(final_df, df_pos_map, on=\"position_id\", how=\"left\")\n",
    "\n",
    "        final_df.rename(columns={\"id\": \"appearance_id\"}, inplace=True)\n",
    "\n",
    "        return final_df\n",
    "\n",
    "    def create_df_teams(self) -> pd.DataFrame:\n",
    "        self.json_teams = self.read_in_site_data(\n",
    "            self.url_teams, headers=self.auth_header\n",
    "        )\n",
    "\n",
    "        initial_scraped_df = self.create_scraped_data_df(self.json_teams[\"teams\"])\n",
    "\n",
    "        keep_vars = [\"id\", \"abbr\", \"name\"]\n",
    "        final_df = initial_scraped_df[keep_vars].copy()\n",
    "\n",
    "        final_df.rename(columns={\"name\": \"team_name\", \"id\": \"team_id\"}, inplace=True)\n",
    "\n",
    "        return final_df\n",
    "\n",
    "    def create_df_players_master(self) -> pd.DataFrame:\n",
    "        \"\"\"Creates a master lookup for player attributes\"\"\"\n",
    "\n",
    "        if len(self.df_appearances) == 0:\n",
    "            self.df_appearances = self.create_df_appearances()\n",
    "\n",
    "        if len(self.df_players) == 0:\n",
    "            self.df_players = self.create_df_players()\n",
    "\n",
    "        if len(self.df_teams) == 0:\n",
    "            self.df_teams = self.create_df_teams()\n",
    "\n",
    "        # Team is more accurate in the df_players data and position from df_appearances\n",
    "        # reflects the posisiton at the time of the draft\n",
    "        df_appearances = self.df_appearances.drop([\"team_id\"], axis=1, inplace=False)\n",
    "        df_players = self.df_players.drop([\"position_id\"], axis=1, inplace=False)\n",
    "\n",
    "        final_df = pd.merge(df_appearances, df_players, on=\"player_id\", how=\"left\")\n",
    "\n",
    "        final_df = pd.merge(final_df, self.df_teams, on=\"team_id\", how=\"left\")\n",
    "\n",
    "        return final_df\n",
    "\n",
    "    def create_df_player_scores(self):\n",
    "        \"\"\"\n",
    "        This no longer appears to work due to either a change in the endpoint\n",
    "        or the starting point week id is wrong\n",
    "        \"\"\"\n",
    "\n",
    "        self.json_player_scores = {\n",
    "            \"player_scores_wk_\"\n",
    "            + str(i + 1): self.read_in_site_data(\n",
    "                self.urls_player_scores[\"player_scores_wk_\" + str(i + 1)],\n",
    "                headers=self.auth_header,\n",
    "            )\n",
    "            for i, wk_id in enumerate(\n",
    "                range(self._player_scores_wk_1_id, self._player_scores_wk_1_id + 17)\n",
    "            )\n",
    "        }\n",
    "\n",
    "        player_scores_df_list = []\n",
    "        for wk_id in range(1, 18):\n",
    "            if (\n",
    "                len(\n",
    "                    self.json_player_scores[\"player_scores_wk_\" + str(wk_id)][\n",
    "                        \"appearances\"\n",
    "                    ]\n",
    "                )\n",
    "                > 0\n",
    "            ):\n",
    "                player_scores_json = self.json_player_scores[\n",
    "                    \"player_scores_wk_\" + str(wk_id)\n",
    "                ]\n",
    "                player_scores_df = self._create_df_player_scores_one_wk(\n",
    "                    player_scores_json[\"appearances\"]\n",
    "                )\n",
    "                player_scores_df[\"week_number\"] = wk_id\n",
    "\n",
    "                player_scores_df_list.append(player_scores_df)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        player_scores_df = pd.concat(player_scores_df_list)\n",
    "        player_scores_df.reset_index(inplace=True)\n",
    "\n",
    "        return player_scores_df\n",
    "\n",
    "    def _create_df_player_scores_one_wk(self, scraped_data: list) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Each weeks player scores are contained in its own URL - this creates a df\n",
    "        of those scores for one week\n",
    "        \"\"\"\n",
    "\n",
    "        initial_scraped_df = self.create_scraped_data_df(scraped_data)\n",
    "        initial_scraped_df.drop([\"latest_news_item_updated_at\"], axis=1, inplace=True)\n",
    "\n",
    "        # 'projection' column values are dicitionaries which can be converted to a df and merged\n",
    "        projection_col = initial_scraped_df[\"projection\"].to_list()\n",
    "        projection_df = self.create_scraped_data_df(projection_col)\n",
    "\n",
    "        projection_df = projection_df[[\"points\"]]\n",
    "        projection_df.rename(columns={\"points\": \"projected_points\"}, inplace=True)\n",
    "\n",
    "        score_col = initial_scraped_df[\"score\"].to_list()\n",
    "        score_df = self.create_scraped_data_df(score_col)\n",
    "\n",
    "        score_df = score_df[[\"points\"]]\n",
    "        score_df.rename(columns={\"points\": \"actual_points\"}, inplace=True)\n",
    "\n",
    "        final_df = pd.merge(\n",
    "            initial_scraped_df,\n",
    "            projection_df,\n",
    "            left_index=True,\n",
    "            right_index=True,\n",
    "            how=\"left\",\n",
    "        )\n",
    "        final_df = pd.merge(\n",
    "            final_df, score_df, left_index=True, right_index=True, how=\"left\"\n",
    "        )\n",
    "\n",
    "        final_df.drop([\"projection\", \"score\"], axis=1, inplace=True)\n",
    "\n",
    "        return final_df\n",
    "\n",
    "    def _create_position_mapping(self, df_appearances: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Creates df that maps position to position_id since this cant be found in the API\n",
    "        \"\"\"\n",
    "\n",
    "        df_pos_map = df_appearances.copy()\n",
    "\n",
    "        df_pos_map[\"position\"] = df_pos_map[\"position_rank\"].str[0:2]\n",
    "        df_pos_map = df_pos_map[[\"position_id\", \"position\"]].loc[\n",
    "            df_pos_map[\"position\"].notnull()\n",
    "        ]\n",
    "        df_pos_map = df_pos_map.drop_duplicates(\n",
    "            subset=[\"position\", \"position_id\"], keep=\"first\"\n",
    "        )\n",
    "\n",
    "        return df_pos_map\n",
    "\n",
    "\n",
    "class ContestRefs(BaseData):\n",
    "    \"\"\" \n",
    "    Compiles all major contest related data into dataframes.\n",
    "    Note that this includes contests specific to a user (e.g. completed\n",
    "    slates, settled slates, etc.)\n",
    "    \"\"\"\n",
    "\n",
    "    url_scoring_types = 'https://stats.underdogfantasy.com/v1/scoring_types'\n",
    "    url_contest_styles = 'https://stats.underdogfantasy.com/v1/contest_styles'\n",
    "\n",
    "    def __init__(self, clear_json_attrs: bool=True):\n",
    "        super().__init__(clear_json_attrs=clear_json_attrs)\n",
    "\n",
    "        self.df_scoring_types = None\n",
    "        self.df_contest_styles = None\n",
    "\n",
    "        self.json = {}\n",
    "\n",
    "    def create_df_scoring_types(self, headers: dict=None, clear_json: bool=False,\n",
    "        update_attr: bool=False\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Creates a scoring type level df with the scoring types of all existing\n",
    "        NFL contests.\n",
    "\n",
    "        Notes:\n",
    "            - This is needed to automate the \"appearances\" (i.e. draft rank)\n",
    "            pull which uses the id as part of the url string\n",
    "            - 'display_stats' contains more descriptive information about each\n",
    "            scoring_type, but that data isn't needed now and would take some\n",
    "            time to pull out and structure.\n",
    "        \"\"\"\n",
    "\n",
    "        if headers is None:\n",
    "            headers=self.auth_header\n",
    "        \n",
    "        self.json['scoring_types'] = self.read_in_site_data(\n",
    "            ContestRefs.url_scoring_types, headers=self.auth_header\n",
    "        )\n",
    "\n",
    "        df = self.create_scraped_data_df(self.json['scoring_types'][\"scoring_types\"])\n",
    "\n",
    "        df = df.loc[df['sport_id'] == 'NFL']\n",
    "\n",
    "        if update_attr:\n",
    "            self.df_scoring_types = df\n",
    "\n",
    "        if clear_json:\n",
    "            del self.json['scoring_types']\n",
    "\n",
    "        return df\n",
    "\n",
    "    def create_df_contest_styles(self, headers: dict=None, clear_json: bool=False,\n",
    "    update_attr: bool=False) -> pd.DataFrame:\n",
    "\n",
    "        if headers is None:\n",
    "            headers=self.auth_header\n",
    "        \n",
    "        self.json['contest_styles'] = self.read_in_site_data(\n",
    "            ContestRefs.url_contest_styles, headers=self.auth_header\n",
    "        )\n",
    "\n",
    "        df = self.create_scraped_data_df(self.json['contest_styles'][\"contest_styles\"])\n",
    "\n",
    "        df = df.loc[df['sport_id'] == 'NFL']\n",
    "\n",
    "        if update_attr:\n",
    "            self.df_contest_styles = df\n",
    "\n",
    "        if clear_json:\n",
    "            del self.json['contest_styles'] \n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "def create_underdog_df_dict(bearer_token: str, sleep_time: int = 0) -> dict:\n",
    "    \"\"\"\n",
    "    Creates a dictionary of dfs containing the most relevant UD data\n",
    "    \n",
    "    TODO: Update to align with the refactored code.\n",
    "    \"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "    # ref_data = ReferenceData()\n",
    "    # ref_data.build_all_dfs()\n",
    "\n",
    "    # user_data = UserData(bearer_token)\n",
    "    # user_data.build_all_dfs()\n",
    "    # league_ids = list(user_data.df_all_leagues[\"id\"])\n",
    "\n",
    "    # league_data = LeagueData(league_ids, bearer_token)\n",
    "    # league_data.build_all_dfs(sleep_time=sleep_time)\n",
    "\n",
    "    # df_players_master = ref_data.df_players_master\n",
    "    # df_player_scores = ref_data.df_player_scores\n",
    "\n",
    "    # player_vars = [\n",
    "    #     \"appearance_id\",\n",
    "    #     \"player_id\",\n",
    "    #     \"position\",\n",
    "    #     \"team_name\",\n",
    "    #     \"first_name\",\n",
    "    #     \"last_name\",\n",
    "    # ]\n",
    "    # df_drafts = pd.merge(\n",
    "    #     league_data.df_drafts,\n",
    "    #     df_players_master[player_vars],\n",
    "    #     on=\"appearance_id\",\n",
    "    #     how=\"left\",\n",
    "    # )\n",
    "    # df_weekly_scores = league_data.df_weekly_scores\n",
    "\n",
    "    # final_dict = {\n",
    "    #     \"df_players_master\": df_players_master,\n",
    "    #     \"df_player_scores\": df_player_scores,\n",
    "    #     \"df_drafts\": df_drafts,\n",
    "    #     \"df_weekly_scores\": df_weekly_scores,\n",
    "    #     \"df_league_info\": user_data.df_all_leagues,\n",
    "    # }\n",
    "\n",
    "    # return final_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bearer eyJhbGciOiJIUzI1NiJ9.eyJqdGkiOiI0ZWNkMDVmZi1kYTExLTQwY2UtYTYwNS05ZDVmZjZlMGMwODciLCJzdWIiOiIwMzcxNzEzMS1lMDUzLTQ1MWQtOWJlNi0wOTc1NWY1ODc1YWUiLCJzY3AiOiJ1c2VyIiwiYXVkIjpudWxsLCJpYXQiOjE2NzczNTk1OTgsImV4cCI6MTY3OTk4OTM0NH0.mYlzfm_4BA89jRnW7GSi11yigzps8ibzUTcyx4g1Mio\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import UD_draft_model.scrapers.scrape_site.pull_bearer_token as pb\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 50)\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "\n",
    "### Variables to change ###\n",
    "chromedriver_path = \"/usr/bin/chromedriver\"\n",
    "username = input(\"Enter Underdog username: \")\n",
    "password = getpass.getpass()\n",
    "\n",
    "### Keep as is ###\n",
    "url = \"https://underdogfantasy.com/lobby\"\n",
    "bearer_token = pb.pull_bearer_token(url, chromedriver_path, username, password)\n",
    "\n",
    "print(bearer_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DraftsDetail(BaseData):\n",
    "    \"\"\"Compiles all major league specific data into dataframes\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, league_ids: list, bearer_token: str, clear_json_attrs: bool = True\n",
    "    ):\n",
    "        super().__init__(clear_json_attrs=clear_json_attrs)\n",
    "\n",
    "        self.auth_header['authorization'] = bearer_token\n",
    "\n",
    "        self.league_ids = league_ids\n",
    "\n",
    "        self.url_drafts = {}\n",
    "        self.url_weekly_scores = {}\n",
    "        for league_id in league_ids:\n",
    "            url_draft = \"https://api.underdogfantasy.com/v2/drafts/\" + league_id\n",
    "            url_weekly_scores = (\n",
    "                \"https://api.underdogfantasy.com/v1/drafts/\"\n",
    "                + league_id\n",
    "                + \"/weekly_scores\"\n",
    "            )\n",
    "\n",
    "            self.url_drafts[league_id] = url_draft\n",
    "            self.url_weekly_scores[league_id] = url_weekly_scores\n",
    "\n",
    "        self.json_drafts = {}\n",
    "        self.json_weekly_scores = {}\n",
    "\n",
    "        self.df_drafts = pd.DataFrame()\n",
    "        self.df_draft_entries = pd.DataFrame()\n",
    "        self.df_weekly_scores = pd.DataFrame()\n",
    "\n",
    "    def create_df_drafts(self) -> pd.DataFrame:\n",
    "        dfs = []\n",
    "        for league_id in self.league_ids:\n",
    "            df = self._create_df_draft_ind_league(league_id)\n",
    "            dfs.append(df)\n",
    "\n",
    "        final_df = pd.concat(dfs)\n",
    "\n",
    "        return final_df\n",
    "\n",
    "    def create_df_draft_entries(self) -> pd.DataFrame:\n",
    "        dfs = []\n",
    "        for league_id in self.league_ids:\n",
    "            df = self._create_df_draft_entries_ind_league(league_id)\n",
    "            dfs.append(df)\n",
    "\n",
    "        final_df = pd.concat(dfs)\n",
    "\n",
    "        return final_df\n",
    "\n",
    "    def create_df_weekly_scores(self) -> pd.DataFrame:\n",
    "        dfs = []\n",
    "        for league_id in self.league_ids:\n",
    "            df = self._create_df_weekly_scores_ind_league(league_id)\n",
    "            dfs.append(df)\n",
    "\n",
    "        final_df = pd.concat(dfs)\n",
    "        final_df.reset_index(inplace=True)\n",
    "\n",
    "        week_mapping = self._create_week_id_mapping()\n",
    "        final_df = pd.merge(final_df, week_mapping, on=\"week_id\")\n",
    "\n",
    "        return final_df\n",
    "\n",
    "    def _create_df_draft_ind_league(self, league_id: str) -> pd.DataFrame:\n",
    "        self.json_drafts[league_id] = self.read_in_site_data(\n",
    "            self.url_drafts[league_id], headers=self.auth_header\n",
    "        )\n",
    "        scraped_data = self.json_drafts[league_id][\"draft\"][\"picks\"]\n",
    "\n",
    "        initial_scraped_df = self.create_scraped_data_df(scraped_data)\n",
    "        initial_scraped_df.drop([\"projection_average\"], axis=1, inplace=True)\n",
    "\n",
    "        initial_scraped_df[\"draft_id\"] = league_id\n",
    "\n",
    "        return initial_scraped_df\n",
    "\n",
    "    def _create_df_draft_entries_ind_league(self, league_id: str) -> pd.DataFrame:\n",
    "        \"\"\" \n",
    "        Creates a df of all users in the draft, sorted by pick order.\n",
    "        \"\"\"\n",
    "\n",
    "        json = self.read_in_site_data(url, self.auth_header)\n",
    "\n",
    "        df_entries = self.create_scraped_data_df(json['draft']['draft_entries'])\n",
    "        df_users = self.create_scraped_data_df(json['draft']['users'])\n",
    "\n",
    "        df_users.rename(columns={'id': 'user_id'}, inplace=True)\n",
    "        df_users = df_users[['user_id', 'username']]\n",
    "\n",
    "        df = pd.merge(df_entries, df_users, how='left', on='user_id')\n",
    "        df = df.sort_values(by='pick_order').reset_index(drop=True)\n",
    "\n",
    "        df['draft_id'] = league_id\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _create_df_weekly_scores_ind_league(self, league_id: str) -> pd.DataFrame:\n",
    "        self.json_weekly_scores[league_id] = self.read_in_site_data(\n",
    "            self.url_weekly_scores[league_id], headers=self.auth_header\n",
    "        )\n",
    "        scraped_data = self.json_weekly_scores[league_id][\"draft_weekly_scores\"]\n",
    "\n",
    "        initial_scraped_df = self.create_scraped_data_df(scraped_data)\n",
    "\n",
    "        weekly_scores = self._pull_out_weekly_scores(initial_scraped_df)\n",
    "\n",
    "        initial_scraped_df.drop([\"week\", \"draft_entries_points\"], axis=1, inplace=True)\n",
    "\n",
    "        final_scraped_df = pd.merge(\n",
    "            left=weekly_scores, right=initial_scraped_df, on=\"id\", how=\"left\"\n",
    "        )\n",
    "        final_scraped_df.drop([\"id\"], axis=1, inplace=True)\n",
    "\n",
    "        return final_scraped_df\n",
    "\n",
    "    def _pull_out_weekly_scores(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Each row represents one week where each teams score is contained\n",
    "        within a dicitonary for that week. This pulls those scores out and\n",
    "        puts them in a Team/Week level df\n",
    "        \"\"\"\n",
    "\n",
    "        df = df.copy()\n",
    "\n",
    "        all_weekly_scores = []\n",
    "        for index, row in df.iterrows():\n",
    "            row_id = row[\"id\"]\n",
    "            week_id = row[\"week\"][\"id\"]\n",
    "            status = row[\"week\"][\"status\"]\n",
    "            points_dict = row[\"draft_entries_points\"]\n",
    "\n",
    "            for user_id, points in points_dict.items():\n",
    "                weekly_scores = [row_id, week_id, status, user_id, points]\n",
    "\n",
    "                all_weekly_scores.append(weekly_scores)\n",
    "\n",
    "        columns = [\"id\", \"week_id\", \"status\", \"user_id\", \"total_points\"]\n",
    "        df = pd.DataFrame(data=all_weekly_scores, columns=columns)\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import UD_draft_model.scrapers.scrape_site.scrape_league_data as scrape_site\n",
    "\n",
    "\n",
    "active_drafts = scrape_site.DraftsActive(bearer_token\n",
    "df = active_drafts.create_df_active_drafts()\n",
    "\n",
    "slate_id = df['slate_id'].iloc[0]\n",
    "scoring_type_id = df['scoring_type_id'].iloc[0]\n",
    "draft_id = [df['id'].iloc[0]]\n",
    "refs = scrape_site.ReferenceData(slate_id, scoring_type_id)\n",
    "\n",
    "draft_detail = scrape_site.DraftsDetail(draft_id, bearer_token)\n",
    "\n",
    "url = [url for url in draft_detail.url_drafts.values()][0]\n",
    "header = draft_detail.auth_header\n",
    "header['authorization'] = bearer_token\n",
    "\n",
    "json = draft_detail.read_in_site_data(url, headers=header)\n",
    "\n",
    "# df_players = refs.create_df_players_master()\n",
    "# df_draft = draft_detail.create_df_drafts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>auto_pick_at</th>\n",
       "      <th>clock</th>\n",
       "      <th>contest_style_id</th>\n",
       "      <th>draft_at</th>\n",
       "      <th>draft_entry_id</th>\n",
       "      <th>draft_type</th>\n",
       "      <th>entry_count</th>\n",
       "      <th>entry_role</th>\n",
       "      <th>entry_style_id</th>\n",
       "      <th>pick_count</th>\n",
       "      <th>slate_id</th>\n",
       "      <th>source</th>\n",
       "      <th>source_entry_style_id</th>\n",
       "      <th>status</th>\n",
       "      <th>title</th>\n",
       "      <th>user_auto_pick</th>\n",
       "      <th>user_pick_order</th>\n",
       "      <th>scoring_type_id</th>\n",
       "      <th>rounds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>317e9315-c5cf-415e-bc6c-6a607b57747e</td>\n",
       "      <td>2023-02-26T05:04:11Z</td>\n",
       "      <td>28800</td>\n",
       "      <td>978b95dd-7c25-467c-83c9-332d90a557a4</td>\n",
       "      <td>2023-02-25T18:54:30Z</td>\n",
       "      <td>7d55e3d8-5168-46be-ba6e-5edbd651ed93</td>\n",
       "      <td>slow</td>\n",
       "      <td>12</td>\n",
       "      <td>None</td>\n",
       "      <td>473af08f-55a4-5f56-b401-2e6ce8d0e096</td>\n",
       "      <td>9</td>\n",
       "      <td>b84244dc-aa63-4b62-bdd5-8fccd365c074</td>\n",
       "      <td>sit_and_go</td>\n",
       "      <td>None</td>\n",
       "      <td>drafting</td>\n",
       "      <td>None</td>\n",
       "      <td>off</td>\n",
       "      <td>1</td>\n",
       "      <td>ccf300b0-9197-5951-bd96-cba84ad71e86</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id          auto_pick_at  clock  \\\n",
       "0  317e9315-c5cf-415e-bc6c-6a607b57747e  2023-02-26T05:04:11Z  28800   \n",
       "\n",
       "                       contest_style_id              draft_at  \\\n",
       "0  978b95dd-7c25-467c-83c9-332d90a557a4  2023-02-25T18:54:30Z   \n",
       "\n",
       "                         draft_entry_id draft_type  entry_count entry_role  \\\n",
       "0  7d55e3d8-5168-46be-ba6e-5edbd651ed93       slow           12       None   \n",
       "\n",
       "                         entry_style_id  pick_count  \\\n",
       "0  473af08f-55a4-5f56-b401-2e6ce8d0e096           9   \n",
       "\n",
       "                               slate_id      source source_entry_style_id  \\\n",
       "0  b84244dc-aa63-4b62-bdd5-8fccd365c074  sit_and_go                  None   \n",
       "\n",
       "     status title user_auto_pick  user_pick_order  \\\n",
       "0  drafting  None            off                1   \n",
       "\n",
       "                        scoring_type_id  rounds  \n",
       "0  ccf300b0-9197-5951-bd96-cba84ad71e86      20  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# contest_refs = ContestRefs()\n",
    "\n",
    "# contest_refs.create_df_contest_styles()\n",
    "\n",
    "active_drafts = DraftsActive(bearer_token)\n",
    "df = active_drafts.create_df_active_drafts()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>auto_pick</th>\n",
       "      <th>payout</th>\n",
       "      <th>payout_text</th>\n",
       "      <th>pick_order</th>\n",
       "      <th>place</th>\n",
       "      <th>points</th>\n",
       "      <th>share_link</th>\n",
       "      <th>title</th>\n",
       "      <th>user_id</th>\n",
       "      <th>username</th>\n",
       "      <th>draft_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7d55e3d8-5168-46be-ba6e-5edbd651ed93</td>\n",
       "      <td>off</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>03717131-e053-451d-9be6-09755f5875ae</td>\n",
       "      <td>CONNORDELONG</td>\n",
       "      <td>317e9315-c5cf-415e-bc6c-6a607b57747e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d2f0c07d-88ef-41b6-bb80-0caa7585a847</td>\n",
       "      <td>off</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>6cf9cd0c-d74f-4942-8bc0-572bc3518a3b</td>\n",
       "      <td>FRESHLYBAKED</td>\n",
       "      <td>317e9315-c5cf-415e-bc6c-6a607b57747e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ec5fd82a-3203-4286-8bd9-5ee079652d73</td>\n",
       "      <td>off</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>76f9314e-4c41-4507-8bd5-fcb631d82f73</td>\n",
       "      <td>IAMSCAM561</td>\n",
       "      <td>317e9315-c5cf-415e-bc6c-6a607b57747e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>111784cb-ea10-4ba8-9ee1-f7076497c808</td>\n",
       "      <td>off</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>d747d4fa-d8cb-45f6-8353-dc4c2257ff38</td>\n",
       "      <td>KATABS</td>\n",
       "      <td>317e9315-c5cf-415e-bc6c-6a607b57747e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3ab3c61d-adf3-4d39-b8de-d9e0080dbcc1</td>\n",
       "      <td>off</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>10e759f3-d633-4003-a75d-6634b1287696</td>\n",
       "      <td>FISHE2JS</td>\n",
       "      <td>317e9315-c5cf-415e-bc6c-6a607b57747e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5e306051-d09f-466b-afd2-4b5258ffdfd3</td>\n",
       "      <td>off</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>6</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>f972412e-3e67-4113-9a66-11451ff4a09c</td>\n",
       "      <td>CORNHUSKINGHOG</td>\n",
       "      <td>317e9315-c5cf-415e-bc6c-6a607b57747e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>93744170-d43d-4e3b-bbc1-8d6fde83c71f</td>\n",
       "      <td>off</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>7</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>ae2789f4-9195-4215-b36b-c513e5e3342c</td>\n",
       "      <td>MEDIEVALGRIDIRON</td>\n",
       "      <td>317e9315-c5cf-415e-bc6c-6a607b57747e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>05e7905a-a5dc-41a8-981e-daa808de9fc7</td>\n",
       "      <td>off</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>8</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1737f75f-d241-4422-acc4-2bf9b11b11a8</td>\n",
       "      <td>STANSBURY9</td>\n",
       "      <td>317e9315-c5cf-415e-bc6c-6a607b57747e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bb8b9e5d-6ede-4a67-a530-b1b99d635f4d</td>\n",
       "      <td>off</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>9</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>32bbb20f-7dd1-4d4a-b89a-dc2bf74a2f58</td>\n",
       "      <td>AFRANCO1109</td>\n",
       "      <td>317e9315-c5cf-415e-bc6c-6a607b57747e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6b283a1a-e828-4221-a029-6c38fc69d9a2</td>\n",
       "      <td>off</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>b5cbe30e-89c0-4bbd-89f5-633700dadd2b</td>\n",
       "      <td>DBURKS27</td>\n",
       "      <td>317e9315-c5cf-415e-bc6c-6a607b57747e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>dadb5c17-a4bc-494e-b187-d755b68ad865</td>\n",
       "      <td>off</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>11</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0b7208d3-1448-4612-b7a0-812f7fdefcbd</td>\n",
       "      <td>ANDREWEDLER</td>\n",
       "      <td>317e9315-c5cf-415e-bc6c-6a607b57747e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>31e108e4-8fa5-46d4-8e49-cf2dc0a32bc3</td>\n",
       "      <td>off</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>12</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>5174dcf3-9c9d-44e0-99d8-716f1d59e0aa</td>\n",
       "      <td>DUTCHBOY241</td>\n",
       "      <td>317e9315-c5cf-415e-bc6c-6a607b57747e</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      id auto_pick payout payout_text  \\\n",
       "0   7d55e3d8-5168-46be-ba6e-5edbd651ed93       off   None        None   \n",
       "1   d2f0c07d-88ef-41b6-bb80-0caa7585a847       off   None        None   \n",
       "2   ec5fd82a-3203-4286-8bd9-5ee079652d73       off   None        None   \n",
       "3   111784cb-ea10-4ba8-9ee1-f7076497c808       off   None        None   \n",
       "4   3ab3c61d-adf3-4d39-b8de-d9e0080dbcc1       off   None        None   \n",
       "5   5e306051-d09f-466b-afd2-4b5258ffdfd3       off   None        None   \n",
       "6   93744170-d43d-4e3b-bbc1-8d6fde83c71f       off   None        None   \n",
       "7   05e7905a-a5dc-41a8-981e-daa808de9fc7       off   None        None   \n",
       "8   bb8b9e5d-6ede-4a67-a530-b1b99d635f4d       off   None        None   \n",
       "9   6b283a1a-e828-4221-a029-6c38fc69d9a2       off   None        None   \n",
       "10  dadb5c17-a4bc-494e-b187-d755b68ad865       off   None        None   \n",
       "11  31e108e4-8fa5-46d4-8e49-cf2dc0a32bc3       off   None        None   \n",
       "\n",
       "    pick_order place points share_link title  \\\n",
       "0            1  None   None       None  None   \n",
       "1            2  None   None       None  None   \n",
       "2            3  None   None       None  None   \n",
       "3            4  None   None       None  None   \n",
       "4            5  None   None       None  None   \n",
       "5            6  None   None       None  None   \n",
       "6            7  None   None       None  None   \n",
       "7            8  None   None       None  None   \n",
       "8            9  None   None       None  None   \n",
       "9           10  None   None       None  None   \n",
       "10          11  None   None       None  None   \n",
       "11          12  None   None       None  None   \n",
       "\n",
       "                                 user_id          username  \\\n",
       "0   03717131-e053-451d-9be6-09755f5875ae      CONNORDELONG   \n",
       "1   6cf9cd0c-d74f-4942-8bc0-572bc3518a3b      FRESHLYBAKED   \n",
       "2   76f9314e-4c41-4507-8bd5-fcb631d82f73        IAMSCAM561   \n",
       "3   d747d4fa-d8cb-45f6-8353-dc4c2257ff38            KATABS   \n",
       "4   10e759f3-d633-4003-a75d-6634b1287696          FISHE2JS   \n",
       "5   f972412e-3e67-4113-9a66-11451ff4a09c    CORNHUSKINGHOG   \n",
       "6   ae2789f4-9195-4215-b36b-c513e5e3342c  MEDIEVALGRIDIRON   \n",
       "7   1737f75f-d241-4422-acc4-2bf9b11b11a8        STANSBURY9   \n",
       "8   32bbb20f-7dd1-4d4a-b89a-dc2bf74a2f58       AFRANCO1109   \n",
       "9   b5cbe30e-89c0-4bbd-89f5-633700dadd2b          DBURKS27   \n",
       "10  0b7208d3-1448-4612-b7a0-812f7fdefcbd       ANDREWEDLER   \n",
       "11  5174dcf3-9c9d-44e0-99d8-716f1d59e0aa       DUTCHBOY241   \n",
       "\n",
       "                                draft_id  \n",
       "0   317e9315-c5cf-415e-bc6c-6a607b57747e  \n",
       "1   317e9315-c5cf-415e-bc6c-6a607b57747e  \n",
       "2   317e9315-c5cf-415e-bc6c-6a607b57747e  \n",
       "3   317e9315-c5cf-415e-bc6c-6a607b57747e  \n",
       "4   317e9315-c5cf-415e-bc6c-6a607b57747e  \n",
       "5   317e9315-c5cf-415e-bc6c-6a607b57747e  \n",
       "6   317e9315-c5cf-415e-bc6c-6a607b57747e  \n",
       "7   317e9315-c5cf-415e-bc6c-6a607b57747e  \n",
       "8   317e9315-c5cf-415e-bc6c-6a607b57747e  \n",
       "9   317e9315-c5cf-415e-bc6c-6a607b57747e  \n",
       "10  317e9315-c5cf-415e-bc6c-6a607b57747e  \n",
       "11  317e9315-c5cf-415e-bc6c-6a607b57747e  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "draft_detail = DraftsDetail(draft_id, bearer_token)\n",
    "\n",
    "draft_detail.create_df_draft_entries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>auto_pick</th>\n",
       "      <th>payout</th>\n",
       "      <th>payout_text</th>\n",
       "      <th>pick_order</th>\n",
       "      <th>place</th>\n",
       "      <th>points</th>\n",
       "      <th>share_link</th>\n",
       "      <th>title</th>\n",
       "      <th>user_id</th>\n",
       "      <th>username</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7d55e3d8-5168-46be-ba6e-5edbd651ed93</td>\n",
       "      <td>off</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>03717131-e053-451d-9be6-09755f5875ae</td>\n",
       "      <td>CONNORDELONG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d2f0c07d-88ef-41b6-bb80-0caa7585a847</td>\n",
       "      <td>off</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>6cf9cd0c-d74f-4942-8bc0-572bc3518a3b</td>\n",
       "      <td>FRESHLYBAKED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ec5fd82a-3203-4286-8bd9-5ee079652d73</td>\n",
       "      <td>off</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>76f9314e-4c41-4507-8bd5-fcb631d82f73</td>\n",
       "      <td>IAMSCAM561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>111784cb-ea10-4ba8-9ee1-f7076497c808</td>\n",
       "      <td>off</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>d747d4fa-d8cb-45f6-8353-dc4c2257ff38</td>\n",
       "      <td>KATABS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3ab3c61d-adf3-4d39-b8de-d9e0080dbcc1</td>\n",
       "      <td>off</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>10e759f3-d633-4003-a75d-6634b1287696</td>\n",
       "      <td>FISHE2JS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5e306051-d09f-466b-afd2-4b5258ffdfd3</td>\n",
       "      <td>off</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>6</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>f972412e-3e67-4113-9a66-11451ff4a09c</td>\n",
       "      <td>CORNHUSKINGHOG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>93744170-d43d-4e3b-bbc1-8d6fde83c71f</td>\n",
       "      <td>off</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>7</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>ae2789f4-9195-4215-b36b-c513e5e3342c</td>\n",
       "      <td>MEDIEVALGRIDIRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>05e7905a-a5dc-41a8-981e-daa808de9fc7</td>\n",
       "      <td>off</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>8</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1737f75f-d241-4422-acc4-2bf9b11b11a8</td>\n",
       "      <td>STANSBURY9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bb8b9e5d-6ede-4a67-a530-b1b99d635f4d</td>\n",
       "      <td>off</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>9</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>32bbb20f-7dd1-4d4a-b89a-dc2bf74a2f58</td>\n",
       "      <td>AFRANCO1109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6b283a1a-e828-4221-a029-6c38fc69d9a2</td>\n",
       "      <td>off</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>b5cbe30e-89c0-4bbd-89f5-633700dadd2b</td>\n",
       "      <td>DBURKS27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>dadb5c17-a4bc-494e-b187-d755b68ad865</td>\n",
       "      <td>off</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>11</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0b7208d3-1448-4612-b7a0-812f7fdefcbd</td>\n",
       "      <td>ANDREWEDLER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>31e108e4-8fa5-46d4-8e49-cf2dc0a32bc3</td>\n",
       "      <td>off</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>12</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>5174dcf3-9c9d-44e0-99d8-716f1d59e0aa</td>\n",
       "      <td>DUTCHBOY241</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      id auto_pick payout payout_text  \\\n",
       "0   7d55e3d8-5168-46be-ba6e-5edbd651ed93       off   None        None   \n",
       "1   d2f0c07d-88ef-41b6-bb80-0caa7585a847       off   None        None   \n",
       "2   ec5fd82a-3203-4286-8bd9-5ee079652d73       off   None        None   \n",
       "3   111784cb-ea10-4ba8-9ee1-f7076497c808       off   None        None   \n",
       "4   3ab3c61d-adf3-4d39-b8de-d9e0080dbcc1       off   None        None   \n",
       "5   5e306051-d09f-466b-afd2-4b5258ffdfd3       off   None        None   \n",
       "6   93744170-d43d-4e3b-bbc1-8d6fde83c71f       off   None        None   \n",
       "7   05e7905a-a5dc-41a8-981e-daa808de9fc7       off   None        None   \n",
       "8   bb8b9e5d-6ede-4a67-a530-b1b99d635f4d       off   None        None   \n",
       "9   6b283a1a-e828-4221-a029-6c38fc69d9a2       off   None        None   \n",
       "10  dadb5c17-a4bc-494e-b187-d755b68ad865       off   None        None   \n",
       "11  31e108e4-8fa5-46d4-8e49-cf2dc0a32bc3       off   None        None   \n",
       "\n",
       "    pick_order place points share_link title  \\\n",
       "0            1  None   None       None  None   \n",
       "1            2  None   None       None  None   \n",
       "2            3  None   None       None  None   \n",
       "3            4  None   None       None  None   \n",
       "4            5  None   None       None  None   \n",
       "5            6  None   None       None  None   \n",
       "6            7  None   None       None  None   \n",
       "7            8  None   None       None  None   \n",
       "8            9  None   None       None  None   \n",
       "9           10  None   None       None  None   \n",
       "10          11  None   None       None  None   \n",
       "11          12  None   None       None  None   \n",
       "\n",
       "                                 user_id          username  \n",
       "0   03717131-e053-451d-9be6-09755f5875ae      CONNORDELONG  \n",
       "1   6cf9cd0c-d74f-4942-8bc0-572bc3518a3b      FRESHLYBAKED  \n",
       "2   76f9314e-4c41-4507-8bd5-fcb631d82f73        IAMSCAM561  \n",
       "3   d747d4fa-d8cb-45f6-8353-dc4c2257ff38            KATABS  \n",
       "4   10e759f3-d633-4003-a75d-6634b1287696          FISHE2JS  \n",
       "5   f972412e-3e67-4113-9a66-11451ff4a09c    CORNHUSKINGHOG  \n",
       "6   ae2789f4-9195-4215-b36b-c513e5e3342c  MEDIEVALGRIDIRON  \n",
       "7   1737f75f-d241-4422-acc4-2bf9b11b11a8        STANSBURY9  \n",
       "8   32bbb20f-7dd1-4d4a-b89a-dc2bf74a2f58       AFRANCO1109  \n",
       "9   b5cbe30e-89c0-4bbd-89f5-633700dadd2b          DBURKS27  \n",
       "10  0b7208d3-1448-4612-b7a0-812f7fdefcbd       ANDREWEDLER  \n",
       "11  5174dcf3-9c9d-44e0-99d8-716f1d59e0aa       DUTCHBOY241  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_users = draft_detail.create_scraped_data_df(json['draft']['users'])\n",
    "df_entries = draft_detail.create_scraped_data_df(json['draft']['draft_entries'])\n",
    "\n",
    "df_users.rename(columns={'id': 'user_id'}, inplace=True)\n",
    "df_users = df_users[['user_id', 'username']]\n",
    "\n",
    "df = pd.merge(df_entries, df_users, how='left', on='user_id')\n",
    "df = df.sort_values(by='pick_order').reset_index(drop=True)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dc4492b5efc155a9cdcda09d43a20d3fd18b0b2d151b680a1414de88ecead1b9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
