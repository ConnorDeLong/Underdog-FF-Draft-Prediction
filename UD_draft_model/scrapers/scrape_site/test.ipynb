{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from UD_draft_model.scrapers.scrape_site.pull_bearer_token import pull_bearer_token\n",
    "\n",
    "\n",
    "class BaseData:\n",
    "    def __init__(self, clear_json_attrs: bool=True):\n",
    "        self._clear_json_attrs = clear_json_attrs\n",
    "\n",
    "        # user-agent and/or accept headers sometimes required\n",
    "        self.auth_header = {\n",
    "            \"accept\": \"application/json\",\n",
    "            \"user-agent\": \"Mozilla/5.0 (X11; Linux x86_64) \\\n",
    "                                AppleWebKit/537.36 (KHTML, like Gecko) \\\n",
    "                                Chrome/99.0.4844.51 Safari/537.36\",\n",
    "        }\n",
    "\n",
    "        self._player_scores_wk_1_id = 78\n",
    "        self._player_scores_wk_last_id = 78 + 17\n",
    "\n",
    "    def build_all_dfs(self, sleep_time: int=0):\n",
    "        \"\"\"\n",
    "        Overwrites every 'df_' attribute with a df that is created by running the\n",
    "        'create_' method that matches it. This serves as the primary method\n",
    "        for building the dfs associated with the class\n",
    "        \"\"\"\n",
    "\n",
    "        attrs = [attr for attr in dir(self) if attr.startswith(\"df_\")]\n",
    "        for attr in attrs:\n",
    "            method_name = \"create_\" + attr\n",
    "            self.__dict__[attr] = getattr(self, method_name)()\n",
    "            try:\n",
    "                self.__dict__[attr] = getattr(self, method_name)()\n",
    "            except:\n",
    "                print(getattr(self, method_name), \"failed to run\")\n",
    "\n",
    "            if sleep_time > 0:\n",
    "                time.sleep(sleep_time)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        if self._clear_json_attrs == True:\n",
    "            self.clear_json_attrs()\n",
    "\n",
    "    def clear_json_attrs(self):\n",
    "        \"\"\"\n",
    "        Clears all the atttributes that hold the json data pulled from the API.\n",
    "        By default, this is executed when the build_all_dfs method is run\n",
    "        \"\"\"\n",
    "\n",
    "        attrs = [attr for attr in dir(self) if attr.startswith(\"json\")]\n",
    "        for attr in attrs:\n",
    "            self.__dict__[attr] = {}\n",
    "\n",
    "    def read_in_site_data(self, url, headers: dict=None) -> dict:\n",
    "        \"\"\"Pulls in the raw data from the API and returns it as a dict\"\"\"\n",
    "\n",
    "        if headers is None:\n",
    "            headers = {}\n",
    "\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        site_data = response.json()\n",
    "\n",
    "        return site_data\n",
    "\n",
    "    def create_scraped_data_df(self, scraped_data: list) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Converts a list of dictionaries into a df where the keys of the dicts are\n",
    "        used for the columns and the values are placed in the rows.\n",
    "        NOTE: this assumes the keys in all dicts are the same.\n",
    "        \"\"\"\n",
    "\n",
    "        # Get the dicionary keys to identify the columns of the list for each output dict key\n",
    "        output_data_cols = []\n",
    "        for output_data_col in scraped_data[0].keys():\n",
    "            output_data_cols.append(output_data_col)\n",
    "\n",
    "        final_data_dict = {\"columns\": output_data_cols}\n",
    "        for data_dict_id, data in enumerate(scraped_data):\n",
    "            all_data_elements = []\n",
    "            for output_data_col in output_data_cols:\n",
    "                try:\n",
    "                    data_element = data[output_data_col]\n",
    "                except:\n",
    "                    # Note: This should probably be conditional on the data type, \n",
    "                    # but just using N/A for now.\n",
    "                    data_element = \"N/A\"\n",
    "\n",
    "                all_data_elements.append(data_element)\n",
    "\n",
    "            final_data_dict[data_dict_id] = all_data_elements\n",
    "\n",
    "        final_data_df = self._convert_data_dict_to_df(final_data_dict)\n",
    "\n",
    "        return final_data_df\n",
    "\n",
    "    def _convert_data_dict_to_df(self, scraped_data_dict: dict) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Converts the dict from the create_scraped_data_dict function to a df\n",
    "        NOTE: The input dict takes the following form:\n",
    "        {'columns': [<column names>], 1: [<column values>], 2: [<column_values>], ...}\n",
    "        \"\"\"\n",
    "\n",
    "        columns = scraped_data_dict[\"columns\"]\n",
    "\n",
    "        data_keys = list(scraped_data_dict.keys())[1:]\n",
    "\n",
    "        data_for_df = []\n",
    "        for data_key in data_keys:\n",
    "            data = scraped_data_dict[data_key]\n",
    "\n",
    "            data_for_df.append(data)\n",
    "\n",
    "        final_df = pd.DataFrame(data=data_for_df, columns=columns)\n",
    "\n",
    "        return final_df\n",
    "\n",
    "    def _create_week_id_mapping(self) -> pd.DataFrame:\n",
    "        \"\"\"Creates a map between the APIs Week ID and the actual Week number\"\"\"\n",
    "\n",
    "        wk_numbers = []\n",
    "        wk_ids = []\n",
    "        for wk_number, wk_id in enumerate(\n",
    "            range(self._player_scores_wk_1_id, self._player_scores_wk_last_id + 1)\n",
    "        ):\n",
    "            wk_numbers.append(wk_number + 1)\n",
    "            wk_ids.append(wk_id)\n",
    "\n",
    "        mapping = {\"week_number\": wk_numbers, \"week_id\": wk_ids}\n",
    "        df_mapping = pd.DataFrame(data=mapping)\n",
    "\n",
    "        return df_mapping\n",
    "\n",
    "\n",
    "class ReferenceData(BaseData):\n",
    "    \"\"\"Compiles all major reference data into dataframes\"\"\"\n",
    "\n",
    "    def __init__(self, clear_json_attrs: bool = True, slate_id: str = None):\n",
    "        super().__init__(clear_json_attrs=clear_json_attrs, slate_id=slate_id)\n",
    "\n",
    "        # week ID seems right but can't find the correct url for it\n",
    "        # self._player_scores_wk_1_id = 78\n",
    "        self._player_scores_wk_1_id = 1186\n",
    "\n",
    "        self.url_players = (\n",
    "            \"https://stats.underdogfantasy.com/v1/slates/\" + self.slate_id + \"/players\"\n",
    "        )\n",
    "        self.url_appearances = (\n",
    "            \"https://stats.underdogfantasy.com/v1/slates/\"\n",
    "            + self.slate_id\n",
    "            + \"/scoring_types/ccf300b0-9197-5951-bd96-cba84ad71e86/appearances\"\n",
    "        )\n",
    "        self.url_teams = \"https://stats.underdogfantasy.com/v1/teams\"\n",
    "\n",
    "        base_url_player_scores = \"https://stats.underdogfantasy.com/v1/weeks/\"\n",
    "        end_url_player_scores = (\n",
    "            \"/scoring_types/ccf300b0-9197-5951-bd96-cba84ad71e86/appearances\"\n",
    "        )\n",
    "        self.urls_player_scores = {\n",
    "            \"player_scores_wk_\"\n",
    "            + str(i + 1): base_url_player_scores\n",
    "            + str(wk_id)\n",
    "            + end_url_player_scores\n",
    "            for i, wk_id in enumerate(\n",
    "                range(self._player_scores_wk_1_id, self._player_scores_wk_1_id + 17)\n",
    "            )\n",
    "        }\n",
    "\n",
    "        self.df_players = pd.DataFrame()\n",
    "        self.df_appearances = pd.DataFrame()\n",
    "        self.df_teams = pd.DataFrame()\n",
    "        self.df_players_master = pd.DataFrame()\n",
    "        self.df_player_scores = pd.DataFrame()\n",
    "\n",
    "    def build_all_dfs(self):\n",
    "        attrs = [attr for attr in dir(self) if attr.startswith(\"df_\")]\n",
    "        for attr in attrs:\n",
    "            if attr == \"df_players_master\":\n",
    "                pass\n",
    "            else:\n",
    "                try:\n",
    "                    method_name = \"create_\" + attr\n",
    "                    self.__dict__[attr] = getattr(self, method_name)()\n",
    "                except:\n",
    "                    print(getattr(self, method_name), \"failed to run\")\n",
    "\n",
    "        # This ensures the dfs it depends on are created\n",
    "        self.df_players_master = self.create_df_players_master()\n",
    "\n",
    "        if self._clear_json_attrs == True:\n",
    "            self.clear_json_attrs()\n",
    "\n",
    "    def create_df_players(self) -> pd.DataFrame:\n",
    "        self.json_players = self.read_in_site_data(\n",
    "            self.url_players, headers=self.auth_header\n",
    "        )\n",
    "\n",
    "        initial_scraped_df = self.create_scraped_data_df(self.json_players[\"players\"])\n",
    "        initial_scraped_df.drop([\"image_url\"], axis=1, inplace=True)\n",
    "        initial_scraped_df.rename(columns={\"id\": \"player_id\"}, inplace=True)\n",
    "\n",
    "        return initial_scraped_df\n",
    "\n",
    "    def create_df_appearances(self) -> pd.DataFrame:\n",
    "        self.json_appearances = self.read_in_site_data(\n",
    "            self.url_appearances, headers=self.auth_header\n",
    "        )\n",
    "\n",
    "        initial_scraped_df = self.create_scraped_data_df(\n",
    "            self.json_appearances[\"appearances\"]\n",
    "        )\n",
    "        initial_scraped_df.drop(\n",
    "            [\"latest_news_item_updated_at\", \"score\"], axis=1, inplace=True\n",
    "        )\n",
    "\n",
    "        # 'projection' column values are dicitionaries which can be converted to a df and merged\n",
    "        projection_col = initial_scraped_df[\"projection\"].to_list()\n",
    "        projection_df = self.create_scraped_data_df(projection_col)\n",
    "\n",
    "        projection_df.drop([\"id\", \"scoring_type_id\"], axis=1, inplace=True)\n",
    "        projection_df.rename(\n",
    "            columns={\"points\": \"season_projected_points\"}, inplace=True\n",
    "        )\n",
    "\n",
    "        final_df = pd.merge(\n",
    "            initial_scraped_df,\n",
    "            projection_df,\n",
    "            left_index=True,\n",
    "            right_index=True,\n",
    "            how=\"left\",\n",
    "        )\n",
    "        final_df.drop([\"projection\"], axis=1, inplace=True)\n",
    "\n",
    "        df_pos_map = self._create_position_mapping(final_df)\n",
    "        final_df = pd.merge(final_df, df_pos_map, on=\"position_id\", how=\"left\")\n",
    "\n",
    "        final_df.rename(columns={\"id\": \"appearance_id\"}, inplace=True)\n",
    "\n",
    "        return final_df\n",
    "\n",
    "    def create_df_teams(self) -> pd.DataFrame:\n",
    "        self.json_teams = self.read_in_site_data(\n",
    "            self.url_teams, headers=self.auth_header\n",
    "        )\n",
    "\n",
    "        initial_scraped_df = self.create_scraped_data_df(self.json_teams[\"teams\"])\n",
    "\n",
    "        keep_vars = [\"id\", \"abbr\", \"name\"]\n",
    "        final_df = initial_scraped_df[keep_vars].copy()\n",
    "\n",
    "        final_df.rename(columns={\"name\": \"team_name\", \"id\": \"team_id\"}, inplace=True)\n",
    "\n",
    "        return final_df\n",
    "\n",
    "    def create_df_players_master(self) -> pd.DataFrame:\n",
    "        \"\"\"Creates a master lookup for player attributes\"\"\"\n",
    "\n",
    "        if len(self.df_appearances) == 0:\n",
    "            self.df_appearances = self.create_df_appearances()\n",
    "\n",
    "        if len(self.df_players) == 0:\n",
    "            self.df_players = self.create_df_players()\n",
    "\n",
    "        if len(self.df_teams) == 0:\n",
    "            self.df_teams = self.create_df_teams()\n",
    "\n",
    "        # Team is more accurate in the df_players data and position from df_appearances\n",
    "        # reflects the posisiton at the time of the draft\n",
    "        df_appearances = self.df_appearances.drop([\"team_id\"], axis=1, inplace=False)\n",
    "        df_players = self.df_players.drop([\"position_id\"], axis=1, inplace=False)\n",
    "\n",
    "        final_df = pd.merge(df_appearances, df_players, on=\"player_id\", how=\"left\")\n",
    "\n",
    "        final_df = pd.merge(final_df, self.df_teams, on=\"team_id\", how=\"left\")\n",
    "\n",
    "        return final_df\n",
    "\n",
    "    def create_df_player_scores(self):\n",
    "        \"\"\"\n",
    "        This no longer appears to work due to either a change in the endpoint\n",
    "        or the starting point week id is wrong\n",
    "        \"\"\"\n",
    "\n",
    "        self.json_player_scores = {\n",
    "            \"player_scores_wk_\"\n",
    "            + str(i + 1): self.read_in_site_data(\n",
    "                self.urls_player_scores[\"player_scores_wk_\" + str(i + 1)],\n",
    "                headers=self.auth_header,\n",
    "            )\n",
    "            for i, wk_id in enumerate(\n",
    "                range(self._player_scores_wk_1_id, self._player_scores_wk_1_id + 17)\n",
    "            )\n",
    "        }\n",
    "\n",
    "        player_scores_df_list = []\n",
    "        for wk_id in range(1, 18):\n",
    "            if (\n",
    "                len(\n",
    "                    self.json_player_scores[\"player_scores_wk_\" + str(wk_id)][\n",
    "                        \"appearances\"\n",
    "                    ]\n",
    "                )\n",
    "                > 0\n",
    "            ):\n",
    "                player_scores_json = self.json_player_scores[\n",
    "                    \"player_scores_wk_\" + str(wk_id)\n",
    "                ]\n",
    "                player_scores_df = self._create_df_player_scores_one_wk(\n",
    "                    player_scores_json[\"appearances\"]\n",
    "                )\n",
    "                player_scores_df[\"week_number\"] = wk_id\n",
    "\n",
    "                player_scores_df_list.append(player_scores_df)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        player_scores_df = pd.concat(player_scores_df_list)\n",
    "        player_scores_df.reset_index(inplace=True)\n",
    "\n",
    "        return player_scores_df\n",
    "\n",
    "    def _create_df_player_scores_one_wk(self, scraped_data: list) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Each weeks player scores are contained in its own URL - this creates a df\n",
    "        of those scores for one week\n",
    "        \"\"\"\n",
    "\n",
    "        initial_scraped_df = self.create_scraped_data_df(scraped_data)\n",
    "        initial_scraped_df.drop([\"latest_news_item_updated_at\"], axis=1, inplace=True)\n",
    "\n",
    "        # 'projection' column values are dicitionaries which can be converted to a df and merged\n",
    "        projection_col = initial_scraped_df[\"projection\"].to_list()\n",
    "        projection_df = self.create_scraped_data_df(projection_col)\n",
    "\n",
    "        projection_df = projection_df[[\"points\"]]\n",
    "        projection_df.rename(columns={\"points\": \"projected_points\"}, inplace=True)\n",
    "\n",
    "        score_col = initial_scraped_df[\"score\"].to_list()\n",
    "        score_df = self.create_scraped_data_df(score_col)\n",
    "\n",
    "        score_df = score_df[[\"points\"]]\n",
    "        score_df.rename(columns={\"points\": \"actual_points\"}, inplace=True)\n",
    "\n",
    "        final_df = pd.merge(\n",
    "            initial_scraped_df,\n",
    "            projection_df,\n",
    "            left_index=True,\n",
    "            right_index=True,\n",
    "            how=\"left\",\n",
    "        )\n",
    "        final_df = pd.merge(\n",
    "            final_df, score_df, left_index=True, right_index=True, how=\"left\"\n",
    "        )\n",
    "\n",
    "        final_df.drop([\"projection\", \"score\"], axis=1, inplace=True)\n",
    "\n",
    "        return final_df\n",
    "\n",
    "    def _create_position_mapping(self, df_appearances: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Creates df that maps position to position_id since this cant be found in the API\n",
    "        \"\"\"\n",
    "\n",
    "        df_pos_map = df_appearances.copy()\n",
    "\n",
    "        df_pos_map[\"position\"] = df_pos_map[\"position_rank\"].str[0:2]\n",
    "        df_pos_map = df_pos_map[[\"position_id\", \"position\"]].loc[\n",
    "            df_pos_map[\"position\"].notnull()\n",
    "        ]\n",
    "        df_pos_map = df_pos_map.drop_duplicates(\n",
    "            subset=[\"position\", \"position_id\"], keep=\"first\"\n",
    "        )\n",
    "\n",
    "        return df_pos_map\n",
    "\n",
    "\n",
    "class LeagueData(BaseData):\n",
    "    \"\"\"Compiles all major league specific data into dataframes\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, league_ids: list, bearer_token: str, clear_json_attrs: bool = True\n",
    "    ):\n",
    "        super().__init__(clear_json_attrs=clear_json_attrs)\n",
    "\n",
    "        self.auth_header['authorization'] = bearer_token\n",
    "\n",
    "        self.url_drafts = {}\n",
    "        self.url_weekly_scores = {}\n",
    "        for league_id in league_ids:\n",
    "            url_draft = \"https://api.underdogfantasy.com/v2/drafts/\" + league_id\n",
    "            url_weekly_scores = (\n",
    "                \"https://api.underdogfantasy.com/v1/drafts/\"\n",
    "                + league_id\n",
    "                + \"/weekly_scores\"\n",
    "            )\n",
    "\n",
    "            self.url_drafts[league_id] = url_draft\n",
    "            self.url_weekly_scores[league_id] = url_weekly_scores\n",
    "\n",
    "        self.json_drafts = {}\n",
    "        self.json_weekly_scores = {}\n",
    "\n",
    "        self.df_drafts = pd.DataFrame()\n",
    "        self.df_weekly_scores = pd.DataFrame()\n",
    "\n",
    "    def create_df_drafts(self) -> pd.DataFrame:\n",
    "        dfs = []\n",
    "        for league_id in self.league_ids:\n",
    "            df = self._create_df_draft_ind_league(league_id)\n",
    "            dfs.append(df)\n",
    "\n",
    "        final_df = pd.concat(dfs)\n",
    "\n",
    "        return final_df\n",
    "\n",
    "    def create_df_weekly_scores(self) -> pd.DataFrame:\n",
    "        dfs = []\n",
    "        for league_id in self.league_ids:\n",
    "            df = self._create_df_weekly_scores_ind_league(league_id)\n",
    "            dfs.append(df)\n",
    "\n",
    "        final_df = pd.concat(dfs)\n",
    "        final_df.reset_index(inplace=True)\n",
    "\n",
    "        week_mapping = self._create_week_id_mapping()\n",
    "        final_df = pd.merge(final_df, week_mapping, on=\"week_id\")\n",
    "\n",
    "        return final_df\n",
    "\n",
    "    def _create_df_draft_ind_league(self, league_id: str) -> pd.DataFrame:\n",
    "        self.json_drafts[league_id] = self.read_in_site_data(\n",
    "            self.url_drafts[league_id], headers=self.auth_header\n",
    "        )\n",
    "        scraped_data = self.json_drafts[league_id][\"draft\"][\"picks\"]\n",
    "\n",
    "        initial_scraped_df = self.create_scraped_data_df(scraped_data)\n",
    "        initial_scraped_df.drop([\"projection_average\"], axis=1, inplace=True)\n",
    "\n",
    "        initial_scraped_df[\"draft_id\"] = league_id\n",
    "\n",
    "        return initial_scraped_df\n",
    "\n",
    "    def _create_df_weekly_scores_ind_league(self, league_id: str) -> pd.DataFrame:\n",
    "        self.json_weekly_scores[league_id] = self.read_in_site_data(\n",
    "            self.url_weekly_scores[league_id], headers=self.auth_header\n",
    "        )\n",
    "        scraped_data = self.json_weekly_scores[league_id][\"draft_weekly_scores\"]\n",
    "\n",
    "        initial_scraped_df = self.create_scraped_data_df(scraped_data)\n",
    "\n",
    "        weekly_scores = self._pull_out_weekly_scores(initial_scraped_df)\n",
    "\n",
    "        initial_scraped_df.drop([\"week\", \"draft_entries_points\"], axis=1, inplace=True)\n",
    "\n",
    "        final_scraped_df = pd.merge(\n",
    "            left=weekly_scores, right=initial_scraped_df, on=\"id\", how=\"left\"\n",
    "        )\n",
    "        final_scraped_df.drop([\"id\"], axis=1, inplace=True)\n",
    "\n",
    "        return final_scraped_df\n",
    "\n",
    "    def _pull_out_weekly_scores(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Each row represents one week where each teams score is contained\n",
    "        within a dicitonary for that week. This pulls those scores out and\n",
    "        puts them in a Team/Week level df\n",
    "        \"\"\"\n",
    "\n",
    "        df = df.copy()\n",
    "\n",
    "        all_weekly_scores = []\n",
    "        for index, row in df.iterrows():\n",
    "            row_id = row[\"id\"]\n",
    "            week_id = row[\"week\"][\"id\"]\n",
    "            status = row[\"week\"][\"status\"]\n",
    "            points_dict = row[\"draft_entries_points\"]\n",
    "\n",
    "            for user_id, points in points_dict.items():\n",
    "                weekly_scores = [row_id, week_id, status, user_id, points]\n",
    "\n",
    "                all_weekly_scores.append(weekly_scores)\n",
    "\n",
    "        columns = [\"id\", \"week_id\", \"status\", \"user_id\", \"total_points\"]\n",
    "        df = pd.DataFrame(data=all_weekly_scores, columns=columns)\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "class UserData(BaseData):\n",
    "    \n",
    "    def __init__(self, bearer_token: str, clear_json_attrs: bool = True):\n",
    "        \"\"\"\n",
    "        Note: This requires the user-agent header - Should be able to grab this\n",
    "        with the bearer token, but hard coding for now\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(clear_json_attrs=clear_json_attrs)\n",
    "\n",
    "        self.auth_header['authorization'] = bearer_token\n",
    "\n",
    "        self.slate_id = '87a5caba-d5d7-46d9-a798-018d7c116213'\n",
    "\n",
    "        self.url_live_leagues = (\n",
    "            'https://api.underdogfantasy.com/v3/user/active_drafts'\n",
    "        )\n",
    "        # self.url_base_leagues = (\n",
    "        #     \"https://api.underdogfantasy.com/v2/user/slates/\"\n",
    "        #     + self.slate_id\n",
    "        #     + \"/live_drafts\"\n",
    "        # )\n",
    "        # self.url_base_leagues = (\n",
    "        #     'https://api.underdogfantasy.com/v2/user/slates/' \n",
    "        #     + self.slate_id \n",
    "        #     + '/completed_drafts'\n",
    "        # )\n",
    "        self.url_base_leagues = (\n",
    "            'https://api.underdogfantasy.com/v2/user/slates/' \n",
    "            + self.slate_id \n",
    "            + '/settled_drafts'\n",
    "        )\n",
    "        self.url_tourney_league_ids = (\n",
    "            \"https://api.underdogfantasy.com/v1/user/slates/\"\n",
    "            + self.slate_id\n",
    "            + \"/tournament_rounds\"\n",
    "        )\n",
    "\n",
    "        self.json_leagues = {}\n",
    "\n",
    "        self.df_all_leagues = pd.DataFrame()\n",
    "\n",
    "    def create_df_all_leagues(self, league_urls: list=None) -> pd.DataFrame:\n",
    "        if league_urls is None:\n",
    "            league_urls = self._create_league_urls()\n",
    "\n",
    "        leagues = []\n",
    "        for i, league_url in enumerate(league_urls):\n",
    "            df = self._create_df_leagues(league_url, \"league_\" + str(i + 1))\n",
    "            leagues.append(df)\n",
    "\n",
    "        df_all_leagues = pd.concat(leagues)\n",
    "        df_all_leagues.reset_index(inplace=True)\n",
    "        df_all_leagues.drop(columns=[\"index\"], inplace=True)\n",
    "\n",
    "        return df_all_leagues\n",
    "\n",
    "    def _create_df_leagues(self, url_base: str, json_leagues_key: str) -> pd.DataFrame:\n",
    "        self.json_leagues[json_leagues_key] = self._create_json_leagues(url_base)\n",
    "        scraped_data = self.json_leagues[json_leagues_key]\n",
    "\n",
    "        leagues_df_list = []\n",
    "        for leagues_page in scraped_data.values():\n",
    "            leagues_page_df = self.create_scraped_data_df(leagues_page[\"drafts\"])\n",
    "            leagues_df_list.append(leagues_page_df)\n",
    "\n",
    "        leagues_df = pd.concat(leagues_df_list)\n",
    "\n",
    "        return leagues_df\n",
    "\n",
    "    def _create_json_leagues(self, url_base: str) -> dict:\n",
    "        \"\"\"\n",
    "        Loops through all the different pages that contain the league level data\n",
    "        and stores each as an entry in a dict\n",
    "        \"\"\"\n",
    "\n",
    "        url_exists = True\n",
    "        i = 1\n",
    "        leagues_json_dict = {}\n",
    "        while url_exists:\n",
    "            if i == 1:\n",
    "                url = url_base\n",
    "            else:\n",
    "                url = url_base + \"?page=\" + str(i)\n",
    "\n",
    "            leagues = self.read_in_site_data(url, headers=self.auth_header)\n",
    "\n",
    "            if len(leagues[\"drafts\"]) > 0:\n",
    "                leagues_json_dict[\"page_\" + str(i)] = leagues\n",
    "            else:\n",
    "                url_exists = False\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        return leagues_json_dict\n",
    "\n",
    "    def _create_df_tourney_league_ids(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Tournament leagues (i.e. Puppy 1, Puppy 2, etc.) require the ID of the\n",
    "        tourney in order to find all entries in it - This creates of all tourney\n",
    "        IDs that has at least one entry\n",
    "        \"\"\"\n",
    "\n",
    "        json_tourney_league_ids = self.read_in_site_data(\n",
    "            self.url_tourney_league_ids, headers=self.auth_header\n",
    "        )\n",
    "        scraped_data = json_tourney_league_ids[\"tournament_rounds\"]\n",
    "\n",
    "        initial_scraped_df = self.create_scraped_data_df(scraped_data)\n",
    "\n",
    "        # Pulling out the 'id' from the 'tournament' dict in case this is whats needed\n",
    "        tournament_col = initial_scraped_df[\"tournament\"].to_list()\n",
    "        tournament_df = self.create_scraped_data_df(tournament_col)\n",
    "        tournament_df.rename(columns={\"id\": \"tournament_id\"}, inplace=True)\n",
    "        tournament_df = tournament_df[\"tournament_id\"]\n",
    "\n",
    "        initial_scraped_df.drop([\"tournament\"], axis=1, inplace=True)\n",
    "        final_df = initial_scraped_df.join(tournament_df)\n",
    "\n",
    "        return final_df\n",
    "\n",
    "    def _create_league_urls(self, tourney_league_ids: list = None) -> list:\n",
    "        \"\"\"\n",
    "        Creates a list of all the URLs that contain entries\n",
    "        \"\"\"\n",
    "        \n",
    "        if tourney_league_ids is None:\n",
    "            tourney_league_ids = list(self._create_df_tourney_league_ids()[\"id\"])\n",
    "\n",
    "        base_url = \"https://api.underdogfantasy.com/v1/user/tournament_rounds/\"\n",
    "        tourney_league_urls = []\n",
    "        for tourney_league_id in tourney_league_ids:\n",
    "            tourney_league_url = base_url + tourney_league_id + \"/drafts\"\n",
    "            tourney_league_urls.append(tourney_league_url)\n",
    "\n",
    "        tourney_league_urls.append(self.url_base_leagues)\n",
    "\n",
    "        return tourney_league_urls\n",
    "\n",
    "\n",
    "class ContestRefs(BaseData):\n",
    "    \"\"\" \n",
    "    Compiles all major contest related data into dataframes.\n",
    "    Note that this includes contests specific to a user (e.g. completed\n",
    "    slates, settled slates, etc.)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bearer_token: str, clear_json_attrs: bool = True):\n",
    "        super().__init__(clear_json_attrs=clear_json_attrs, slate_id=None)\n",
    "\n",
    "        self.auth_header['authorization'] = bearer_token\n",
    "\n",
    "        self.url_slates_available = (\n",
    "            'https://stats.underdogfantasy.com/v1/sports/nfl/slates'\n",
    "        )\n",
    "        self.url_slates_completed = (\n",
    "            'https://api.underdogfantasy.com/v2/user/completed_slates'\n",
    "        )\n",
    "        self.url_slates_settled = (\n",
    "            'https://api.underdogfantasy.com/v1/user/sports/nfl/settled_slates'\n",
    "        )\n",
    "        self.url_scoring_types = (\n",
    "            'https://stats.underdogfantasy.com/v1/scoring_types'\n",
    "        )\n",
    "        self.url_contest_styles = (\n",
    "            'https://stats.underdogfantasy.com/v1/contest_styles'\n",
    "        )\n",
    "\n",
    "        self.df_slates_available = None\n",
    "        self.df_slates_completed = None\n",
    "        self.df_slates_settled = None\n",
    "        self.df_scoring_types = None\n",
    "        self.df_contest_styles = None\n",
    "\n",
    "        self.json = {}\n",
    "\n",
    "    def create_df_slates_available(self, headers: dict=None, clear_json: bool=False,\n",
    "    update_attr: bool=False) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Creates a slate level df of all slates that are available to \n",
    "        draft in.\n",
    "        \"\"\"\n",
    "\n",
    "        df = self._create_df_slates(\n",
    "            self.url_slates_available, 'slates_available',\n",
    "            headers=headers, clear_json=clear_json\n",
    "        )\n",
    "\n",
    "        if update_attr:\n",
    "            self.df_slates_available = df\n",
    "\n",
    "        return df\n",
    "\n",
    "    def create_df_slates_completed(self, headers: dict=None, clear_json: bool=False,\n",
    "    update_attr: bool=False) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Creates a slate level df of all slates that are completed.\n",
    "        \"\"\"\n",
    "\n",
    "        df = self._create_df_slates(\n",
    "            self.url_slates_completed, 'slates_completed',\n",
    "            headers=headers, clear_json=clear_json\n",
    "        )\n",
    "\n",
    "        if update_attr:\n",
    "            self.df_slates_completed = df\n",
    "\n",
    "        return df\n",
    "\n",
    "    def create_df_slates_settled(self, headers: dict=None, clear_json: bool=False,\n",
    "    update_attr: bool=False) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Creates a slate level df of all slates that are settled.\n",
    "        \"\"\"\n",
    "\n",
    "        df = self._create_df_slates(\n",
    "            self.url_slates_settled, 'slates_settled',\n",
    "            headers=headers, clear_json=clear_json\n",
    "        )\n",
    "\n",
    "        if update_attr:\n",
    "            self.df_slates_settled = df\n",
    "\n",
    "        return df\n",
    "\n",
    "    def create_df_scoring_types(self, headers: dict=None, clear_json: bool=False,\n",
    "    update_attr: bool=False) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Creates a scoring type level df with the scoring types of all existing\n",
    "        NFL contests.\n",
    "\n",
    "        Notes:\n",
    "            - This is needed to automate the \"appearances\" (i.e. draft rank)\n",
    "            pull which uses the id as part of the url string\n",
    "            - 'display_stats' contains more descriptive information about each\n",
    "            scoring_type, but that data isn't needed now and would take some\n",
    "            time to pull out and structure.\n",
    "        \"\"\"\n",
    "\n",
    "        if headers is None:\n",
    "            headers=self.auth_header\n",
    "        \n",
    "        self.json['scoring_types'] = self.read_in_site_data(\n",
    "            self.url_scoring_types, headers=self.auth_header\n",
    "        )\n",
    "\n",
    "        df = self.create_scraped_data_df(self.json['scoring_types'][\"scoring_types\"])\n",
    "\n",
    "        df = df.loc[df['sport_id'] == 'NFL']\n",
    "\n",
    "        if update_attr:\n",
    "            self.df_scoring_types = df\n",
    "\n",
    "        if clear_json:\n",
    "            del self.json['scoring_types']\n",
    "\n",
    "        return df\n",
    "\n",
    "    def create_df_contest_styles(self, headers: dict=None, clear_json: bool=False,\n",
    "    update_attr: bool=False) -> pd.DataFrame:\n",
    "\n",
    "        if headers is None:\n",
    "            headers=self.auth_header\n",
    "        \n",
    "        self.json['contest_styles'] = self.read_in_site_data(\n",
    "            self.url_contest_styles, headers=self.auth_header\n",
    "        )\n",
    "\n",
    "        df = self.create_scraped_data_df(self.json['contest_styles'][\"contest_styles\"])\n",
    "\n",
    "        df = df.loc[df['sport_id'] == 'NFL']\n",
    "\n",
    "        if update_attr:\n",
    "            self.df_contest_styles = df\n",
    "\n",
    "        if clear_json:\n",
    "            del self.json['contest_styles'] \n",
    "\n",
    "        return df\n",
    "\n",
    "    def get_slate_ids(self, slate_type: str):\n",
    "        \"\"\"\n",
    "        Returns a list that contains each slate id for the slate_type.\n",
    "        slate_type must be 'available', 'completed', or 'settled'.\n",
    "        \"\"\"\n",
    "\n",
    "        attr = 'df_slates_' + slate_type.strip()\n",
    "        method_name = 'create_df_slates_' + slate_type.strip()\n",
    "\n",
    "        # NEED TO DOUBLE CHECK THIS - \n",
    "        if self.__dict__[attr] is None:\n",
    "            df = getattr(self, method_name)()\n",
    "        else:\n",
    "            df = self.__dict__[attr]\n",
    "\n",
    "        slates = list(df['id'])\n",
    "\n",
    "        return slates        \n",
    "\n",
    "    def _create_df_slates(self, url_slate: str, slate_type: str,\n",
    "        headers: dict=None, clear_json: bool=False\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\" \n",
    "        Helper method that can be used to create a df for any slate type.\n",
    "        \"\"\"\n",
    "\n",
    "        if headers is None:\n",
    "            headers=self.auth_header\n",
    "        \n",
    "        self.json[slate_type] = self.read_in_site_data(\n",
    "            url_slate, headers=self.auth_header\n",
    "        )\n",
    "\n",
    "        df = self.create_scraped_data_df(self.json[slate_type][\"slates\"])\n",
    "\n",
    "        if clear_json:\n",
    "            del self.json[slate_type]\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "def create_underdog_df_dict(bearer_token: str, sleep_time: int = 0) -> dict:\n",
    "    \"\"\"Creates a dictionary of dfs containing the most relevant UD data\"\"\"\n",
    "\n",
    "    ref_data = ReferenceData()\n",
    "    ref_data.build_all_dfs()\n",
    "\n",
    "    user_data = UserData(bearer_token)\n",
    "    user_data.build_all_dfs()\n",
    "    league_ids = list(user_data.df_all_leagues[\"id\"])\n",
    "\n",
    "    league_data = LeagueData(league_ids, bearer_token)\n",
    "    league_data.build_all_dfs(sleep_time=sleep_time)\n",
    "\n",
    "    df_players_master = ref_data.df_players_master\n",
    "    df_player_scores = ref_data.df_player_scores\n",
    "\n",
    "    player_vars = [\n",
    "        \"appearance_id\",\n",
    "        \"player_id\",\n",
    "        \"position\",\n",
    "        \"team_name\",\n",
    "        \"first_name\",\n",
    "        \"last_name\",\n",
    "    ]\n",
    "    df_drafts = pd.merge(\n",
    "        league_data.df_drafts,\n",
    "        df_players_master[player_vars],\n",
    "        on=\"appearance_id\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "    df_weekly_scores = league_data.df_weekly_scores\n",
    "\n",
    "    final_dict = {\n",
    "        \"df_players_master\": df_players_master,\n",
    "        \"df_player_scores\": df_player_scores,\n",
    "        \"df_drafts\": df_drafts,\n",
    "        \"df_weekly_scores\": df_weekly_scores,\n",
    "        \"df_league_info\": user_data.df_all_leagues,\n",
    "    }\n",
    "\n",
    "    return final_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bearer eyJhbGciOiJIUzI1NiJ9.eyJqdGkiOiI0ZWNkMDVmZi1kYTExLTQwY2UtYTYwNS05ZDVmZjZlMGMwODciLCJzdWIiOiIwMzcxNzEzMS1lMDUzLTQ1MWQtOWJlNi0wOTc1NWY1ODc1YWUiLCJzY3AiOiJ1c2VyIiwiYXVkIjpudWxsLCJpYXQiOjE2NzY3Mzg3OTUsImV4cCI6MTY3OTM2ODU0MX0.GX5wdBf7SVM8eVuBAYnBObz6gJntj8FEn9_bzLSDo50\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 50)\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "\n",
    "### Variables to change ###\n",
    "chromedriver_path = \"/usr/bin/chromedriver\"\n",
    "username = input(\"Enter Underdog username: \")\n",
    "password = getpass.getpass()\n",
    "\n",
    "### Keep as is ###\n",
    "url = \"https://underdogfantasy.com/lobby\"\n",
    "bearer_token = pull_bearer_token(url, chromedriver_path, username, password)\n",
    "\n",
    "print(bearer_token)\n",
    "\n",
    "### Pull all major UD data elements ###\n",
    "# underdog_data = create_underdog_df_dict(bearer_token, sleep_time=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DraftsDetail(BaseData):\n",
    "    \"\"\"Compiles all major league specific data into dataframes\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, league_ids: list, bearer_token: str, clear_json_attrs: bool = True\n",
    "    ):\n",
    "        super().__init__(clear_json_attrs=clear_json_attrs)\n",
    "\n",
    "        self.auth_header['authorization'] = bearer_token\n",
    "\n",
    "        self.league_ids = league_ids\n",
    "\n",
    "        self.url_drafts = {}\n",
    "        self.url_weekly_scores = {}\n",
    "        for league_id in league_ids:\n",
    "            url_draft = \"https://api.underdogfantasy.com/v2/drafts/\" + league_id\n",
    "            url_weekly_scores = (\n",
    "                \"https://api.underdogfantasy.com/v1/drafts/\"\n",
    "                + league_id\n",
    "                + \"/weekly_scores\"\n",
    "            )\n",
    "\n",
    "            self.url_drafts[league_id] = url_draft\n",
    "            self.url_weekly_scores[league_id] = url_weekly_scores\n",
    "\n",
    "        self.json_drafts = {}\n",
    "        self.json_weekly_scores = {}\n",
    "\n",
    "        self.df_drafts = pd.DataFrame()\n",
    "        self.df_weekly_scores = pd.DataFrame()\n",
    "\n",
    "    def create_df_drafts(self) -> pd.DataFrame:\n",
    "        dfs = []\n",
    "        for league_id in self.league_ids:\n",
    "            df = self._create_df_draft_ind_league(league_id)\n",
    "            dfs.append(df)\n",
    "\n",
    "        final_df = pd.concat(dfs)\n",
    "\n",
    "        return final_df\n",
    "\n",
    "    def create_df_weekly_scores(self) -> pd.DataFrame:\n",
    "        dfs = []\n",
    "        for league_id in self.league_ids:\n",
    "            df = self._create_df_weekly_scores_ind_league(league_id)\n",
    "            dfs.append(df)\n",
    "\n",
    "        final_df = pd.concat(dfs)\n",
    "        final_df.reset_index(inplace=True)\n",
    "\n",
    "        week_mapping = self._create_week_id_mapping()\n",
    "        final_df = pd.merge(final_df, week_mapping, on=\"week_id\")\n",
    "\n",
    "        return final_df\n",
    "\n",
    "    def _create_df_draft_ind_league(self, league_id: str) -> pd.DataFrame:\n",
    "        self.json_drafts[league_id] = self.read_in_site_data(\n",
    "            self.url_drafts[league_id], headers=self.auth_header\n",
    "        )\n",
    "        scraped_data = self.json_drafts[league_id][\"draft\"][\"picks\"]\n",
    "\n",
    "        initial_scraped_df = self.create_scraped_data_df(scraped_data)\n",
    "        initial_scraped_df.drop([\"projection_average\"], axis=1, inplace=True)\n",
    "\n",
    "        initial_scraped_df[\"draft_id\"] = league_id\n",
    "\n",
    "        return initial_scraped_df\n",
    "\n",
    "    def _create_df_weekly_scores_ind_league(self, league_id: str) -> pd.DataFrame:\n",
    "        self.json_weekly_scores[league_id] = self.read_in_site_data(\n",
    "            self.url_weekly_scores[league_id], headers=self.auth_header\n",
    "        )\n",
    "        scraped_data = self.json_weekly_scores[league_id][\"draft_weekly_scores\"]\n",
    "\n",
    "        initial_scraped_df = self.create_scraped_data_df(scraped_data)\n",
    "\n",
    "        weekly_scores = self._pull_out_weekly_scores(initial_scraped_df)\n",
    "\n",
    "        initial_scraped_df.drop([\"week\", \"draft_entries_points\"], axis=1, inplace=True)\n",
    "\n",
    "        final_scraped_df = pd.merge(\n",
    "            left=weekly_scores, right=initial_scraped_df, on=\"id\", how=\"left\"\n",
    "        )\n",
    "        final_scraped_df.drop([\"id\"], axis=1, inplace=True)\n",
    "\n",
    "        return final_scraped_df\n",
    "\n",
    "    def _pull_out_weekly_scores(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Each row represents one week where each teams score is contained\n",
    "        within a dicitonary for that week. This pulls those scores out and\n",
    "        puts them in a Team/Week level df\n",
    "        \"\"\"\n",
    "\n",
    "        df = df.copy()\n",
    "\n",
    "        all_weekly_scores = []\n",
    "        for index, row in df.iterrows():\n",
    "            row_id = row[\"id\"]\n",
    "            week_id = row[\"week\"][\"id\"]\n",
    "            status = row[\"week\"][\"status\"]\n",
    "            points_dict = row[\"draft_entries_points\"]\n",
    "\n",
    "            for user_id, points in points_dict.items():\n",
    "                weekly_scores = [row_id, week_id, status, user_id, points]\n",
    "\n",
    "                all_weekly_scores.append(weekly_scores)\n",
    "\n",
    "        columns = [\"id\", \"week_id\", \"status\", \"user_id\", \"total_points\"]\n",
    "        df = pd.DataFrame(data=all_weekly_scores, columns=columns)\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "class DraftsActive(BaseData):\n",
    "\n",
    "    url = 'https://api.underdogfantasy.com/v3/user/active_drafts'\n",
    "\n",
    "    def __init__(self, bearer_token: str, clear_json_attrs: bool=True):\n",
    "        super().__init__(clear_json_attrs=clear_json_attrs)\n",
    "\n",
    "        self.auth_header['authorization'] = bearer_token\n",
    "\n",
    "        self.json = {}\n",
    "        self.df_active_drafts = None\n",
    "\n",
    "    def create_df_active_drafts(self) -> pd.DataFrame:\n",
    "        \"\"\" \n",
    "        Creates a draft level df of all active drafts. \n",
    "        \"\"\"\n",
    "\n",
    "        self.json = self.read_in_site_data(\n",
    "            DraftsActive.url, headers=self.auth_header\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            df = self.create_scraped_data_df(self.json[\"drafts\"])\n",
    "            df = self._add_scoring_type(df)\n",
    "        except IndexError:\n",
    "            print(f'No data found in {DraftsActive.url} - no df will be returned')\n",
    "            df = None\n",
    "\n",
    "        if self.clear_json_attrs:\n",
    "            self.json = {}\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _add_scoring_type(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\" \n",
    "        Scoring type required to get the rankings (appearances) used\n",
    "        for the draft\n",
    "        \"\"\"\n",
    "\n",
    "        contest_refs = ContestRefs()\n",
    "        df_styles = contest_refs.create_df_contest_styles()\n",
    "        df_styles.rename(columns={'id': 'contest_style_id'}, inplace=True)\n",
    "\n",
    "        df = pd.merge(\n",
    "            df, df_styles[['contest_style_id', 'scoring_type_id']],\n",
    "            how='left', on='contest_style_id'\n",
    "        )\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "class Drafts(BaseData):\n",
    "    \"\"\"\n",
    "    Compiles all completed or settled draft level data for a slate.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        bearer_token: str,\n",
    "        slate: Slate,\n",
    "        clear_json_attrs: bool=True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Note: This requires the user-agent header - Should be able to grab this\n",
    "        with the bearer token, but hard coding for now\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(clear_json_attrs=clear_json_attrs)\n",
    "\n",
    "        self.auth_header['authorization'] = bearer_token\n",
    "        self.slate = slate\n",
    "\n",
    "        url_suffix = f'/{self.slate.slate_type}_drafts'\n",
    "        self.url_base_leagues = (\n",
    "            \"https://api.underdogfantasy.com/v2/user/slates/\"\n",
    "            + self.slate.id\n",
    "            + url_suffix\n",
    "        )\n",
    "        self.url_tourney_league_ids = (\n",
    "            \"https://api.underdogfantasy.com/v1/user/slates/\"\n",
    "            + self.slate.id\n",
    "            + \"/tournament_rounds\"\n",
    "        )\n",
    "\n",
    "        self.json_leagues = {}\n",
    "        self.df_all_leagues = pd.DataFrame()\n",
    "\n",
    "    def create_df_all_leagues(self, league_urls: list=None) -> pd.DataFrame:\n",
    "        if league_urls is None:\n",
    "            league_urls = self.get_league_urls()\n",
    "\n",
    "        leagues = []\n",
    "        for i, league_url in enumerate(league_urls):\n",
    "            df = self._create_df_leagues(league_url, \"league_\" + str(i + 1))\n",
    "            leagues.append(df)\n",
    "\n",
    "        df_all_leagues = pd.concat(leagues)\n",
    "        df_all_leagues.reset_index(inplace=True)\n",
    "        df_all_leagues.drop(columns=[\"index\"], inplace=True)\n",
    "\n",
    "        return df_all_leagues\n",
    "\n",
    "    def get_league_urls(self) -> list:\n",
    "        \"\"\" \n",
    "        Creates a list of all urls which store draft level data for the slate.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            tourney_league_urls = self._create_tourney_league_urls()\n",
    "        except IndexError:\n",
    "            tourney_league_urls = []\n",
    "\n",
    "        if self.slate.draft_count > 0:\n",
    "            base_league_url = [self.url_base_leagues]\n",
    "        else:\n",
    "            base_league_url = []\n",
    "\n",
    "        urls = base_league_url + tourney_league_urls\n",
    "\n",
    "        return urls        \n",
    "\n",
    "    def _create_df_leagues(self, url_base: str, json_leagues_key: str) -> pd.DataFrame:\n",
    "        self.json_leagues[json_leagues_key] = self._create_json_leagues(url_base)\n",
    "        scraped_data = self.json_leagues[json_leagues_key]\n",
    "\n",
    "        leagues_df_list = []\n",
    "        for leagues_page in scraped_data.values():\n",
    "            leagues_page_df = self.create_scraped_data_df(leagues_page[\"drafts\"])\n",
    "            leagues_df_list.append(leagues_page_df)\n",
    "\n",
    "        leagues_df = pd.concat(leagues_df_list)\n",
    "\n",
    "        return leagues_df\n",
    "\n",
    "    def _create_json_leagues(self, url_base: str) -> dict:\n",
    "        \"\"\"\n",
    "        Loops through all the different pages that contain the league level data\n",
    "        and stores each as an entry in a dict\n",
    "        \"\"\"\n",
    "\n",
    "        url_exists = True\n",
    "        i = 1\n",
    "        leagues_json_dict = {}\n",
    "        while url_exists:\n",
    "            if i == 1:\n",
    "                url = url_base\n",
    "            else:\n",
    "                url = url_base + \"?page=\" + str(i)\n",
    "\n",
    "            leagues = self.read_in_site_data(url, headers=self.auth_header)\n",
    "\n",
    "            if len(leagues[\"drafts\"]) > 0:\n",
    "                leagues_json_dict[\"page_\" + str(i)] = leagues\n",
    "            else:\n",
    "                url_exists = False\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        return leagues_json_dict\n",
    "\n",
    "    def _create_df_tourney_league_ids(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Tournament leagues (i.e. Puppy 1, Puppy 2, etc.) require the ID of the\n",
    "        tourney in order to find all entries in it - This creates of all tourney\n",
    "        IDs that has at least one entry\n",
    "        \"\"\"\n",
    "\n",
    "        json_tourney_league_ids = self.read_in_site_data(\n",
    "            self.url_tourney_league_ids, headers=self.auth_header\n",
    "        )\n",
    "        scraped_data = json_tourney_league_ids[\"tournament_rounds\"]\n",
    "\n",
    "        initial_scraped_df = self.create_scraped_data_df(scraped_data)\n",
    "\n",
    "        # Pulling out the 'id' from the 'tournament' dict in case this is whats needed\n",
    "        tournament_col = initial_scraped_df[\"tournament\"].to_list()\n",
    "        tournament_df = self.create_scraped_data_df(tournament_col)\n",
    "        tournament_df.rename(columns={\"id\": \"tournament_id\"}, inplace=True)\n",
    "        tournament_df = tournament_df[\"tournament_id\"]\n",
    "\n",
    "        initial_scraped_df.drop([\"tournament\"], axis=1, inplace=True)\n",
    "        final_df = initial_scraped_df.join(tournament_df)\n",
    "\n",
    "        return final_df\n",
    "\n",
    "    def _create_tourney_league_urls(self) -> list:\n",
    "        \"\"\"\n",
    "        Creates a list of all the URLs that contain entries\n",
    "        \"\"\"\n",
    "        \n",
    "        tourney_league_ids = list(self._create_df_tourney_league_ids()[\"id\"])\n",
    "\n",
    "        base_url = \"https://api.underdogfantasy.com/v1/user/tournament_rounds/\"\n",
    "        tourney_league_urls = []\n",
    "        for tourney_league_id in tourney_league_ids:\n",
    "            tourney_league_url = base_url + tourney_league_id + \"/drafts\"\n",
    "\n",
    "            tourney_league_urls.append(tourney_league_url)\n",
    "\n",
    "        return tourney_league_urls\n",
    "\n",
    "\n",
    "class Slates(BaseData):\n",
    "    \"\"\" \n",
    "    Compiles all available and completed slates for a specific slate type. \n",
    "    \"\"\"\n",
    "\n",
    "    url_slates_available = (\n",
    "        'https://stats.underdogfantasy.com/v1/sports/nfl/slates'\n",
    "    )\n",
    "    url_slates_completed = (\n",
    "        'https://api.underdogfantasy.com/v2/user/completed_slates'\n",
    "    )\n",
    "    url_slates_settled = (\n",
    "        'https://api.underdogfantasy.com/v1/user/sports/nfl/settled_slates'\n",
    "    )\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        bearer_token: str,\n",
    "        slate_type: str,\n",
    "        clear_json_attrs: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        slate_type must be 'available', 'completed', or 'settled'\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(clear_json_attrs=clear_json_attrs, slate_id=None)\n",
    "\n",
    "        self.auth_header['authorization'] = bearer_token\n",
    "        self.slate_type = slate_type\n",
    "\n",
    "        self.df_slates = None\n",
    "        self.slates = []\n",
    "\n",
    "        self.json = {}\n",
    "\n",
    "    def create_df_slates(self, headers: dict=None, clear_json: bool=False\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\" \n",
    "        Creates df of all the slates found.\n",
    "        \"\"\"\n",
    "\n",
    "        if headers is None:\n",
    "            headers=self.auth_header\n",
    "\n",
    "        url = self._get_url()\n",
    "        \n",
    "        self.json = self.read_in_site_data(url, headers=self.auth_header)\n",
    "\n",
    "        try:\n",
    "            df = self.create_scraped_data_df(self.json[\"slates\"])\n",
    "\n",
    "            # This is stored as a list, but seems to always only contain\n",
    "            # one id.\n",
    "            df['contest_style_ids'] = (\n",
    "                df['contest_style_ids'].apply(lambda x: x[0])\n",
    "            )\n",
    "        except IndexError:\n",
    "            print(f'No data found in {url} - no df will be returned')\n",
    "            df = None\n",
    "\n",
    "        self.slates = self._create_slates(df)\n",
    "\n",
    "        if clear_json:\n",
    "            self.json = {}\n",
    "\n",
    "        return df   \n",
    "\n",
    "    def _get_url(self) -> str:\n",
    "        \"\"\"\n",
    "        Selects the url to be used based on the slate_type.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.slate_type == 'available':\n",
    "            url = Slates.url_slates_available\n",
    "        elif self.slate_type == 'completed':\n",
    "            url = Slates.url_slates_completed\n",
    "        elif self.slate_type == 'settled':\n",
    "            url = Slates.url_slates_settled\n",
    "\n",
    "        return url\n",
    "\n",
    "    def _create_slates(self, df_slates: pd.DataFrame) -> list:\n",
    "        \"\"\" \n",
    "        Creates a list of Slate objects.\n",
    "        \"\"\"\n",
    "\n",
    "        slates = []\n",
    "        for i in range(len(df_slates)):\n",
    "            slate = Slate(df_slates.iloc[i], self.slate_type)\n",
    "\n",
    "            slates.append(slate)\n",
    "\n",
    "        return slates\n",
    "\n",
    "\n",
    "class Slate:\n",
    "\n",
    "    def __init__(self, df_slate: pd.Series, slate_type):\n",
    "        self.id = df_slate['id']\n",
    "        self.contest_style_ids = df_slate['contest_style_ids']\n",
    "        self.description = df_slate['description']\n",
    "        self.title = df_slate['title']\n",
    "        self.slate_type = slate_type\n",
    "\n",
    "        try:\n",
    "            self.draft_count = df_slate['draft_count']\n",
    "            self.tournament_draft_count = df_slate['tournament_draft_count']\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "class ReferenceData(BaseData):\n",
    "    \"\"\"Compiles all major reference data into dataframes\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        slate_id: str,\n",
    "        scoring_type_id: str,\n",
    "        clear_json_attrs: bool=True, \n",
    "    ):\n",
    "        super().__init__(clear_json_attrs=clear_json_attrs)\n",
    "\n",
    "        self.slate_id = slate_id\n",
    "        self.scoring_type_id = scoring_type_id\n",
    "\n",
    "        # week ID seems right but can't find the correct url for it\n",
    "        # self._player_scores_wk_1_id = 78\n",
    "        self._player_scores_wk_1_id = 1186\n",
    "\n",
    "        self.url_players = (\n",
    "            'https://stats.underdogfantasy.com/v1/slates/'\n",
    "            + self.slate_id \n",
    "            + '/players'\n",
    "        )\n",
    "        self.url_appearances = (\n",
    "            'https://stats.underdogfantasy.com/v1/slates/'\n",
    "            + self.slate_id\n",
    "            + '/scoring_types/'\n",
    "            + self.scoring_type_id\n",
    "            + '/appearances'\n",
    "        )\n",
    "        self.url_teams = 'https://stats.underdogfantasy.com/v1/teams'\n",
    "\n",
    "        base_url_player_scores = 'https://stats.underdogfantasy.com/v1/weeks/'\n",
    "        end_url_player_scores = (\n",
    "            '/scoring_types/'\n",
    "            + self.scoring_type_id\n",
    "            + '/appearances'\n",
    "        )\n",
    "        self.urls_player_scores = {\n",
    "            \"player_scores_wk_\"\n",
    "            + str(i + 1): base_url_player_scores\n",
    "            + str(wk_id)\n",
    "            + end_url_player_scores\n",
    "            for i, wk_id in enumerate(\n",
    "                range(self._player_scores_wk_1_id, self._player_scores_wk_1_id + 17)\n",
    "            )\n",
    "        }\n",
    "\n",
    "        self.df_players = pd.DataFrame()\n",
    "        self.df_appearances = pd.DataFrame()\n",
    "        self.df_teams = pd.DataFrame()\n",
    "        self.df_players_master = pd.DataFrame()\n",
    "        self.df_player_scores = pd.DataFrame()\n",
    "\n",
    "    def build_all_dfs(self):\n",
    "        attrs = [attr for attr in dir(self) if attr.startswith(\"df_\")]\n",
    "        for attr in attrs:\n",
    "            if attr == \"df_players_master\":\n",
    "                pass\n",
    "            else:\n",
    "                try:\n",
    "                    method_name = \"create_\" + attr\n",
    "                    self.__dict__[attr] = getattr(self, method_name)()\n",
    "                except:\n",
    "                    print(getattr(self, method_name), \"failed to run\")\n",
    "\n",
    "        # This ensures the dfs it depends on are created\n",
    "        self.df_players_master = self.create_df_players_master()\n",
    "\n",
    "        if self._clear_json_attrs == True:\n",
    "            self.clear_json_attrs()\n",
    "\n",
    "    def create_df_players(self) -> pd.DataFrame:\n",
    "        self.json_players = self.read_in_site_data(\n",
    "            self.url_players, headers=self.auth_header\n",
    "        )\n",
    "\n",
    "        initial_scraped_df = self.create_scraped_data_df(self.json_players[\"players\"])\n",
    "        initial_scraped_df.drop([\"image_url\"], axis=1, inplace=True)\n",
    "        initial_scraped_df.rename(columns={\"id\": \"player_id\"}, inplace=True)\n",
    "\n",
    "        return initial_scraped_df\n",
    "\n",
    "    def create_df_appearances(self) -> pd.DataFrame:\n",
    "        self.json_appearances = self.read_in_site_data(\n",
    "            self.url_appearances, headers=self.auth_header\n",
    "        )\n",
    "\n",
    "        initial_scraped_df = self.create_scraped_data_df(\n",
    "            self.json_appearances[\"appearances\"]\n",
    "        )\n",
    "        initial_scraped_df.drop(\n",
    "            [\"latest_news_item_updated_at\", \"score\"], axis=1, inplace=True\n",
    "        )\n",
    "\n",
    "        # 'projection' column values are dicitionaries which can be converted to a df and merged\n",
    "        projection_col = initial_scraped_df[\"projection\"].to_list()\n",
    "        projection_df = self.create_scraped_data_df(projection_col)\n",
    "\n",
    "        projection_df.drop([\"id\", \"scoring_type_id\"], axis=1, inplace=True)\n",
    "        projection_df.rename(\n",
    "            columns={\"points\": \"season_projected_points\"}, inplace=True\n",
    "        )\n",
    "\n",
    "        final_df = pd.merge(\n",
    "            initial_scraped_df,\n",
    "            projection_df,\n",
    "            left_index=True,\n",
    "            right_index=True,\n",
    "            how=\"left\",\n",
    "        )\n",
    "        final_df.drop([\"projection\"], axis=1, inplace=True)\n",
    "\n",
    "        df_pos_map = self._create_position_mapping(final_df)\n",
    "        final_df = pd.merge(final_df, df_pos_map, on=\"position_id\", how=\"left\")\n",
    "\n",
    "        final_df.rename(columns={\"id\": \"appearance_id\"}, inplace=True)\n",
    "\n",
    "        return final_df\n",
    "\n",
    "    def create_df_teams(self) -> pd.DataFrame:\n",
    "        self.json_teams = self.read_in_site_data(\n",
    "            self.url_teams, headers=self.auth_header\n",
    "        )\n",
    "\n",
    "        initial_scraped_df = self.create_scraped_data_df(self.json_teams[\"teams\"])\n",
    "\n",
    "        keep_vars = [\"id\", \"abbr\", \"name\"]\n",
    "        final_df = initial_scraped_df[keep_vars].copy()\n",
    "\n",
    "        final_df.rename(columns={\"name\": \"team_name\", \"id\": \"team_id\"}, inplace=True)\n",
    "\n",
    "        return final_df\n",
    "\n",
    "    def create_df_players_master(self) -> pd.DataFrame:\n",
    "        \"\"\"Creates a master lookup for player attributes\"\"\"\n",
    "\n",
    "        if len(self.df_appearances) == 0:\n",
    "            self.df_appearances = self.create_df_appearances()\n",
    "\n",
    "        if len(self.df_players) == 0:\n",
    "            self.df_players = self.create_df_players()\n",
    "\n",
    "        if len(self.df_teams) == 0:\n",
    "            self.df_teams = self.create_df_teams()\n",
    "\n",
    "        # Team is more accurate in the df_players data and position from df_appearances\n",
    "        # reflects the posisiton at the time of the draft\n",
    "        df_appearances = self.df_appearances.drop([\"team_id\"], axis=1, inplace=False)\n",
    "        df_players = self.df_players.drop([\"position_id\"], axis=1, inplace=False)\n",
    "\n",
    "        final_df = pd.merge(df_appearances, df_players, on=\"player_id\", how=\"left\")\n",
    "\n",
    "        final_df = pd.merge(final_df, self.df_teams, on=\"team_id\", how=\"left\")\n",
    "\n",
    "        return final_df\n",
    "\n",
    "    def create_df_player_scores(self):\n",
    "        \"\"\"\n",
    "        This no longer appears to work due to either a change in the endpoint\n",
    "        or the starting point week id is wrong\n",
    "        \"\"\"\n",
    "\n",
    "        self.json_player_scores = {\n",
    "            \"player_scores_wk_\"\n",
    "            + str(i + 1): self.read_in_site_data(\n",
    "                self.urls_player_scores[\"player_scores_wk_\" + str(i + 1)],\n",
    "                headers=self.auth_header,\n",
    "            )\n",
    "            for i, wk_id in enumerate(\n",
    "                range(self._player_scores_wk_1_id, self._player_scores_wk_1_id + 17)\n",
    "            )\n",
    "        }\n",
    "\n",
    "        player_scores_df_list = []\n",
    "        for wk_id in range(1, 18):\n",
    "            if (\n",
    "                len(\n",
    "                    self.json_player_scores[\"player_scores_wk_\" + str(wk_id)][\n",
    "                        \"appearances\"\n",
    "                    ]\n",
    "                )\n",
    "                > 0\n",
    "            ):\n",
    "                player_scores_json = self.json_player_scores[\n",
    "                    \"player_scores_wk_\" + str(wk_id)\n",
    "                ]\n",
    "                player_scores_df = self._create_df_player_scores_one_wk(\n",
    "                    player_scores_json[\"appearances\"]\n",
    "                )\n",
    "                player_scores_df[\"week_number\"] = wk_id\n",
    "\n",
    "                player_scores_df_list.append(player_scores_df)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        player_scores_df = pd.concat(player_scores_df_list)\n",
    "        player_scores_df.reset_index(inplace=True)\n",
    "\n",
    "        return player_scores_df\n",
    "\n",
    "    def _create_df_player_scores_one_wk(self, scraped_data: list) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Each weeks player scores are contained in its own URL - this creates a df\n",
    "        of those scores for one week\n",
    "        \"\"\"\n",
    "\n",
    "        initial_scraped_df = self.create_scraped_data_df(scraped_data)\n",
    "        initial_scraped_df.drop([\"latest_news_item_updated_at\"], axis=1, inplace=True)\n",
    "\n",
    "        # 'projection' column values are dicitionaries which can be converted to a df and merged\n",
    "        projection_col = initial_scraped_df[\"projection\"].to_list()\n",
    "        projection_df = self.create_scraped_data_df(projection_col)\n",
    "\n",
    "        projection_df = projection_df[[\"points\"]]\n",
    "        projection_df.rename(columns={\"points\": \"projected_points\"}, inplace=True)\n",
    "\n",
    "        score_col = initial_scraped_df[\"score\"].to_list()\n",
    "        score_df = self.create_scraped_data_df(score_col)\n",
    "\n",
    "        score_df = score_df[[\"points\"]]\n",
    "        score_df.rename(columns={\"points\": \"actual_points\"}, inplace=True)\n",
    "\n",
    "        final_df = pd.merge(\n",
    "            initial_scraped_df,\n",
    "            projection_df,\n",
    "            left_index=True,\n",
    "            right_index=True,\n",
    "            how=\"left\",\n",
    "        )\n",
    "        final_df = pd.merge(\n",
    "            final_df, score_df, left_index=True, right_index=True, how=\"left\"\n",
    "        )\n",
    "\n",
    "        final_df.drop([\"projection\", \"score\"], axis=1, inplace=True)\n",
    "\n",
    "        return final_df\n",
    "\n",
    "    def _create_position_mapping(self, df_appearances: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Creates df that maps position to position_id since this cant be found in the API\n",
    "        \"\"\"\n",
    "\n",
    "        df_pos_map = df_appearances.copy()\n",
    "\n",
    "        df_pos_map[\"position\"] = df_pos_map[\"position_rank\"].str[0:2]\n",
    "        df_pos_map = df_pos_map[[\"position_id\", \"position\"]].loc[\n",
    "            df_pos_map[\"position\"].notnull()\n",
    "        ]\n",
    "        df_pos_map = df_pos_map.drop_duplicates(\n",
    "            subset=[\"position\", \"position_id\"], keep=\"first\"\n",
    "        )\n",
    "\n",
    "        return df_pos_map\n",
    "\n",
    "\n",
    "class ContestRefs(BaseData):\n",
    "    \"\"\" \n",
    "    Compiles all major contest related data into dataframes.\n",
    "    Note that this includes contests specific to a user (e.g. completed\n",
    "    slates, settled slates, etc.)\n",
    "    \"\"\"\n",
    "\n",
    "    url_scoring_types = 'https://stats.underdogfantasy.com/v1/scoring_types'\n",
    "    url_contest_styles = 'https://stats.underdogfantasy.com/v1/contest_styles'\n",
    "\n",
    "    def __init__(self, clear_json_attrs: bool=True):\n",
    "        super().__init__(clear_json_attrs=clear_json_attrs)\n",
    "\n",
    "        self.df_scoring_types = None\n",
    "        self.df_contest_styles = None\n",
    "\n",
    "        self.json = {}\n",
    "\n",
    "    def create_df_scoring_types(self, headers: dict=None, clear_json: bool=False,\n",
    "        update_attr: bool=False\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Creates a scoring type level df with the scoring types of all existing\n",
    "        NFL contests.\n",
    "\n",
    "        Notes:\n",
    "            - This is needed to automate the \"appearances\" (i.e. draft rank)\n",
    "            pull which uses the id as part of the url string\n",
    "            - 'display_stats' contains more descriptive information about each\n",
    "            scoring_type, but that data isn't needed now and would take some\n",
    "            time to pull out and structure.\n",
    "        \"\"\"\n",
    "\n",
    "        if headers is None:\n",
    "            headers=self.auth_header\n",
    "        \n",
    "        self.json['scoring_types'] = self.read_in_site_data(\n",
    "            ContestRefs.url_scoring_types, headers=self.auth_header\n",
    "        )\n",
    "\n",
    "        df = self.create_scraped_data_df(self.json['scoring_types'][\"scoring_types\"])\n",
    "\n",
    "        df = df.loc[df['sport_id'] == 'NFL']\n",
    "\n",
    "        if update_attr:\n",
    "            self.df_scoring_types = df\n",
    "\n",
    "        if clear_json:\n",
    "            del self.json['scoring_types']\n",
    "\n",
    "        return df\n",
    "\n",
    "    def create_df_contest_styles(self, headers: dict=None, clear_json: bool=False,\n",
    "    update_attr: bool=False) -> pd.DataFrame:\n",
    "\n",
    "        if headers is None:\n",
    "            headers=self.auth_header\n",
    "        \n",
    "        self.json['contest_styles'] = self.read_in_site_data(\n",
    "            ContestRefs.url_contest_styles, headers=self.auth_header\n",
    "        )\n",
    "\n",
    "        df = self.create_scraped_data_df(self.json['contest_styles'][\"contest_styles\"])\n",
    "\n",
    "        df = df.loc[df['sport_id'] == 'NFL']\n",
    "\n",
    "        if update_attr:\n",
    "            self.df_contest_styles = df\n",
    "\n",
    "        if clear_json:\n",
    "            del self.json['contest_styles'] \n",
    "\n",
    "        return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>appearance_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>draft_entry_id</th>\n",
       "      <th>number</th>\n",
       "      <th>pick_slot_id</th>\n",
       "      <th>points</th>\n",
       "      <th>projection_adp</th>\n",
       "      <th>projection_points</th>\n",
       "      <th>swapped</th>\n",
       "      <th>draft_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>463eefb2-188e-4eae-8ca5-a2c3adf87b63</td>\n",
       "      <td>0e383613-0c4f-4f3c-8145-73c0863a1929</td>\n",
       "      <td>2023-02-19T18:32:56Z</td>\n",
       "      <td>a9347221-f68a-4e54-9f9d-17f787ed9f38</td>\n",
       "      <td>1</td>\n",
       "      <td>c167e7db-a42f-40f2-a364-34aadb768ada</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.2</td>\n",
       "      <td>323.2</td>\n",
       "      <td>False</td>\n",
       "      <td>62f06e49-34b7-4f7d-8fec-91c47a3f12b2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3728840c-0b51-472a-aa16-692670f468da</td>\n",
       "      <td>0a57b8db-0a17-4b0b-bab2-cfadb788d9b8</td>\n",
       "      <td>2023-02-19T18:33:16Z</td>\n",
       "      <td>da5c930f-10cf-49f3-a12c-deb891ae35ac</td>\n",
       "      <td>2</td>\n",
       "      <td>8b25d7d2-613b-4a5a-b47b-a6ea125841c0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.9</td>\n",
       "      <td>303.5</td>\n",
       "      <td>False</td>\n",
       "      <td>62f06e49-34b7-4f7d-8fec-91c47a3f12b2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ad57512c-5ba5-4fb5-a74d-50d9fa11cb11</td>\n",
       "      <td>c307679c-8ee9-47e3-882f-f386bc00f510</td>\n",
       "      <td>2023-02-19T18:33:20Z</td>\n",
       "      <td>4d2c2878-7603-42e8-815c-36360d46ceb0</td>\n",
       "      <td>3</td>\n",
       "      <td>c167e7db-a42f-40f2-a364-34aadb768ada</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>288.3</td>\n",
       "      <td>False</td>\n",
       "      <td>62f06e49-34b7-4f7d-8fec-91c47a3f12b2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e58f4191-7194-4ab5-bf37-6400ef719f22</td>\n",
       "      <td>e32a0878-ba15-4fb1-8518-d86663bbaa17</td>\n",
       "      <td>2023-02-19T18:33:28Z</td>\n",
       "      <td>4d2c2878-7603-42e8-815c-36360d46ceb0</td>\n",
       "      <td>4</td>\n",
       "      <td>fde985da-1936-4179-a7f7-29902edaa0c3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>302.0</td>\n",
       "      <td>False</td>\n",
       "      <td>62f06e49-34b7-4f7d-8fec-91c47a3f12b2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0af36e78-9369-4966-8d57-14dfe740d775</td>\n",
       "      <td>c7bc600e-e3be-48ff-86cf-3b1d0ad42a5f</td>\n",
       "      <td>2023-02-19T18:33:47Z</td>\n",
       "      <td>da5c930f-10cf-49f3-a12c-deb891ae35ac</td>\n",
       "      <td>5</td>\n",
       "      <td>16a20025-458b-4d97-a2cf-4c6516e040d1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.7</td>\n",
       "      <td>273.0</td>\n",
       "      <td>False</td>\n",
       "      <td>62f06e49-34b7-4f7d-8fec-91c47a3f12b2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>34bba6de-c64c-48b2-825a-ec7bf9408512</td>\n",
       "      <td>f9f43909-97dd-47f0-ac02-647dd3cf96fe</td>\n",
       "      <td>2023-02-19T18:38:42Z</td>\n",
       "      <td>da5c930f-10cf-49f3-a12c-deb891ae35ac</td>\n",
       "      <td>56</td>\n",
       "      <td>e8ea66a3-20f8-4ca8-b25a-ec14f3840f25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>68.2</td>\n",
       "      <td>236.2</td>\n",
       "      <td>False</td>\n",
       "      <td>62f06e49-34b7-4f7d-8fec-91c47a3f12b2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>ae1503f8-a0f1-4394-aa5b-e52f1f682806</td>\n",
       "      <td>fb40e666-8ac7-4ea2-bfa7-5a8a418613b7</td>\n",
       "      <td>2023-02-19T18:38:50Z</td>\n",
       "      <td>4d2c2878-7603-42e8-815c-36360d46ceb0</td>\n",
       "      <td>57</td>\n",
       "      <td>e8ea66a3-20f8-4ca8-b25a-ec14f3840f25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>64.3</td>\n",
       "      <td>157.3</td>\n",
       "      <td>False</td>\n",
       "      <td>62f06e49-34b7-4f7d-8fec-91c47a3f12b2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>32f1a379-71dd-4b47-8e06-c1b27e9e54cf</td>\n",
       "      <td>042a54f2-279e-4c0d-a12e-e16c9b36dfb9</td>\n",
       "      <td>2023-02-19T18:38:56Z</td>\n",
       "      <td>4d2c2878-7603-42e8-815c-36360d46ceb0</td>\n",
       "      <td>58</td>\n",
       "      <td>fbfcd132-422c-4d26-954a-498fbabc04a5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>60.6</td>\n",
       "      <td>203.8</td>\n",
       "      <td>False</td>\n",
       "      <td>62f06e49-34b7-4f7d-8fec-91c47a3f12b2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>bfd18657-3f6f-4e43-9131-1ab97ea3b9ae</td>\n",
       "      <td>f6b96c96-fe00-4dcb-93e1-8a5e38daa12a</td>\n",
       "      <td>2023-02-19T18:39:17Z</td>\n",
       "      <td>da5c930f-10cf-49f3-a12c-deb891ae35ac</td>\n",
       "      <td>59</td>\n",
       "      <td>fbfcd132-422c-4d26-954a-498fbabc04a5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>53.3</td>\n",
       "      <td>155.9</td>\n",
       "      <td>False</td>\n",
       "      <td>62f06e49-34b7-4f7d-8fec-91c47a3f12b2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>d1a29f92-e7f2-4724-9e51-bcff07e02c3b</td>\n",
       "      <td>dfc21723-ad0d-4b5f-8116-0049796bca6b</td>\n",
       "      <td>2023-02-19T18:39:19Z</td>\n",
       "      <td>a9347221-f68a-4e54-9f9d-17f787ed9f38</td>\n",
       "      <td>60</td>\n",
       "      <td>16a20025-458b-4d97-a2cf-4c6516e040d1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>61.9</td>\n",
       "      <td>177.5</td>\n",
       "      <td>False</td>\n",
       "      <td>62f06e49-34b7-4f7d-8fec-91c47a3f12b2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60 rows  11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      id  \\\n",
       "0   463eefb2-188e-4eae-8ca5-a2c3adf87b63   \n",
       "1   3728840c-0b51-472a-aa16-692670f468da   \n",
       "2   ad57512c-5ba5-4fb5-a74d-50d9fa11cb11   \n",
       "3   e58f4191-7194-4ab5-bf37-6400ef719f22   \n",
       "4   0af36e78-9369-4966-8d57-14dfe740d775   \n",
       "..                                   ...   \n",
       "55  34bba6de-c64c-48b2-825a-ec7bf9408512   \n",
       "56  ae1503f8-a0f1-4394-aa5b-e52f1f682806   \n",
       "57  32f1a379-71dd-4b47-8e06-c1b27e9e54cf   \n",
       "58  bfd18657-3f6f-4e43-9131-1ab97ea3b9ae   \n",
       "59  d1a29f92-e7f2-4724-9e51-bcff07e02c3b   \n",
       "\n",
       "                           appearance_id            created_at  \\\n",
       "0   0e383613-0c4f-4f3c-8145-73c0863a1929  2023-02-19T18:32:56Z   \n",
       "1   0a57b8db-0a17-4b0b-bab2-cfadb788d9b8  2023-02-19T18:33:16Z   \n",
       "2   c307679c-8ee9-47e3-882f-f386bc00f510  2023-02-19T18:33:20Z   \n",
       "3   e32a0878-ba15-4fb1-8518-d86663bbaa17  2023-02-19T18:33:28Z   \n",
       "4   c7bc600e-e3be-48ff-86cf-3b1d0ad42a5f  2023-02-19T18:33:47Z   \n",
       "..                                   ...                   ...   \n",
       "55  f9f43909-97dd-47f0-ac02-647dd3cf96fe  2023-02-19T18:38:42Z   \n",
       "56  fb40e666-8ac7-4ea2-bfa7-5a8a418613b7  2023-02-19T18:38:50Z   \n",
       "57  042a54f2-279e-4c0d-a12e-e16c9b36dfb9  2023-02-19T18:38:56Z   \n",
       "58  f6b96c96-fe00-4dcb-93e1-8a5e38daa12a  2023-02-19T18:39:17Z   \n",
       "59  dfc21723-ad0d-4b5f-8116-0049796bca6b  2023-02-19T18:39:19Z   \n",
       "\n",
       "                          draft_entry_id  number  \\\n",
       "0   a9347221-f68a-4e54-9f9d-17f787ed9f38       1   \n",
       "1   da5c930f-10cf-49f3-a12c-deb891ae35ac       2   \n",
       "2   4d2c2878-7603-42e8-815c-36360d46ceb0       3   \n",
       "3   4d2c2878-7603-42e8-815c-36360d46ceb0       4   \n",
       "4   da5c930f-10cf-49f3-a12c-deb891ae35ac       5   \n",
       "..                                   ...     ...   \n",
       "55  da5c930f-10cf-49f3-a12c-deb891ae35ac      56   \n",
       "56  4d2c2878-7603-42e8-815c-36360d46ceb0      57   \n",
       "57  4d2c2878-7603-42e8-815c-36360d46ceb0      58   \n",
       "58  da5c930f-10cf-49f3-a12c-deb891ae35ac      59   \n",
       "59  a9347221-f68a-4e54-9f9d-17f787ed9f38      60   \n",
       "\n",
       "                            pick_slot_id points projection_adp  \\\n",
       "0   c167e7db-a42f-40f2-a364-34aadb768ada   0.00            1.2   \n",
       "1   8b25d7d2-613b-4a5a-b47b-a6ea125841c0   0.00            2.9   \n",
       "2   c167e7db-a42f-40f2-a364-34aadb768ada   0.00            2.6   \n",
       "3   fde985da-1936-4179-a7f7-29902edaa0c3   0.00            5.0   \n",
       "4   16a20025-458b-4d97-a2cf-4c6516e040d1   0.00            4.7   \n",
       "..                                   ...    ...            ...   \n",
       "55  e8ea66a3-20f8-4ca8-b25a-ec14f3840f25   0.00           68.2   \n",
       "56  e8ea66a3-20f8-4ca8-b25a-ec14f3840f25   0.00           64.3   \n",
       "57  fbfcd132-422c-4d26-954a-498fbabc04a5   0.00           60.6   \n",
       "58  fbfcd132-422c-4d26-954a-498fbabc04a5   0.00           53.3   \n",
       "59  16a20025-458b-4d97-a2cf-4c6516e040d1   0.00           61.9   \n",
       "\n",
       "   projection_points  swapped                              draft_id  \n",
       "0              323.2    False  62f06e49-34b7-4f7d-8fec-91c47a3f12b2  \n",
       "1              303.5    False  62f06e49-34b7-4f7d-8fec-91c47a3f12b2  \n",
       "2              288.3    False  62f06e49-34b7-4f7d-8fec-91c47a3f12b2  \n",
       "3              302.0    False  62f06e49-34b7-4f7d-8fec-91c47a3f12b2  \n",
       "4              273.0    False  62f06e49-34b7-4f7d-8fec-91c47a3f12b2  \n",
       "..               ...      ...                                   ...  \n",
       "55             236.2    False  62f06e49-34b7-4f7d-8fec-91c47a3f12b2  \n",
       "56             157.3    False  62f06e49-34b7-4f7d-8fec-91c47a3f12b2  \n",
       "57             203.8    False  62f06e49-34b7-4f7d-8fec-91c47a3f12b2  \n",
       "58             155.9    False  62f06e49-34b7-4f7d-8fec-91c47a3f12b2  \n",
       "59             177.5    False  62f06e49-34b7-4f7d-8fec-91c47a3f12b2  \n",
       "\n",
       "[60 rows x 11 columns]"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from os.path import join\n",
    "\n",
    "# out = '/home/cdelong/Python-Projects/UD-Draft-Model/Repo-Work/UD-Draft-Model/UD_draft_model/scratch'\n",
    "# active_drafts = DraftsActive(bearer_token)\n",
    "\n",
    "# df = pd.read_pickle(join(out, 'active_drafts2.pkl'))\n",
    "# _df = active_drafts._add_scoring_type(df.copy())\n",
    "\n",
    "slate_id = _df['slate_id'].iloc[0]\n",
    "scoring_type_id = _df['scoring_type_id'].iloc[0]\n",
    "draft_id = [_df['id'].iloc[0]]\n",
    "refs = ReferenceData(slate_id, scoring_type_id)\n",
    "\n",
    "draft_detail = DraftsDetail(draft_id, bearer_token)\n",
    "\n",
    "# df_players = refs.create_df_players_master()\n",
    "\n",
    "draft_detail.create_df_drafts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>auto_pick_at</th>\n",
       "      <th>clock</th>\n",
       "      <th>contest_style_id</th>\n",
       "      <th>draft_at</th>\n",
       "      <th>draft_entry_id</th>\n",
       "      <th>draft_type</th>\n",
       "      <th>entry_count</th>\n",
       "      <th>entry_role</th>\n",
       "      <th>entry_style_id</th>\n",
       "      <th>pick_count</th>\n",
       "      <th>slate_id</th>\n",
       "      <th>source</th>\n",
       "      <th>source_entry_style_id</th>\n",
       "      <th>status</th>\n",
       "      <th>title</th>\n",
       "      <th>user_auto_pick</th>\n",
       "      <th>user_pick_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>62f06e49-34b7-4f7d-8fec-91c47a3f12b2</td>\n",
       "      <td>2023-02-19T18:33:23Z</td>\n",
       "      <td>30</td>\n",
       "      <td>978b95dd-7c25-467c-83c9-332d90a557a4</td>\n",
       "      <td>2023-02-19T18:32:53Z</td>\n",
       "      <td>a9347221-f68a-4e54-9f9d-17f787ed9f38</td>\n",
       "      <td>fast</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>293d827b-2a0a-555f-b8da-1bc86766ed2f</td>\n",
       "      <td>0</td>\n",
       "      <td>b84244dc-aa63-4b62-bdd5-8fccd365c074</td>\n",
       "      <td>sit_and_go</td>\n",
       "      <td>None</td>\n",
       "      <td>drafting</td>\n",
       "      <td>None</td>\n",
       "      <td>user</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id          auto_pick_at  clock  \\\n",
       "0  62f06e49-34b7-4f7d-8fec-91c47a3f12b2  2023-02-19T18:33:23Z     30   \n",
       "\n",
       "                       contest_style_id              draft_at  \\\n",
       "0  978b95dd-7c25-467c-83c9-332d90a557a4  2023-02-19T18:32:53Z   \n",
       "\n",
       "                         draft_entry_id draft_type  entry_count entry_role  \\\n",
       "0  a9347221-f68a-4e54-9f9d-17f787ed9f38       fast            3       None   \n",
       "\n",
       "                         entry_style_id  pick_count  \\\n",
       "0  293d827b-2a0a-555f-b8da-1bc86766ed2f           0   \n",
       "\n",
       "                               slate_id      source source_entry_style_id  \\\n",
       "0  b84244dc-aa63-4b62-bdd5-8fccd365c074  sit_and_go                  None   \n",
       "\n",
       "     status title user_auto_pick  user_pick_order  \n",
       "0  drafting  None           user                1  "
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from os.path import join\n",
    "\n",
    "active_drafts = DraftsActive(bearer_token)\n",
    "\n",
    "data = active_drafts.read_in_site_data(\n",
    "    DraftsActive.url, headers=active_drafts.auth_header\n",
    ")\n",
    "\n",
    "df = active_drafts.create_df_active_drafts()\n",
    "\n",
    "# out = '/home/cdelong/Python-Projects/UD-Draft-Model/Repo-Work/UD-Draft-Model/UD_draft_model/scratch'\n",
    "# with open(join(out, 'active_drafts2.json'), 'w') as f:\n",
    "#     json.dump(data, f)\n",
    "\n",
    "# df.to_pickle(join(out, 'active_drafts2.pkl'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>contest_style_ids</th>\n",
       "      <th>cutoff_at</th>\n",
       "      <th>description</th>\n",
       "      <th>game_count</th>\n",
       "      <th>lobby_hidden</th>\n",
       "      <th>sport_id</th>\n",
       "      <th>start_at</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b84244dc-aa63-4b62-bdd5-8fccd365c074</td>\n",
       "      <td>978b95dd-7c25-467c-83c9-332d90a557a4</td>\n",
       "      <td>2023-04-28T23:52:00Z</td>\n",
       "      <td>2023 Pre-Draft Best Ball</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>NFL</td>\n",
       "      <td>2023-04-29T00:00:00Z</td>\n",
       "      <td>2023 Pre-Draft Best Ball</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id                     contest_style_ids  \\\n",
       "0  b84244dc-aa63-4b62-bdd5-8fccd365c074  978b95dd-7c25-467c-83c9-332d90a557a4   \n",
       "\n",
       "              cutoff_at               description game_count  lobby_hidden  \\\n",
       "0  2023-04-28T23:52:00Z  2023 Pre-Draft Best Ball       None         False   \n",
       "\n",
       "  sport_id              start_at                     title  \n",
       "0      NFL  2023-04-29T00:00:00Z  2023 Pre-Draft Best Ball  "
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slates = Slates(bearer_token, 'available')\n",
    "slates.create_df_slates()\n",
    "# slate = slates.slates[2]\n",
    "# drafts = Drafts(bearer_token, slate)\n",
    "\n",
    "# df_drafts = drafts.create_df_all_leagues()\n",
    "# draft_ids = list(df_drafts['id'])\n",
    "\n",
    "# draft_detail = DraftsDetail(draft_ids, bearer_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>contest_style_ids</th>\n",
       "      <th>cutoff_at</th>\n",
       "      <th>description</th>\n",
       "      <th>game_count</th>\n",
       "      <th>lobby_hidden</th>\n",
       "      <th>sport_id</th>\n",
       "      <th>start_at</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b84244dc-aa63-4b62-bdd5-8fccd365c074</td>\n",
       "      <td>[978b95dd-7c25-467c-83c9-332d90a557a4]</td>\n",
       "      <td>2023-04-28T23:52:00Z</td>\n",
       "      <td>2023 Pre-Draft Best Ball</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>NFL</td>\n",
       "      <td>2023-04-29T00:00:00Z</td>\n",
       "      <td>2023 Pre-Draft Best Ball</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  \\\n",
       "0  b84244dc-aa63-4b62-bdd5-8fccd365c074   \n",
       "\n",
       "                        contest_style_ids             cutoff_at  \\\n",
       "0  [978b95dd-7c25-467c-83c9-332d90a557a4]  2023-04-28T23:52:00Z   \n",
       "\n",
       "                description game_count  lobby_hidden sport_id  \\\n",
       "0  2023 Pre-Draft Best Ball       None         False      NFL   \n",
       "\n",
       "               start_at                     title  \n",
       "0  2023-04-29T00:00:00Z  2023 Pre-Draft Best Ball  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_available = 'https://stats.underdogfantasy.com/v1/sports/nfl/slates'\n",
    "url_completed = 'https://api.underdogfantasy.com/v2/user/completed_slates'\n",
    "url_settled = 'https://api.underdogfantasy.com/v1/user/sports/nfl/settled_slates'\n",
    "\n",
    "url_st = 'https://stats.underdogfantasy.com/v1/scoring_types'\n",
    "\n",
    "auth_header = {\n",
    "    \"accept\": \"application/json\",    \n",
    "    \"authorization\": bearer_token,\n",
    "    \"user-agent\": \"Mozilla/5.0 (X11; Linux x86_64) \\\n",
    "                        AppleWebKit/537.36 (KHTML, like Gecko) \\\n",
    "                        Chrome/99.0.4844.51 Safari/537.36\"\n",
    "}\n",
    "\n",
    "base = BaseData()\n",
    "\n",
    "data = base.read_in_site_data(url_available, headers=auth_header)\n",
    "# data = base.read_in_site_data(url_st)\n",
    "\n",
    "base.create_scraped_data_df(data['slates'])\n",
    "\n",
    "# data['slates'][0].keys()\n",
    "# data['scoring_types'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# site.create_df_slates_completed(headers=auth_header)\n",
    "# site.create_df_scoring_types()\n",
    "# site.create_df_contest_styles()\n",
    "\n",
    "# df = site.create_df_contest_styles()\n",
    "\n",
    "# df.loc[df['status'] == 'active']\n",
    "\n",
    "\n",
    "# leagues.create_df_drafts()\n",
    "\n",
    "# user.create_df_all_leagues()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'drafts': [{'id': 'bf473ecd-e3b6-4777-86c5-35db00b34c1a', 'auto_pick_at': '2023-02-12T21:45:42Z', 'clock': 30, 'contest_style_id': '502f3754-7bf4-4cc4-baf3-ea7d93f09318', 'draft_at': '2023-02-12T21:45:12Z', 'draft_entry_id': '08ddbd4a-4053-447c-9e97-82fccca02f4c', 'draft_type': 'fast', 'entry_count': 2, 'entry_role': None, 'entry_style_id': '2e20d083-b067-4016-a6f3-1d7a9470d6d7', 'pick_count': 0, 'slate_id': '53ec8ef2-e1f2-401c-9a83-c0838d6a2b77', 'source': 'sit_and_go', 'source_entry_style_id': None, 'status': 'drafting', 'title': None, 'user_auto_pick': 'off', 'user_pick_order': 1}]}\n"
     ]
    }
   ],
   "source": [
    "# url = 'https://api.underdogfantasy.com/v2/drafts/5439a246-a197-42b3-a7bf-a0de1eda4a6d'\n",
    "\n",
    "# url = (\n",
    "#     \"https://api.underdogfantasy.com/v2/user/slates/\"\n",
    "#     + '53ec8ef2-e1f2-401c-9a83-c0838d6a2b77'\n",
    "#     + \"/live_drafts\"\n",
    "# )\n",
    "\n",
    "url = 'https://api.underdogfantasy.com/v3/user/active_drafts'\n",
    "\n",
    "user = UserData(bearer_token)\n",
    "\n",
    "active_drafts = user.read_in_site_data(url, user.auth_header)\n",
    "print(active_drafts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>auto_pick_at</th>\n",
       "      <th>clock</th>\n",
       "      <th>contest_style_id</th>\n",
       "      <th>draft_at</th>\n",
       "      <th>draft_entry_id</th>\n",
       "      <th>draft_type</th>\n",
       "      <th>entry_count</th>\n",
       "      <th>entry_role</th>\n",
       "      <th>entry_style_id</th>\n",
       "      <th>pick_count</th>\n",
       "      <th>slate_id</th>\n",
       "      <th>source</th>\n",
       "      <th>source_entry_style_id</th>\n",
       "      <th>status</th>\n",
       "      <th>title</th>\n",
       "      <th>user_auto_pick</th>\n",
       "      <th>user_pick_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bf473ecd-e3b6-4777-86c5-35db00b34c1a</td>\n",
       "      <td>2023-02-12T21:45:42Z</td>\n",
       "      <td>30</td>\n",
       "      <td>502f3754-7bf4-4cc4-baf3-ea7d93f09318</td>\n",
       "      <td>2023-02-12T21:45:12Z</td>\n",
       "      <td>08ddbd4a-4053-447c-9e97-82fccca02f4c</td>\n",
       "      <td>fast</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>2e20d083-b067-4016-a6f3-1d7a9470d6d7</td>\n",
       "      <td>0</td>\n",
       "      <td>53ec8ef2-e1f2-401c-9a83-c0838d6a2b77</td>\n",
       "      <td>sit_and_go</td>\n",
       "      <td>None</td>\n",
       "      <td>drafting</td>\n",
       "      <td>None</td>\n",
       "      <td>off</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id          auto_pick_at  clock  \\\n",
       "0  bf473ecd-e3b6-4777-86c5-35db00b34c1a  2023-02-12T21:45:42Z     30   \n",
       "\n",
       "                       contest_style_id              draft_at  \\\n",
       "0  502f3754-7bf4-4cc4-baf3-ea7d93f09318  2023-02-12T21:45:12Z   \n",
       "\n",
       "                         draft_entry_id draft_type  entry_count entry_role  \\\n",
       "0  08ddbd4a-4053-447c-9e97-82fccca02f4c       fast            2       None   \n",
       "\n",
       "                         entry_style_id  pick_count  \\\n",
       "0  2e20d083-b067-4016-a6f3-1d7a9470d6d7           0   \n",
       "\n",
       "                               slate_id      source source_entry_style_id  \\\n",
       "0  53ec8ef2-e1f2-401c-9a83-c0838d6a2b77  sit_and_go                  None   \n",
       "\n",
       "     status title user_auto_pick  user_pick_order  \n",
       "0  drafting  None            off                1  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################## CODE FOR ACTIVE DRAFTS #############################\n",
    "\n",
    "# url = 'https://api.underdogfantasy.com/v3/user/active_drafts'\n",
    "\n",
    "# user = UserData(bearer_token)\n",
    "\n",
    "# active_drafts = user.read_in_site_data(url, user.auth_header)\n",
    "\n",
    "df = user.create_scraped_data_df(active_drafts['drafts'])\n",
    "\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dc4492b5efc155a9cdcda09d43a20d3fd18b0b2d151b680a1414de88ecead1b9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
